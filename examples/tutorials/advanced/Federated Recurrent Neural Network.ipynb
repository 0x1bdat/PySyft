{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning of a Recurrent Neural Network for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you are going to learn how to train a Recurrent Neural Network in a federated way with the purpose of *classifying* a person's name as belonging to a certain nationality with a certain probability. \n",
    "\n",
    "Following distributed training, the resulting model is going to be able to perform operations like:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "predict(model_pointers[\"bob\"], \"Qing\", alice) #alice is our worker\n",
    "\n",
    " Qing\n",
    "(-1.43) Korean\n",
    "(-1.74) Vietnamese\n",
    "(-2.18) Arabic\n",
    "\n",
    "predict(model_pointers[\"alice\"], \"Daniele\", alice)\n",
    "\n",
    " Daniele\n",
    "(-1.58) French\n",
    "(-2.04) Scottish\n",
    "(-2.07) Dutch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The present example is inspired by an official Pytorch [tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html), which I ported to PySyft with  the purpose of learning a Recurrent Neural Network in a federated way.The present tutorial is self-contained, so there are no dependencies on external pieces of code apart from a few Python libraries.\n",
    "\n",
    "**Tutorial's author**: Daniele Gadler. [@DanyEle](https://github.com/danyele). Feel free to contact me in case of issues!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Step: Dependencies!\n",
    "\n",
    "Make sure you have all the following packages installed, or install them via:\n",
    "\n",
    "pip3 install <package_name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import string\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math\n",
    "import syft as sy\n",
    "import pandas as pd\n",
    "import random\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "from syft.workers import WebsocketClientWorker\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step: Data pre-processing and transformation\n",
    "\n",
    "We are going to train our neural network based on some dataset, so download the following dataset [LINK](https://download.pytorch.org/tutorial/data.zip) and extract it to the same directory of this Jupyter notebook. After you've extracted the data, you'll be able to read it in Python after initializing a few basic functions for parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all the files in a certain path\n",
    "def findFiles(path):\n",
    "    return glob.glob(path)\n",
    "\n",
    "# Read a file and split into lines\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "#convert a string 's' in unicode format to ASCII format\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/names/Greek.txt\n",
      "data/names/Italian.txt\n",
      "data/names/Chinese.txt\n",
      "data/names/German.txt\n",
      "data/names/Japanese.txt\n",
      "data/names/Russian.txt\n",
      "data/names/Portuguese.txt\n",
      "data/names/Dutch.txt\n",
      "data/names/Scottish.txt\n",
      "data/names/Irish.txt\n",
      "data/names/Arabic.txt\n",
      "data/names/French.txt\n",
      "data/names/Vietnamese.txt\n",
      "data/names/Polish.txt\n",
      "data/names/Korean.txt\n",
      "data/names/Czech.txt\n",
      "data/names/English.txt\n",
      "data/names/Spanish.txt\n",
      "Amount of categories:18\n"
     ]
    }
   ],
   "source": [
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "#dictionary containing the nation as key and the names as values\n",
    "#Example: category_lines[\"italian\"] = [\"Abandonato\",\"Abatangelo\",\"Abatantuono\",...]\n",
    "category_lines = {}\n",
    "#List containing the different categories in the data\n",
    "all_categories = []\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    print(filename)\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines   \n",
    "    \n",
    "n_categories = len(all_categories)\n",
    "\n",
    "print(\"Amount of categories:\" + str(n_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to format the data so as to make it compliant with the format requested by PySyft and Pytorch. Firstly, we define a dataset class, specifying how batches ought to be extracted from the dataset in order for them to be assigned to the different workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageDataset(Dataset):\n",
    "    #Constructor is mandatory\n",
    "        def __init__(self, text, labels, transform=None):\n",
    "            self.data = text\n",
    "            self.targets = labels #categories\n",
    "            #self.to_torchtensor()\n",
    "            self.transform = transform\n",
    "        \n",
    "        def to_torchtensor(self):            \n",
    "            self.data = torch.from_numpy(self.text, requires_grad=True)\n",
    "            self.labels =torch.from_numpy(self.targets, requires_grad=True)\n",
    "        \n",
    "        def __len__(self):\n",
    "            #Mandatory\n",
    "            '''Returns:\n",
    "                    Length [int]: Length of Dataset/batches\n",
    "            '''\n",
    "            return len(self.data)\n",
    "    \n",
    "        def __getitem__(self, idx): \n",
    "            #Mandatory \n",
    "            \n",
    "            '''Returns:\n",
    "                     Data [Torch Tensor]: \n",
    "                     Target [ Torch Tensor]:\n",
    "            '''\n",
    "            sample=self.data[idx]\n",
    "            target=self.targets[idx]\n",
    "                    \n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "    \n",
    "            return sample,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The list of arguments for our program. We will be needing most of them soon.\n",
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 1\n",
    "        self.learning_rate = 0.005\n",
    "        self.epochs = 10000\n",
    "        self.federate_after_n_batches = 15000\n",
    "        self.seed = 1\n",
    "        self.print_every = 200\n",
    "        self.plot_every = 100\n",
    "        self.use_cuda = False\n",
    "        \n",
    "args = Arguments()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adamou', 'Agelakos', 'Akrivopoulos', 'Alexandropoulos', 'Anetakis', 'Angelopoulos', 'Antimisiaris', 'Antipas', 'Antonakos', 'Antoniadis', 'Antonopoulos', 'Antonopoulos', 'Antonopoulos', 'Arvanitoyannis', 'Avgerinos', 'Banos', 'Batsakis', 'Bekyros', 'Belesis']\n",
      "['Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek', 'Greek']\n",
      "\n",
      " \n",
      " Amount of data points loaded: 20074\n"
     ]
    }
   ],
   "source": [
    "#We now need to unwrap data samples so as to have them all in one single list instead of a dictionary\n",
    "#instead of a dictionary where different categories are addressed by key.\n",
    "#We want:\n",
    "#data_points(X) = [d1,....,dn]\n",
    "#categories(Y) = [c1,....,cn]\n",
    "\n",
    "#Set of names(X)\n",
    "names_list = []\n",
    "#Set of labels (Y)\n",
    "category_list = []\n",
    "\n",
    "#Convert into a list with corresponding label.\n",
    "\n",
    "for nation, names in category_lines.items():\n",
    "    #iterate over every single name\n",
    "    for name in names:\n",
    "        names_list.append(name)      #input data point\n",
    "        category_list.append(nation) #label\n",
    "        \n",
    "#let's see if it was successfully loaded. Each data sample(X) should have its own corresponding category(Y)\n",
    "print(names_list[1:20])\n",
    "print(category_list[1:20])\n",
    "\n",
    "print(\"\\n \\n Amount of data points loaded: \" + str(len(names_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now need to turn our categories into numbers, as PyTorch cannot really understand plain text\n",
    "#Example: \"Greek\" ---> 0\n",
    "\n",
    "#Assign an integer to every category\n",
    "categories_numerical = pd.factorize(category_list)[0]\n",
    "#Let's wrap our categories with a tensor, so that it can be loaded by LanguageDataset\n",
    "category_tensor = torch.tensor(np.array(categories_numerical), dtype=torch.long)\n",
    "#Ready to be processed by torch.from_numpy in LanguageDataset\n",
    "categories_numpy = np.array(category_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamidis\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n",
      "(8, 1, 57)\n"
     ]
    }
   ],
   "source": [
    "#We now need to turn every single character in each input line string into a vector of vector, so that it\n",
    "#can actualyl be processed by our Recurrent Neural Network. \n",
    "#Example: character \"a\" = array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        #0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        #0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        #0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n",
    "#A word is made up one one such vector per character. This technique is known as generating \"word embeddings\",\n",
    "#as we embed a word into a vector of vectors.\n",
    "       \n",
    "\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "    \n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters) #Daniele: len(max_line_size) was len(line)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    #Daniele: add blank elements over here\n",
    "    return tensor    \n",
    "    \n",
    "    \n",
    "    \n",
    "def list_strings_to_list_tensors(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensor(line)\n",
    "        lineNumpy = lineTensor.numpy()\n",
    "        lines_tensors.append(lineNumpy)\n",
    "        \n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors(names_list)\n",
    "\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adamidis\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "torch.Size([19, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "#Now we need to identify the longest word in the dataset, as all tensors need to have the same shape \n",
    "#in order to fit into a numpy array, so we append vectors containing just \"0\"s into our words' up to the maximum\n",
    "#word size, such that all embeddings have the same shape.\n",
    "\n",
    "max_line_size = max(len(x) for x in lines_tensors)\n",
    "\n",
    "def lineToTensorFillEmpty(line, max_line_size):\n",
    "    tensor = torch.zeros(max_line_size, 1, n_letters) #notice the difference between this method and the previous one\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "        \n",
    "        #Vectors with (0,0,.... ,0) are placed where there are no characters\n",
    "    return tensor\n",
    "\n",
    "def list_strings_to_list_tensors_fill_empty(names_list):\n",
    "    lines_tensors = []\n",
    "    for index, line in enumerate(names_list):\n",
    "        lineTensor = lineToTensorFillEmpty(line, max_line_size)\n",
    "        lines_tensors.append(lineTensor)\n",
    "    return(lines_tensors)\n",
    "\n",
    "lines_tensors = list_strings_to_list_tensors_fill_empty(names_list)\n",
    "\n",
    "#Let's take a look at what a word now looks like\n",
    "print(names_list[0])\n",
    "print(lines_tensors[0])\n",
    "print(lines_tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20074, 19, 1, 57)\n",
      "(20074, 19, 57)\n"
     ]
    }
   ],
   "source": [
    "#And finally, from a list, we can create a numpy array with all our word embeddings having the same shape:\n",
    "array_lines_tensors = np.stack(lines_tensors)\n",
    "#However, such operation introduces, one extra dimension (look at the dimension with index=2 having size '1')\n",
    "print(array_lines_tensors.shape)\n",
    "#Because that dimension just has size 1, we can get rid of it with the following function call\n",
    "array_lines_proper_dimension = np.squeeze(array_lines_tensors, axis=2)\n",
    "print(array_lines_proper_dimension.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data unbalancing and batch randomization:\n",
    "You may have noticed that our dataset is strongly unbalanced and contains a lot of data points in the \"russian.txt\" dataset. However, we would still like to take a random batch during our training procedure at every iteration. In order to prevent our neural network from classifying a data point as always belonging to the \"Russian\" category, we first pick a random category and then select a data point from that category. To do that, we construct a dictionary mapping a certain category to the corresponding starting index in the list of data points (e.g.: lines). Afterwards, we will take a datapoint starting from the starting index found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Greek': 0, 'Italian': 203, 'Chinese': 912, 'German': 1180, 'Japanese': 1904, 'Russian': 2895, 'Portuguese': 12303, 'Dutch': 12377, 'Scottish': 12674, 'Irish': 12774, 'Arabic': 13006, 'French': 15006, 'Vietnamese': 15283, 'Polish': 15356, 'Korean': 15495, 'Czech': 15589, 'English': 16108, 'Spanish': 19776}\n"
     ]
    }
   ],
   "source": [
    "def find_start_index_per_category(category_list):\n",
    "    categories_start_index = {}\n",
    "    \n",
    "    #Initialize every category with an empty list\n",
    "    for category in all_categories:\n",
    "        categories_start_index[category] = []\n",
    "    \n",
    "    #Insert the start index of each category into the dictionary categories_start_index\n",
    "    #Example: \"Italian\" --> 203\n",
    "    #         \"Spanish\" --> 19776\n",
    "    last_category = None\n",
    "    i = 0\n",
    "    for name in names_list:\n",
    "        cur_category = category_list[i]\n",
    "        if(cur_category != last_category):\n",
    "            categories_start_index[cur_category] = i\n",
    "            last_category = cur_category\n",
    "        \n",
    "        i = i + 1\n",
    "        \n",
    "    return(categories_start_index)\n",
    "\n",
    "categories_start_index = find_start_index_per_category(category_list)\n",
    "\n",
    "print(categories_start_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define our functions to take a random data sample from the dataset\n",
    "\n",
    "def randomChoice(l):\n",
    "    rand_value = random.randint(0, len(l) - 1)\n",
    "    return l[rand_value], rand_value\n",
    "\n",
    "\n",
    "def randomTrainingIndex():\n",
    "    category, rand_cat_index = randomChoice(all_categories) #cat = category, it's not a random animal\n",
    "    #print(category)\n",
    "    #rand_line_index is a relative index for a data point within the random category rand_cat_index\n",
    "    line, rand_line_index = randomChoice(category_lines[category])\n",
    "    category_start_index = categories_start_index[category]\n",
    "    absolute_index = category_start_index + rand_line_index\n",
    "    return(absolute_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model - Recurrent Neural Network\n",
    "Hey, I must admit that was indeed a lot of data preprocessing and transformation, but it was well worth it. \n",
    "We have defined almost all the function we'll be needing during the training procedure and our data is ready\n",
    "to be fed into the neural network, which we're creating now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
      "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Two hidden layers, based on simple linear layers\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "#Let's instantiate the neural network already:\n",
    "n_hidden = 128\n",
    "#Instantiate RNN\n",
    "\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "model = RNN(n_letters, n_hidden, n_categories).to(device)\n",
    "#The final softmax layer will produce a probability for each one of our 18 categories\n",
    "print(model)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's define our workers. You can either use remote workers or virtual workers\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: define remote worker dani\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: also define remote worker ele\n",
    "#inad = sy.VirtualWorker(hook, id=\"inad\")  # <-- NEW: also define remote worker ele\n",
    "\n",
    "workers_virtual = [alice, bob]\n",
    "\n",
    "#If you have your workers operating remotely, like on Raspberry PIs\n",
    "#kwargs_websocket_alice = {\"host\": \"ip_alice\", \"hook\": hook}\n",
    "#alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket_alice)\n",
    "#kwargs_websocket_bob = {\"host\": \"ip_bob\", \"hook\": hook}\n",
    "#bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket_bob)\n",
    "#workers_virtual = [alice, bob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array_lines_proper_dimension = our data points(X)\n",
    "#categories_numpy = our labels (Y)\n",
    "langDataset =  LanguageDataset(array_lines_proper_dimension, categories_numpy)\n",
    "\n",
    "#assign the data points and the corresponding categories to workers.\n",
    "federated_train_loader = sy.FederatedDataLoader(\n",
    "            langDataset\n",
    "            .federate(workers_virtual),\n",
    "            batch_size=args.batch_size)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Training!\n",
    "\n",
    "\n",
    "It's now time to train our Recurrent Neural Network based on the processed data. To do that, we need to define a few more functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def fw_bw_pass_model(model_pointers, line_single, category_single):\n",
    "    #get the right initialized model\n",
    "    model_ptr = model_pointers[line_single.location.id]   \n",
    "    line_reshaped = line_single.reshape(max_line_size, 1, len(all_letters))\n",
    "    line_reshaped, category_single = line_reshaped.to(device), category_single.to(device)\n",
    "    #Firstly, initialize hidden layer\n",
    "    hidden_init = model_ptr.initHidden() \n",
    "    #And now zero grad the model (not the other way around)\n",
    "    model_ptr.zero_grad()\n",
    "    hidden_ptr = hidden_init.send(line_single.location)\n",
    "    amount_lines_non_zero = len(torch.nonzero(line_reshaped.copy().get()))\n",
    "    #now need to perform forward passes\n",
    "    for i in range(amount_lines_non_zero): \n",
    "        output, hidden_ptr = model_ptr(line_reshaped[i], hidden_ptr) \n",
    "    criterion = nn.NLLLoss()   \n",
    "    loss = criterion(output, category_single) \n",
    "    loss.backward()\n",
    "    \n",
    "    model_got = model_ptr.get() \n",
    "    \n",
    "    #Perform model weights' updates    \n",
    "    for param in model_got.parameters():\n",
    "        param.data.add_(-args.learning_rate, param.grad.data)\n",
    "        \n",
    "    model_sent = model_got.send(line_single.location.id)\n",
    "    model_pointers[line_single.location.id] = model_sent\n",
    "    \n",
    "    return(model_pointers, loss, output)\n",
    "            \n",
    "  \n",
    "    \n",
    "def train_RNN(n_iters, print_every, plot_every, federate_after_n_batches, list_federated_train_loader):\n",
    "    current_loss = 0\n",
    "    all_losses = []    \n",
    "    \n",
    "    model_pointers = {}\n",
    "    \n",
    "    #Send the model to every single worker just before the training procedure starts\n",
    "    for worker in workers_virtual:\n",
    "        model_copied = model.copy()\n",
    "        model_ptr = model_copied.send(worker) \n",
    "        model_pointers[worker.id] = model_ptr\n",
    "\n",
    "    #extract a random element from the list and perform training on it\n",
    "    for iter in range(1, n_iters + 1):        \n",
    "        #random_index = random.randint(0, len(list_federated_train_loader) - 1)\n",
    "        random_index = randomTrainingIndex()\n",
    "        line_single, category_single = list_federated_train_loader[random_index]\n",
    "        #print(category_single.copy().get())\n",
    "        line_name = names_list[random_index]\n",
    "        model_pointers, loss, output = fw_bw_pass_model(model_pointers, line_single, category_single)\n",
    "        #model_pointers = fed_avg_every_n_iters(model_pointers, iter, args.federate_after_n_batches)\n",
    "        #Update the current loss and scale it by the amount of samples in that dataset\n",
    "        loss_got = loss.get().item() \n",
    "        current_loss += loss_got\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "             \n",
    "        if(iter % print_every == 0):\n",
    "            output_got = output.get()  #Without copy()\n",
    "            guess, guess_i = categoryFromOutput(output_got)\n",
    "            category = all_categories[category_single.copy().get().item()]\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss_got, line_name, guess, correct))\n",
    "    return(all_losses, model_pointers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating list of batches for the workers...\n"
     ]
    }
   ],
   "source": [
    "#In order for the defined randomization process to work, we need to wrap the data points and categories into\n",
    "#a list, from that we're going to take a random batch. This may take a few seconds\n",
    "\n",
    "print(\"Generating list of batches for the workers...\")\n",
    "list_federated_train_loader = list(federated_train_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 2% (0m 4s) 2.8028 Chambers / Arabic ✗ (English)\n",
      "400 4% (0m 8s) 2.6589 Bohler / German ✓\n",
      "600 6% (0m 13s) 2.8136 Akker / Vietnamese ✗ (Dutch)\n",
      "800 8% (0m 17s) 2.8038 Siew / German ✗ (Chinese)\n",
      "1000 10% (0m 21s) 2.3961 Fini / Italian ✓\n",
      "1200 12% (0m 25s) 2.7228 Rheem / Portuguese ✗ (Korean)\n",
      "1400 14% (0m 29s) 2.5078 Yee / German ✗ (Chinese)\n",
      "1600 16% (0m 33s) 1.6568 Jukovin / Greek ✗ (Russian)\n",
      "1800 18% (0m 37s) 2.2568 Stolarz / Scottish ✗ (Polish)\n",
      "2000 20% (0m 41s) 2.5628 Grabski / English ✗ (Czech)\n",
      "2200 22% (0m 45s) 2.2239 Ocaskova / Portuguese ✗ (Czech)\n",
      "2400 24% (0m 49s) 2.6895 Seo / Portuguese ✗ (Korean)\n",
      "2600 26% (0m 53s) 1.8414 Kouretas / Russian ✗ (Greek)\n",
      "2800 28% (0m 57s) 2.6989 Shamon / Czech ✗ (Arabic)\n",
      "3000 30% (1m 1s) 2.2445 Ortega / Spanish ✓\n",
      "3200 32% (1m 5s) 1.1349 Adamidis / Greek ✓\n",
      "3400 34% (1m 9s) 3.7562 Valov / Portuguese ✗ (Russian)\n",
      "3600 36% (1m 13s) 2.2061 Donnell / Dutch ✗ (Irish)\n",
      "3800 38% (1m 17s) 2.3568 Castellano / Polish ✗ (Spanish)\n",
      "4000 40% (1m 21s) 2.2555 Jelobinsky / Italian ✗ (Russian)\n",
      "4200 42% (1m 26s) 2.6169 Helm / Korean ✗ (English)\n",
      "4400 44% (1m 30s) 1.5586 Banh / Vietnamese ✓\n",
      "4600 46% (1m 34s) 2.6749 Magalhaes / Dutch ✗ (Portuguese)\n",
      "4800 48% (1m 38s) 0.9457 Khu / Chinese ✓\n",
      "5000 50% (1m 42s) 2.4379 Brady / English ✗ (Irish)\n",
      "5200 52% (1m 45s) 2.1599 Marqueringh / French ✗ (Dutch)\n",
      "5400 54% (1m 49s) 2.3970 Donnchadh / French ✗ (Irish)\n",
      "5600 56% (1m 53s) 2.8751 Caron / Scottish ✗ (French)\n",
      "5800 57% (1m 57s) 2.8957 Grant / Vietnamese ✗ (Scottish)\n",
      "6000 60% (2m 1s) 2.7254 Salazar / Polish ✗ (Portuguese)\n",
      "6200 62% (2m 5s) 1.9254 Issa / Korean ✗ (Arabic)\n",
      "6400 64% (2m 9s) 2.3535 Schoonenburg / Polish ✗ (Dutch)\n",
      "6600 66% (2m 13s) 1.4826 Peters / Greek ✗ (German)\n",
      "6800 68% (2m 17s) 1.1307 Ngo / Vietnamese ✓\n",
      "7000 70% (2m 21s) 0.6199 Baumhauer / German ✓\n",
      "7200 72% (2m 25s) 1.2334 Alessandri / Italian ✓\n",
      "7400 74% (2m 29s) 1.7646 O'Gorman / Dutch ✗ (Irish)\n",
      "7600 76% (2m 33s) 2.1734 Vandale / French ✗ (Dutch)\n",
      "7800 78% (2m 37s) 2.1211 Kanak / Arabic ✗ (Czech)\n",
      "8000 80% (2m 41s) 2.4031 Pierre / Portuguese ✗ (French)\n",
      "8200 82% (2m 45s) 0.9801 Kuwabara / Japanese ✓\n",
      "8400 84% (2m 49s) 1.5552 Zubizarreta / Spanish ✓\n",
      "8600 86% (2m 53s) 1.5033 Jmulev / German ✗ (Russian)\n",
      "8800 88% (2m 57s) 2.3862 Quyen / Arabic ✗ (Vietnamese)\n",
      "9000 90% (3m 2s) 2.8548 Ito / Chinese ✗ (Japanese)\n",
      "9200 92% (3m 6s) 0.3674 Guan / Chinese ✓\n",
      "9400 94% (3m 10s) 1.8062 Soares / English ✗ (Portuguese)\n",
      "9600 96% (3m 14s) 1.3105 Neville / Irish ✓\n",
      "9800 98% (3m 18s) 0.8155 Yim / Korean ✓\n",
      "10000 100% (3m 22s) 1.9471 Amari / Arabic ✓\n"
     ]
    }
   ],
   "source": [
    "#And finally,let's launch our training\n",
    "start = time.time()\n",
    "all_losses, model_pointers = train_RNN(args.epochs, args.print_every, args.plot_every, args.federate_after_n_batches, list_federated_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3a97b8c860>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8XFd5+P/PM6uk0WhfLEuyJMu7Ey+xkzhOyA4JKSShpGwlhBRIwxdKoKzl11KW0m+BFspOAwkBviFAQ9ghELKHxE7sxLu8ypYtWfu+b3N+f9y5oxlpJI1kjUbWPO/XS69Id67unNE495lznnOeI8YYlFJKKQBHohuglFJq4dCgoJRSKkSDglJKqRANCkoppUI0KCillArRoKCUUipEg4JSSqkQDQpKKaVCNCgopZQKcSW6ATOVl5dnysvLE90MpZQ6r+zevbvFGJM/3XnnXVAoLy9n165diW6GUkqdV0SkJpbzdPhIKaVUiAYFpZRSIRoUlFJKhWhQUEopFaJBQSmlVIgGBaWUUiEaFJRSSoUkTVCobe/jM785yPBoINFNUUqpBStpgsKhs118/y+n+MHzpxLdFKWUWrCSJii8el0h164p4CuPHaWhcyDRzVFKqQUpaYKCiPCvr1/HcMDw+d9XJbo5Sim1ICVNUAAoy/Xx3qsq+c3es/zleEuim6OUUgtOUgUFgPdeXcmynDQ+9asDDI1o0lkppcIlXVBIcTv5zM3rOdHcy1f+fDTRzVFKqQUl6YICwDVrCnjz1lK+8/QJXjjRmujmKKXUgpGUQQHgU69fR3muj3/82R46+4YT3RyllFoQkjYo+LwuvvqWTTR3D/LJX+zHGJPoJimlVMIlbVAA2FCSxT++ZhW/21/P/zxTnejmKKVUwp1323HOtb+/spKq+m7+4w+HSXE5eOflFYluklJKJUzcegoiUioiT4rIIRE5KCL3RDknU0R+IyJ7g+fcGa/2TMbpEL78po3csL6QT//mEA+9eHq+m6CUUgtGPIePRoAPG2PWAduA94nIunHnvA84ZIzZCFwN/JeIeOLYpqjcTgdfe+tmrl6dzyd/sZ8f7Yhpf2ullFp04hYUjDH1xpiXg993A1VA8fjTAL+ICJAOtGEFk3nndTn5ztu3cO3qAv7llwf4/O8OEQho8lkplVzmJdEsIuXAZmDnuIe+AawFzgL7gXuMMROWGYvIXSKyS0R2NTc3x62dKW4n975jK+/cXs53nz3Jex/cTf/QaNyeTymlFpq4BwURSQd+DnzQGNM17uEbgD3AUmAT8A0RyRh/DWPMvcaYrcaYrfn5+XFtr9MhfPrm9fzr69fxp0ONfOPJY3F9PqWUWkjiGhRExI0VEB40xjwS5ZQ7gUeM5ThwElgTzzbF6s7LK7huTQE/21XLiG7Mo5RKEvGcfSTAfUCVMebLk5x2GrgueH4hsBpYMAsG3nzxMpq7B3nySPyGrJRSaiGJZ0/hcuB24FoR2RP8uklE7haRu4PnfA7YLiL7gceBjxtjFkxN62tW55Pv9/LTl3SaqlIqOcRt8Zox5jlApjnnLPCaeLXhXLmcDm7bUsK9z1TT2DVAYUZKopuklFJxldRlLmLxpq2ljAYMD++uTXRTlFIq7jQoTKMiz8e25Tn8bNcZXbeglFr0NCjE4C0XL6OmtY97n63mySNN7K5p013blFKLUtIXxIvFjRcsIe93Hv7jD4dDxy6pyOHH774Ul1PjqlJq8dCgEIMUt5OnPnoNDZ39dA2MsOtUG//++8N85c9H+egNC2JZhVJKzQkNCjFK97pYUeAH4KJl2Zxo6uWbT57gkopcrloV31XWSik1X3TsY5Y+c8t61izx86Gf7qGhcyDRzVFKqTmhQWGWUtxOvvG2ixgYHuW9D+5mYFgL5ymlzn8aFM7BioJ0vvymjew508FH/nevTllVSp33NCicoxsvKOITN67ht/vq+a/HjiS6OUopdU400TwH7rpyOada+/jmkycoy/Xxpq2liW6SUkrNivYU5oCI8Nlb1nPFijz+5ZcHONLQnegmKaXUrGhQmCNup4OvvHkT/hQ3//DQy5p4VkqdlzQozKF8v5f/etNGjjb28PnfVSW6OUopNWMaFObYVavyefcVFfxoRw1/OtiQ6OYopdSMaFCIg4/euJr1SzP45C/20z0wnOjmKKVUzDQoxIHX5eT//vWFtPQM8Y0njye6OUopFTMNCnGyoSSL27aU8P3nTlHT2pvo5iilVEw0KMTRR29YjcspmnRWSp03NCjEUWFGCu+7ZgV/OtTI88dbEt0cpZSalgaFOHvXFRUUZ6Xyud9VaW0kpdSCp0EhzlLcTj78mlVU1XfxxOGmRDdHKaWmFLegICKlIvKkiBwSkYMics8k510tInuC5zwdr/Yk0us3LqU4K5VvPXUcY7S3oJRauOLZUxgBPmyMWQdsA94nIuvCTxCRLOBbwM3GmPXA38SxPQnjdjr4+6uW8/LpDnaebEt0c5RSalJxCwrGmHpjzMvB77uBKqB43GlvAx4xxpwOnrdox1fetLWUvHQP33rqRKKbopRSk5qXnIKIlAObgZ3jHloFZIvIUyKyW0TeMR/tSYQUt5M7L6/gmaPNHKjrTHRzlFIqqrgHBRFJB34OfNAY0zXuYRewBfgr4AbgX0RkVZRr3CUiu0RkV3Nzc7ybHDdv31ZGutfFt5/W3oJSamGKa1AQETdWQHjQGPNIlFNqgT8aY3qNMS3AM8DG8ScZY+41xmw1xmzNz8+PZ5PjKjPVzVsuLuUP++vpHRxJdHOUUmqCeM4+EuA+oMoY8+VJTvsVcIWIuEQkDbgUK/ewaG1fkUvAwMGz4ztNSimVePHcjvNy4HZgv4jsCR77JLAMwBjzHWNMlYg8CuwDAsD3jDEH4timhNtQkgXA3jMdXFKRk+DWKKVUpLgFBWPMc4DEcN6XgC/Fqx0LTV66l+KsVPbUdiS6KUopNYGuaE6ATaVZ7NOgoJRagDQoJMDG0kzOtPXT2jOY6KYopVQEDQoJsDGYV9hXq+sVlFILiwaFBLigOBOHwJ4zOoSklFpYNCgkgM/rYmWBX/MKSqkFR4NCgmwszWRvbadWTVVKLSgaFBJkY2kWbb1D1Lb3J7opSikVokEhQexk814dQlJKLSAaFBJk9RI/HpeDvZpsVkotIBoUEsTtdHDB0gz2ntFpqUqphUODQgJtKMlif12nVkxVSi0YGhQS6HUbihgaDfCBh15hNKCzkJRSiadBIYG2lufw6ZvX8/jhJj7320OJbo5SSsW1dLaKwe3byjjV0st9z52kIs/HHdvLE90kpVQS057CAvDJm9by6nWFfOY3BznZ0pvo5iilkpgGhQXA6RA+f+sFADy8+0yCW6OUSmYaFBaIgowUXrUyn1+8XEdAk85KqQTRoLCA3LalhLOdA7xQ3ZropiilkpQGhQXk1esK8ae4eHh3baKbopRKUhoUFpAUt5PXbVjKowca6NEFbUqpBNCgsMDctqWE/uFRfr+/PtFNUUolIQ0KC8xFy7KoyPPxcx1CUkolgAaFBUZEeONFxew82UZte1+im6OUSjJxCwoiUioiT4rIIRE5KCL3THHuxSIyIiK3xas955OrVxcAuoezUmr+xbOnMAJ82BizDtgGvE9E1o0/SUScwBeAP8WxLeeVlYXpOB1CVX1XopuilEoycQsKxph6Y8zLwe+7gSqgOMqp/wD8HGiKV1vON16Xk8p8H4fruxPdFKVUkpmXnIKIlAObgZ3jjhcDbwC+Pc3v3yUiu0RkV3Nzc7yauaCsLcqI2lPoG9Kpqkqp+Il7UBCRdKyewAeNMePvcv8NfNwYE5jqGsaYe40xW40xW/Pz8+PV1AVlbVEGZzsH6OwbDh2r6+hn02ce44G/nExgy5RSi1lcg4KIuLECwoPGmEeinLIV+ImInAJuA74lIrfGs03nizVL/ABUNYzF0eeONTM0GuDff3+YA3W6jadSau7Fc/aRAPcBVcaYL0c7xxhTYYwpN8aUAw8D/8cY88t4tel8sq4oA4DDYUNIL5xoJcfnIdvn5gM/eUWHkpRScy6ePYXLgduBa0VkT/DrJhG5W0TujuPzLgr5fi85Pg9VwWSzMYYXqlvZXpnLV960iZMtvXz2N7pbm1JqbsVt5zVjzHOAzOD8d8arLecjEWFtkZ/DweGjky29NHYNclllLttX5HH3VZV8+6kT3HRhEVeuSo48i1Iq/nRF8wK2ZkkGRxq7GQ2YUDnty5bnAvCh61fhcTr4y4mWRDZRKbXIaFBYwNYWZTAwHOBUay8vnGilwO+lIs8HgMfloDwvjRNNun2nUmruaFBYwOwZSIfOdrGjuo3LKnOx8veWyvx0qpt7EtU8pdQipEFhAbPLXfx231laegZDQ0e2yvx0atr6GBqJXOaxr7aDpu6B+WyqUmqR0KCwgNnlLh471AjAZZXjgkKBj9GA4XTb2BDSaMDwtu/u5L//fGxe26qUWhw0KCxwa4syCBhYmpnCspy0iMcq89MBOB6WVzjR3EPP4AjHm3RYSSk1cxoUFrg1S6xFbNuWR+YTAJYHg8KJsLzCvlprpfOpFk1AK6VmLqagICKVIuINfn+1iHxARLLi2zQFsG5pMCiMGzoCSPe6WJKREhEU9tdaezA0dQ/qimel1IzF2lP4OTAqIiuAe4FS4Mdxa5UKubwyl8/esp6bNy6N+nhlgY8TzWO9gv11ndgdilMtunObUmpmYg0KAWPMCFaZ668bYz4KFMWvWcrmcjp4x2XlpLidUR+vzE+nuqkHYwwjowEOnu3i0oocAE616hCSUmpmYg0KwyLyVuAO4LfBY+74NEnNRGV+Ot2DIzR3D3KsqYfBkQA3b7T2MtKgoJSaqViDwp3AZcDnjTEnRaQC+FH8mqVitaIgOAOpuYf9wSTztuU55KV7NdmslJqxmAriGWMOAR8AEJFswG+M+UI8G6ZiUxmagdTLkYYu/F4X5bk+ynPTNKeglJqxWGcfPSUiGSKSA7wMfFdEou6RoOZXYYYXn8fJiaYe9td1cUFxJg6HUJ7nmzB89PXHj/HdZ6pn9Tx7z3Rw+307GRgenYtmK6UWqFiHjzKDW2n+NfBDY8ylwPXxa5aKlYhQWZBOVX0XVfVdbCjJBKAiz0dT9yC9g9a01JHRAPc+U80Dz5+a1fM88nItzx5r4VCUfaOVUotHrEHBJSJFwJsYSzSrBaIyP51dNe0MjQS4oNgKCuW5VjVVu7dw8GwX3YMj1HX009w9OOPn2FHdBsCRhu45arVSaiGKNSh8FvgjcMIY85KILAe0uM4CUZlv1UACQj2FslyrJEZNq5VXeP5Ea+j8PWc6Jr2WMWZCgb3WnkGONFrBQIOCUotbTEHBGPO/xpgNxpj3Bn+uNsa8Mb5NU7Gyk80ZKa5QfaTy4L4LJ4MzkJ4/0UJZbhpOh7DnTPuk1/rO09Vc/aUnIwLDiyetXoLP4wztBKeUWpxiTTSXiMgvRKQp+PVzESmJd+NUbCqD01I3lGSF6iOle13k+61pqUMjAV461cY1qwtYs8Q/ZU9hd007ZzsHeOpIU+jYjupW0jxOXnthEUcaujHGxPcFKaUSJtbho+8DvwaWBr9+EzymFoCy3DRS3U4uKsuOOF6em0ZNax97znQwMBxge2Uum0qz2HumMzTcNN7JFquO0i/31IWO7ahuY0tZNuuXZtDeNzyrnIRS6vwQa1DIN8Z83xgzEvx6ANDd4hcIr8vJ7z5wBe+9qjLieHmuj5OtvTx/ogWHwKXLc9m8LJuewZGIInq2kdEAp9v6cDuFP1c10TUwHMonbFuey+rgTnCHNa+g1KIVa1BoFZG3i4gz+PV2oHXa31LzZnl+OqmeyPpI5Xk+mrsHeexQIxcUZ5KZ6mZTqVXcds/piUNIZ9r7GR41vPniUoZGAjy6vyGUT9i2PDdUxluTzUotXrEGhb/Dmo7aANQDtwHvnOoXRKRURJ4UkUMiclBE7olyzt+KyD4R2S8iz4vIxhm2X03BnpZ68GxXaNe25Xk+/CkuXomSV7D3e37D5mIq8nz84pU6dlS3kup2sqEkkxyfh3y/V3sKSi1isc4+qjHG3GyMyTfGFBhjbgWmm300AnzYGLMO2Aa8T0TWjTvnJHCVMeZC4HNYZbnVHCnPG9upbXtlHgAOh7CpNCtqstmeqbQ8L51bNxWz42QrfzzYyNbybNxO65/KmiV+jjTqDCSlFqtz2XntH6d60BhTb4x5Ofh9N1AFFI8753ljjD0/cgegM5rmkN1TcDmEi8vHktCbSrM40tA1YROeE829ZKe5yfZ5uHXzUoyBhq4Bti0f2+BndaGfY409kyaqlVLnt3MJCjL9KcETRcqBzcDOKU57F/CHc2iPGsfndVHg97J5WRZpnrHah5tKswgYQlVVbdXNPaEtPstyfVy0zMo/RASFJX4GRwJallupRepcgkJMHxVFJB1r57YPBusnRTvnGqyg8PFJHr9LRHaJyK7m5ubZtjcp/fsbLuSTN62NOBZKNo8bQqpu6aUiuOgN4N2vWs6GkkwuDJbOACZNNp9o7uGLjx7m5m88x7HG+OUcfrWnjg//bG/crq9UspuydLaIdBP95i9A6nQXFxE3VkB40BjzyCTnbAC+B7zWGBN1RpMx5l6C+YatW7fquMUMXL+ucMKx3HQvpTmpEUGhe8Baf7A8fywo3HRhETddGLnB3srCdESsaak3XVjE6dY+/vFne9hV047TIYwGDI8eaGBloT/i937xSi2XLc9jSWbKOb2eP+xv4I+HGvi3Wy+YMNtKKXXupuwpGGP8xpiMKF9+Y8x0AUWA+4AqY0zUMtsisgx4BLjdGHN0ti9CzdzFZTm8UN3KyKhVziI8yTyVFLeT8lwfRxq6aO4e5Pb7d3KsqYdP3rSGFz5xLasL/ew+HVlG40xbHx/66d5ZV2gNd7qtD2PG2quUmlvnMnw0ncuB24FrRWRP8OsmEblbRO4OnvMpIBf4VvDxXXFsjwrz6nWFdPQN8+Ipax1CdbN1k60M6ylMZnWhnwN1Xdz5wIs0dQ3y/Tsv5q4rKynISOGismxermknEJaI3lFtdQCPnuOwkjGG021Wgb/jURbfKaXOXUw7r82GMeY5pklGG2PeDbw7Xm1Qk7tqdT5el4M/HWxke2Ue1c09OASW5aZN+7url/h59GADjV0DfPeOrVy0bGxm05aybB568TQnmntCQ0gvzFFQaO8bpie4P8SJJg0KSsVDPHsKagFL87h41cp8HjvUiDGG6pZeSrLT8LqmH6e/qCwbEfjibRu4ZnVBxGNbgvWXdtdYQ0jGGHYG92Kobe8PbfozG3YvAbSnoFS8aFBIYjesL6Suo58DdV1UN/dGJJmncuXKPPb8y2v464smLispz00jx+dhVzAonGnrp66jn8tXWNNaj5/DJ/ya4DTYkuxU7SkoFScaFJLYdWsLcQg8erCeky290yaZbSJCZpp70scuWmblFWAsn/COy8qBcxtCOhPsKVyzuoCTLb26gE6pONCgkMRyfB4uqcjhpy/V0j88SkWMPYXpbCnLprqll7beIV6obiUv3cN1awrwuBwcO4dP+Kfb+sj3e7mgOIPBkQB17f1z0l6l1BgNCknuhvVLaOmx9keozJu7oABWXmFHdSuXLs/F5XRQmZ9+Tj2FmtY+ynLSQjvNRSv/rZQ6NxoUktyrwxa32SUuztWGkkxcDuEXr9RS3zlWO2lVYTpHz6HC6pm2PpaFBYVzyU8opaLToJDkSrLTuKA4gzSPk8IM75xcM8XtZH1xJn840ADAZaGg4Ods5wDdA8MzvubgyCj1XQOU5qSR7fOQ6/Occ0+ho29ItxZVahwNCoqPvGY1H3nN6tD+znNhy7JsjIF8vze0IG5VcN3CZHmFE8091Lb3RX2str0fY6ytRwEq89PPKSjUd/Zzyecf50+HGmd9DaUWIw0KiqtXF/B3V1TM6TW3Bkt1b1ueGwo2qwqtYZ/JCua9/8ev8PGf74v6mL1GYVlOMCgUpJ/T8NHeM50MjQbYFVzRPZn23iEGhkdn/TxKnW80KKi42Fqejdfl4Lo1Y4vbSrPTSHE7ONo48WYeCBhONPewu6ad4WA9pnD2dNRloZ6Cj/a+Ydp6h2bVvqp6q2DvVLvIGWN4/Tee48uPaVkulTw0KKi4KPCn8OInr+eWTUtDxxwOYUVB9BlI9V0DDI0EGBgOcPDsxArrNa19pLgd5KdbeY/KgnNLNh9usJ7DDg7RtPQMUdvez4G6zknPUWqx0aCg4iYzzT0hT7GqwNq5bbxTYVVPow3pnA7OPLKvt+Icp6VW1XcjYt34m7sHo55j7xlxSiuyqiSiQUHNq1VL/DR0DdDZHzkDyS6F7fe62HWqfcLvWdNRx9ZRFGelkuJ2zKrcRffAMKfb+theac2Kmqy3cCTYoznbOaB5BZU0NCioeTVZsrmmtRevy8H16wrZVdMWMVXULpltJ5nBGopanpc+q8J49vDVrZusLcPtoaQJ54XlG8KL8Sm1mGlQUPNqZYE1LXV8svlkSx9luWlcXJ5DS88Qp1rHbsItPUP0DY2GpqPaKgtmNy31UL11s9++Io/CDC+H66Mnmw83dpMVrPGkm/qoZKFBQc2r4qxUfB7nhE/nNa29lOX6uDg4lTU8r3C6zbohh/cUADYUZ3KmrZ9fvlI3ozYcru8iI8XF0swU1hZlcCjK8FEgYDjW2M11a6wV35pXUMlCg4KaVw6HcGFJJntrx2b0BAKGmrY+KvJ8VOank5Xmjsgr2EM3peOCwh3by9m2PIePPbyPl6ZZbxCuqr6LNUUZiAhrlmRwormHoZHIabB1Hf30DY2ytTybXJ+HU60aFFRy0KCg5t3G0iyqznYxOGIlb+3pqOW5PhwOYWtZNi/VhPUUWvsRsfZRCOdxOfjO27dQkp3KXT/cFdpvYSqBgOFwQzfrijIAWFvkZ3jUTBiGstcvrF7ipzzPp8NHKmloUFDzbnNpFkOjAarqI6d8lgdzBlvKcqhu7qW1Z5D+oVGePtpEUUYKKe6Ju8JlpXm4/50XA/B3D7zESJSFb+HOtPfRNzTK2iIrt7E2GBzGD2fZyeiVBemU5/o41aKJZpUcNCioebexNAuAPaetISJ7aKY8WLrbzis8friJt9+3k1fOdPChV6+a9HrleT4+csNqTjT3crZjYMrntqefrlliBYPleT48TseEZPORhm6Ks1Lxp7ipyEujoWuA/iGdlqoWPw0Kat4VZaZSmOEN5RVOtVjTUZdkpABwYUkmHpeDT/x8H/trO/nm2y7ib7aWTnnN8lwroJztnHrjnar6bhwyVpzP5XSwsjB9QrL5SEM3a5ZY55QFr615BZUMNCiohNhYksWeMx3A2HRUh8Narex1Odlalk2ax8UDd17MTRcWTXu9okwroNSPCwqHG7rY+m9/5ld7rBlKVfVdlOf5SPWMDUWtLcqIqIE0NBLgRHMPq4JBoSLYg9EZSCoZuOJ1YREpBX4IFAIGuNcY89Vx5wjwVeAmoA94pzHm5Xi1SS0cm5Zl8adDjXT0DVHT2hu68dq++pbNBIyhMNh7mE5RppWEHj98tPdMBy09g9zzkz3Udw5Q1dDFhpKsiHPWLPHz8O5aWnoGyUv3crKll5GACfUU7GGtk9pTUEkgnj2FEeDDxph1wDbgfSKybtw5rwVWBr/uAr4dx/aoBWRT8Mb8ypkOatr6QjdeW77fG3NAAEj1OMlOc3O2I7KncKatH4fAX20o4j/+cJgzbf2sDd7sbfZMJDuvYJe3sIeY0r0u8tK92lNQSSFuQcEYU29/6jfGdANVQPG4024BfmgsO4AsEZl+rECd9y4syUQE/nigITQd9VwVZaZS3xnZU6ht76MoM5Wvv2Uzf3/lcgC2ludEnLO2KAOnQ/jGk8foHhjmaEM3ToewPH+sTRV5aToDSSWFeckpiEg5sBnYOe6hYuBM2M+1TAwcahHyp7hZkZ/O7/fXA2PTUc/F0qyUiT2F9n5Kc1JxOIR/umktu//5+tCe0bZsn4f//JsN7DrVzpv/Zwc7qltZnufD6xrLO5Tn+nT4SCWFuAcFEUkHfg580BgzefH6qa9xl4jsEpFdzc3Nc9tAlTCbSrPoGhgBmDB8NBtLs6L3FEqyxwJObnr0fajfsLmE+995Madae9lV0x5KMtvK83w0dw/SMzgyq7YZY6ZdQzGX2nuHZrUXtlJxDQoi4sYKCA8aYx6JckodED7XsCR4LIIx5l5jzFZjzNb8/Pz4NFbNO3u9Qvh01HNRlJlKZ/8wvcEb98DwKI1dg5Rmx9YLuXJVPj+5axtluWlcs7og4rFznYH0yMt1bPu/j08opxEvt9+/k396ZP+8PJdaXOIWFIIzi+4DqowxX57ktF8D7xDLNqDTGFMfrzaphWVTMCiET0c9F0uzIqel2kNJ48tjTGVDSRZPf/QabttSEnG8/BzXKlTVd9HSM0RLT/QNfeZSTWsvB+q6QluYKjUTcZuSClwO3A7sF5E9wWOfBJYBGGO+A/weazrqcawpqXfGsT1qgVm9xI/X5ZiTJDNETktdUeDnTLsVFMYX0puN8jzrGrPtKTQHg0Fz9yBLs2IPUrPx2KFGwCo5rtRMxS0oGGOeA6b8+GesnVTeF682qIXN7XTwuVsuoCJ/roJCZE+htt36pDyTnsJk0jwuCjO8VDfPMigEt/ycj56CHRTaejUoqJmLZ09BqWm96eKpy1fMxJLMFETGFrCdaevH7ZQZrXeYygVLM0OrsGdqvoJCe+8QL51qw+910T04Qt/QCGke/d9cxU7LXKhFw+10kJ/ujegpLM1KxTkH+QqASypyqG7ppalr6qJ70djDR/Ee0nnicBMBAzdvWgpAqw4hqRnSoKAWlaKwaaln2vtjnnkUi0uD6xtenMGGPgCDI6N09FnTQ+0eQyxOtfRG7FUdiz9XNVKY4eXq4Oyp83kIaWgkMOspwGr2NCioRaU4K4W64Kyjuva+Ockn2NYvzSDN4+TFkzMLCuGf1ptjHD7aUd3K1f/5VCg/EIuB4VGePtrM9WsLyUv3WM/dG/8cRrx85c9Hue3bzye6GUlHg4JaVIoyU6nvGKBvaISWnqE5mXlkczsdbCkrbQfrAAAgAElEQVTLnnFQCO8dtMTYU/j6E8cAOFDXOc2ZY1440Urf0CjXrysk12ct0puP4aPBkVE+/vA+TrfGPgX2xZNtob0tJnO6tY/q5pn3ltS50aCgFpWizBT6h0c5eNa64cxlTwHg0oocDjd00z6DYRk7KBRnpcaUaH7ldDt/Od4KwNHGnmnOHvNYVSM+j5PtlbnkBHsK8zF8dPBsFz/ddYYnjzTF/Dsfe3gv7/7BLgaGJ9+4qGtgmKHRAL26udG80qCgFhV7DYD9ab5kDnMKAJdUWHmFl2aQV7CHjNYWZcSUaP7mk8fJSnNzxYo8jjV1T3s+WENHfzzQwNWrC/C6nPg8TrwuB63zEBSON1mBq6k7tgR8IGCo6+inrqOf+547Oel5Xf1WHmYmAVidOw0KalGx1yrYQaF0jnsKG4K7woUPIX37qRN88dHDk/6O3VNYW+Sns3+YwZHJP/lW1Xfx56om7txewcbSTE619k15vu0Xr9TR2jvE325bBoCIkOvzzMvw0YlgUIg1id7cM8jwqCHV7eRbTx6f9Pfsuljnc7L8fKRBQS0qdk9hd007XpeDfH/0AnizleJ2srk0i53BoLCjupUvPHqY7z17ctKZMs3dg2SluUNtm+pG/c0nj5PudfHO7eWsKvQzGjDTluwOBAzffbaaC4szuSysAmxuundeEs3HZhgUaoMrzT9yw2oGRwJ8+bGjUc+zewoaFOaXBgW1qOSle3E5hJ7BEYqzU7FKcM2tSytyOHi2k8auAT768F78XhdDowGePRq9gm9z9yD56V7yghVaJ8srnGzp5Xf767n9sjIy09ysKEgH4Gjj1ENITxxuorq5l/dcuTzi9eb4PPNyQ7WHj2KdWWXXpLpiRR7vuKycn750msMNkUlnYwxdAxoUEkGDglpUnA5hSXAIaS7XKIS7pCKXgIE77n+R2vZ+vnvHVjJT3TxWFX36aHPPIPl+b2ia6GRB4YG/nMTtcPB3l1cAUJmfjkPGPolP5t5nqinOSuWmC5ZEHJ+P4aOB4VHOBMuJxNpTsIPC0qwUPnDdCvwpbr7++PFx1w0wPGrNOmrv06Awn3T9u1p0lmamUtveP+czj2wXlWXhcgiHG7q568rlbFuey7VrCnjycBOjATNhBXVz9yAXLcsa6yl0T7zJdQ0M8/DuWl63sSg05JXidrIsJ43jYcnmgeFRbvras5Rkp/Gh61cC1mK6f3ndOlzOyM94uemeuA8fWVNGrUq3te39BAJm2oq3dR39ZKS48Ke4AdhSlj2h+mxX2F4Q2lOYX9pTUItOUbCE9lyuUQiX5nGxtTyblQXp/OOrVwFw/dpC2vuGefl0e8S5xhhr+MjvDd3sow2z/O+uWnqHRrlze0XE8ZWF/ohpqTuqW6lu7uXFk6284VvP864f7MKf4uLNUWpI5fi8DAwH6Buam1XBL55s44ovPBGxHuF4s9W2y5bnMhowtMXwqf5sR39EpdisNHdoxbfNzifA7HsKM61o+6eDDboPNxoU1CJkl9COV08B4N53bOWR/7OdFLe1ZeeVq/JwO4U/j1uB3Ds0Sv/wKPl+LyluJ36va8Iwy2jA8IPnT7G1LJsLSzIjHltZkM6plt7Q5jyPVzWR6nby/Ceu4+M3rsEh8PdXLifdO7HTn2uvap6DIaS+oRE+8r97qW3v50+HGkLHjzf14BCrLhTENoRU295PcVhQyE7z0DHuxh/eU5hN+w/UdXL1fz4V89RhYwz/8NAr/M8zJ2b8XIuNBgW16Nib7cQrpwCQkeIODX+Atef0tuW5E/IK9k3S7iXk+b0TcgpPHG7idFsfd14e2UsAWFXoZyRgONVqrex94nATV6zMI8fn4b1XV7Lrn1/N+69dGbWNuT671EX0m+qPdtTwxm8/z2hg+hXDX3z0CKfb+shKc/PssZbQ8RNNPZTmpIV6ZbEEhbMd/RSHBeysVDe9Q6MRu9J19Vu9mxS3Y1Y9hRPBHszhaVZN21p7hxgcCYRmRiUzDQpq0Xn1ukJu31bG2qKMeX3e69cWUt3cS3Xz2HCPXVE1P90KVHnpnglB4YHnT1KUmcJr1hdOuKY9A+lYYw9HGrup6+jnujUFE86LJsdnr2qOfqPeWd3K7pr2aesrvXiyjR+8cIo7Livj1k3F7DzZGlqJfLyphxX56RTYQ2PTBIXugWG6BkYih4+C7QzvLdg9hfJc36xyCg3Boog1MZbeqA+WW9egoEFBLUJFmal87tYL8Ljm95/3dWutm/XjVWPlHuz8QainkO6NWNV8rLGbvxxv5fbLynA7J7Z3RUE6Ita0VPu618YYFMamwEa/qTYGA9b9f5l8VXH/0Cgfe3gvJdmpfOzGNVyxIo+B4QAv17QzMhrgZEsvKwrSQ8813bRUe6+LyOEjq8fVEZZHsHMK5bk+2sflG2LREHxtNTFuSWqXW6/rsJLlyUyDglJzpCQ7jTVL/BFDSBOGj9K9EZ+mnzpirW1440WRe0LbxmYg9fB4VSMbSjIpiHHToLGeQvSg0NA1gMdprc6erPDeowfrOdXax+dvvRCf18W2ylxcDuHZ4y2cae9naDRAZUE6Pq8Ln8c5bU9hbDpq+PCR1c7wchb2auay3DQ6+oZiGuKKeG2hnkJsiWO73PrQSGBedsdbyDQoKDWHrltbwO6adroHxvZPcDmErFTr03BeupfO/uHQ+PnumnaW5aRNuTvcygI/u2raeOVMR8y9BIA0j5MUt4PWKDc5YwyNXYO8cUsJaR4n3//LqajXON7Ug9MhXFZprZRO97q4aFk2zx1rCS1as4e48v1emqYJCnZZ8+Jxs4+AiB5BV/8wXpeDJZkpBAx09s+st2Df5E+39cVUZfVs59iwUW1H5BBS/9DolIX7FhsNCkrNoStW5DMaMLxwwqpy2tw9SF66NzR33+4xtPYOYoxh9+l2tpRlT3nNlYXpNHYNYoyVt4iVVf/IGzXR3N5nBaZVhenctqWE3+w9G/VT/qmWPkqzUyOGtq5YmceBs53sCs7sCQ8KzdMUxavrsLZILQgrP5Id7NF09kfmFDJS3VP2doZGArxyup1noqwkb+gcwOkQBoYD0wYq+3x7ecX4vMJ7friLjz68b9prLBYaFJSaQxeVZZHmcfLccWuGjr2a2RZa1dw9RG17f2hh21RWFVo33cIML+uXzix5PlmpC3sMfUlGCu/cXs7QaIAHd9ZMOK+6pZeKPF/EsStW5mEM/HTXGQozvGQEZ2FZQWH64aMlmSkRC9zsXlRkT2GEjBQX2WnBoaWwJPSBuk7e+O3nueDTf+QN33qed9z/YsTaiZHRAM09g1wQ/FvFkmyu7xhg/VJrOnBdWFAwxvDy6Xb21c5ub+7zkQYFpeaQ1+Xk0oocngtO27QXrtny/GP1j+yFbhdN11Mo8ANw7ZrCGddyyk2PXurCTjIXZqawPD+da9cU8P92nI5IshpjONXSS0VeesTvbijOJCPFRUffcKiXAFDgT5k2KNSNW6MA1jCXxxk59XSqnsJv99Wz90wHd1xWxoeDiwdPtIzN+GrpsXIQ9vap41dLR3O2s5/KfB/ZaW5q28eCSF1HP31Do5xp64uYMruYaVBQao69amU+1S291Lb3hYrh2ezvm7sH2V3Tjs/jZHWhf8rrrSr0c8umpdy+rWzGbZmsp9DQad28lwRzGTeuX0JLzyCnw2brNHYN0j88SkVe5HoPl9PB9so8AFbkjwWFfL+XroGRKcffx69mBmuYKyvNTee4nEJGSvSgcKqll2W5afx/f7WOt166LHTMZveCtpRl43TItDvCBQKGxq4BlmSmUpKdFjF8ZNedChgi/jaLWdyCgojcLyJNInJgksczReQ3IrJXRA6KyJ3xaotS8+lVK60b5tNHm2ntHRo3fDQ2dXN3TTublmVNqFk0nsfl4Ktv2cy6GQ4d2c/X0jM4Idna0DWAyFiOw17TcShssVd18NP3+J4CWENIQERPIX+aKrAjowEaugYm9BTASjaH9xS6B0bISHWHho8igkJrLxW51pBWrs+D3+uKCAr2zKPS7DSWZqVMOy21pdfa32FpVgrFWamhZDjA8bASIyfjVAKje2CYf3pkf8ybFMVbPHsKDwA3TvH4+4BDxpiNwNXAf4mIJ47tUWperChIpzDDy6/3nGU0YCKCQqrHSbrXxenWPqrqu9iybOqho3OV4/MwOBKgb9yWlo2dA+Sle0MJ5JWF6TgdErFvsr2PQ0V+ZE4B4DXrCtlYksnlK/JCx+zXOVlit6FrgIBhkqDgiah/1DUwTEaKi1SPk1S3MzRd1RhrdXdZMCiICGV5aZwK6w3YaxSWZKZQluPj9DTDR/bCtaLMVEqyU6ltH5uxdLSxG5/HKmVysiWyWq0xZsZTZaP52a5aHnrxdMT6lkSKW1AwxjwDTFV4xAB+sQZJ04Pnzk3lLqUSSER41cr80EY84zf6yUv38MSRJgJm+nzCuZps9k5D10Bo6Ais9RAr8tM5dHYsKJxs6cHrclAUZbpsQUYKv3r/FSwfN3wEk69qtheujR8+AmsBmx0UjDF09Y+Eyojk+DyhQnuNXYMMDAcihrTKc30ReYOGzgE8LgfZaW7KciMDRjT2cFNRZgrF2akMDAdCf69jTT1sKMkix+eZ0FP4/O+quPDTf+RDP93DU0eaGBmdec7BGMNDL562nmsG+3HHUyJzCt8A1gJngf3APcaY5MjkqEXPHkKCaEFhbJbO5jj3FCbbw6Gxa2DC2oi1Rf6I4aOTLb2U5/qmLYVtixYUDp3tCq0xqOuwbs7FUQoVZqV6QsNHgyMBhkYDZKRaRf5yfJ5QT8G+MZeHzYgqz/VR297PcPCmXN85QFFmitWLyE2js384Il8xnr2moSgzJbSnd217P8YYjjf1sKownYo8H9XNkUHh6aPNpHtdPF7VyDu//xJvuXdHTH+ncLtq2jne1IMI0+7Hfcf9L4YCSDwlMijcAOwBlgKbgG+ISNRBUxG5S0R2iciu5ubou1sptZBEDKukTwwKYE01zUx1E085Puu5ovYUMiPbtW5pBvWdAxE34PHTUaeS6/MgMhYUOvuGufWbf+Ft391B/9DoWE8hM0pQ8Lnp6B8O9hKsG7g91TU7LFlur1Auzw0LCnk+RgMmlCBu6BzrBS3Lsc6raZt8CKm+cwCvy0GOzxOqrFvX0U995wA9gyOsKPRTkeeL6Cl0DwxzvLmHv720jJf++XruvLycXTXtEauyY/Hjnafxe13cuH7JlD2FnsERnj7aPC97SyQyKNwJPGIsx4GTwJpoJxpj7jXGbDXGbM3Pz5/XRio1G3npXtYFk7cTegp+69P7dIvW5kKoUmrYtNSB4VE6+oYjho8A1hVZ8/Sr6rsYGQ1wuq0v4hP5dFxOB7k+T6j+0VNHmxgaDXDwbBcffdgqu53j85AaHKMPl53mYWgkQP/waKgYXkYwYOakuUPDRydbe/E4HRFDUPZQkp1stgKe9drKcq3HplqrcLajP9SzsHsxte19oZlHqwqsnkJT92BoH+79dZ0YAxtLM/G6nFy1yrovTbd1ariOviF+t7+eWzcXs6Eki4augUlXbtszqMKDYbwkMiicBq4DEJFCYDVQncD2KDWnXnvBEoqzUvGN2+vA7ilcFOehIwjbUyHsE2ZojUKU4SOwZiCd7RhgeNSwfAZBASKHxh471EheuoeP3bia3+6r5xev1EZNMkPkArbOYNnsjBTr75bt89Dea90sa1r6KM1Jjdjdzk462+XFGzqjBYXJewrh52ekuMlIcVHb3s+x4A1+ZaE/9HewA8/eM1atqI0l1sLDVcFpxTMJCj9/uY6hkQBvvWRZaIHi8Um2XrXbb7+eeIrnlNSHgBeA1SJSKyLvEpG7ReTu4CmfA7aLyH7gceDjxpiWya6n1Pnm/1yzgsc/fNWE4yXZaYjAxeU5cW9DmsdFqtsZUT7bHkO3b4S23HQvhRleDp3tGpuOGmXm0VTs+kfDowGePtrMtWsKeO9VldyyaSkDw4HQXhfjZaWNlc8e31PI9XnoGRxhcGTUmo46LlCFT0tt6x1iaDQQSo6neVzk+71T9hTqOwcihrSKs9Ooa+/nWGMPuT4POT5P6O9QHQwK+2o7KMtNC5XoKMpMwe91ReySNxU7wbypNIt1SzNCCxSPTRJU7Gm1y+YhKMRtj2ZjzFunefws8Jp4Pb9SieZ0CE7HxKGSmzcuZXWhf0ZDM+cixxe5qtnuKYwfPgJYV5TBofqu0A5wMx2uyPd7qW7u5aWTbXQPjHDdWmsV9hfeuIHewRGuWxO9dlOofHbfcNScAlh5kVOtvRH5Goiclhot4JXlpE26VmE0YGjoGght4QrWjn2nW/to6xtiZfATvP13ONls9xQ62BoW1EWEVUv8HBl3U3/2WDMf+ukeRgMGR3A1+sCwtRtfwMAXb9sQes5UtzM0ZDVeTWsvOT5P6G8ST3ELCkqp6Dwux4RtN+MpN90TMXzUMElPAaxFbM8ea+FoYzd+rys0eylWdv2jx6oa8bgcoVlYKW4n37vj4kl/LyusxpFdNjs0+yj42OH6bgaGA1GDaXmuj/11nWGvbeyT/7LcNJ4/3hr1eVt6BhkNmIjzi7NSef54Cw4Rbt1cHGp/cVYqJ1t6aOoa4GznABtLI2tWrSpM5w8HGjDGhMqR/H5/A/1Do7xxSwkBYzDGulaq20lWmptbN1nXdziEFQXpkw4/1bT2sSxOe46Pp0FBqUWuOCuVfbWdoZtVQ9cAPo8zYjtR27qlGYwEDI8daqIi3zfjWkv56V6GRgP8Zm892ytzSfPEdouJpadg14qqiNJ7Kc/18YcDDaG6RUVhAa8818cjL9cxMDwa2lPbFtrfITOyp9AbXOxnj/UDLM+3ZiDtrbXyCZtKIwP7qkI/D714huaeQQr81vVeOtXGJRU5fPaWC6b9G6wsSOf5E9GDV01rHxeXxz8HBVr7SKlF71Ur86nr6A8NTTR2DVAYpZcAhGZMtfQMzmqmi70BUEvP4IzKfGeGgoKVU/C4HKEbuL0A75XTVqXSaMlWe1rqSzXtOB0SSuaHn/9yTTvPHmvmRztqQlNHx9YojPUUSsL29l5RMFaXqiLPR3VLL3vOWM9hz9ay2TWsjjZYf+e23iGON/VwcUVsuaOVhX4augZCORXb4MgoZzv7WTYPM49AewpKLXrXrLGmSz55uIlVhf6IefzjleX6SHU7g4XwZn4TCl+TYW9PGguvy0max0l73zB9Q6MRY+d2UNhzpmPCdFSbPS11Z3UrBX5v1NlJb/veztCxXafa+OpbNkcsXLOVhC2uWxnWU6jI89E9MMITh5tZXeifMLV2ZdgMpCtW5oX2m4h1QsHKsP24w6crWwvpoHweksygPQWlFr2izFTWLPHzxGGrtk5j1+CkQcHpENYEp6Yun+HMIxhbk7F+aUbEp+9YZAfrH1lls8c+r9rTVXsGR1iWmxZxw7fZN/6WnqEJuZILizP5yGtW8enXr+PH77mU97yqgl/tOcvumnbqO/pJcTtCu7/BWFDI8Xkiehx2kKyq75qQTwBr9XiOzxPKC+yqacfjcrAhxvyRPa31+LiVzfM5HRW0p6BUUrh2TQH/80w1nX3DUw4fgTWE9MrpjlkNHxVmeHE7hRvWL5nx72amuunos6aUhvcUXE4HmaluOvuHJ22TPS21e3Ak4lM/WIHu/deuDP28sSSLX+89y2d/e4jirBSWZqZG5E4yU934PM6ICrAAy8OqxY7PJ0BwBlJhemgG0osn29hYYi1ui0VJdiopbseEaa32dFp7dXa8aU9BqSRwzZoCRgOGX+6pYyRgJu0pgFW3KdfnmXBTjIU/xc2v338Ff3/V8hn/bnaw1EVXsGx2OHsIabIhFHtaKsCSjKl7KD6vi4/fuIa9Zzr486GmiOmo9rXeuKWEmzcujThenJ2K22kFj2g9BbA+7R9r7KFvaIQDdZ0zWotiz0AaPy21prUPn8c545lgs6VBQakksLk0i8xUd6ig2vjVzOFuvKCI3f/y6gkrsWO1tigj5k/H4bLSrKJ43f3DodXMNnt20lRrO+xexPiaTtHcuqmYjSWZDI0GogaRz95yAW8ft6mR0yGU5fpI8zhDi83GW1Xop2dwhN/vb2AkYGa8QHFVgX/CAraa1l6W5c58JthsaVBQKgm4nA6uWpXP4QbrhhNtjUKiZaW6w3IK43sK1o1+quT3WFCYPpfhcAifev06IDKxPJ0rV+bzmnWFUfMaMJYXeHBnDSIzL42+ojCd+s4BusNmINW09c1bkhk0p6BU0rhmTT6/3nsWiL6aOdGsRPMQTodMWLmb47N+nirZavciYn1tW8py+PG7L2VNUew72tmBZDL2uoZXTnewZol/xlVwV9nlLpp6uGhZNqMBw5m2Pl69LvbpvedKg4JSSeKqVQWIgMC8jU/PRFaam4CBwKiJmH0E1taaWWnuqGW3bTesL6SufRWbl0Uf749m+7iSGecqK81DYYaXxq5BLolxfUK41UusoLDrVBsXLcumvrOf4VEzL9VRbTp8pFSSyPF52FyaRYE/Zdp9oRPBLnUBTOgpvOfK5Tx6z5VTbvjjT3Fzz/UrQ1uMJoo9hLR1FgUPS3PSuKQihx88X8PwaCBUMrtsnkpcgAYFpZLKp16/ns/esj7RzYgqO2ytgH9cojnF7VyQeZBo7KAw27IUd1+1nLqOfn63rz60leh8VEe16fCRUklk0yRTKReCiJ5CnHeki6d3bi9ndaF/xov3bFevKmBlQTr/80w1V67Mw+N0zPpas6FBQSm1IISvKp6PEtHxUpqTRuk5DPc4HMJ7rlzOxx7eR1vvICXjNhWKNx0+UkotCNlhPYXM1OT+vHrLpqWhhPV8JplBg4JSaoEIn755PvcU5oLX5eTOyysA5m0fBZsGBaXUgmCtT7B6COdzTmGuvO3SZVTm+9i2PHdenze5+2hKqQUl2+dhYDiA16WfVzNS3Dz+4avn/Xk1KCilFoysNA+9gyPzVudHTaThWCm1YGSluqNuE6rmj/YUlFILxruuqKC9byjRzUhqcQsKInI/8DqgyRgTdddqEbka+G/ADbQYY66KV3uUUgvflavyE92EpBfP4aMHgBsne1BEsoBvATcbY9YDfxPHtiillIpB3IKCMeYZoG2KU94GPGKMOR08vylebVFKKRWbRCaaVwHZIvKUiOwWkXcksC1KKaVIbKLZBWwBrgNSgRdEZIcx5uj4E0XkLuAugGXLls1rI5VSKpkksqdQC/zRGNNrjGkBngE2RjvRGHOvMWarMWZrfr4mopRSKl4SGRR+BVwhIi4RSQMuBaoS2B6llEp68ZyS+hBwNZAnIrXAv2JNPcUY8x1jTJWIPArsAwLA94wxB+LVHqWUUtOLW1Awxrw1hnO+BHwpXm1QSik1M2KMSXQbZkREmoGaWf56HtAyh805XyTj607G1wzJ+bqT8TXDzF93mTFm2qTseRcUzoWI7DLGbE10O+ZbMr7uZHzNkJyvOxlfM8TvdWtBPKWUUiEaFJRSSoUkW1C4N9ENSJBkfN3J+JohOV93Mr5miNPrTqqcglJKqaklW09BKaXUFJImKIjIjSJyRESOi8gnEt2eeBCRUhF5UkQOichBEbkneDxHRB4TkWPB/2Ynuq3xICJOEXlFRH4b/LlCRHYG3/Ofiogn0W2cSyKSJSIPi8hhEakSkcuS4b0WkQ8F/30fEJGHRCRlMb7XInK/iDSJyIGwY1HfX7F8Lfj694nIRbN93qQICiLiBL4JvBZYB7xVRNYltlVxMQJ82BizDtgGvC/4Oj8BPG6MWQk8Hvx5MbqHyFIpXwC+YoxZAbQD70pIq+Lnq8Cjxpg1WHXDqljk77WIFAMfALYGN+9yAm9hcb7XDzBxT5rJ3t/XAiuDX3cB357tkyZFUAAuAY4bY6qNMUPAT4BbEtymOWeMqTfGvBz8vhvrJlGM9Vp/EDztB8CtiWlh/IhICfBXwPeCPwtwLfBw8JRF9bpFJBO4ErgPwBgzZIzpIAnea6xKDKki4gLSgHoW4Xs9yZ40k72/twA/NJYdQJaIFM3meZMlKBQDZ8J+rg0eW7REpBzYDOwECo0x9cGHGoDCBDUrnv4b+BhWHS2AXKDDGDMS/HmxvecVQDPw/eCQ2fdExMcif6+NMXXAfwKnsYJBJ7Cbxf1eh5vs/Z2ze1yyBIWkIiLpwM+BDxpjusIfM9Z0s0U15UxE7L3Adye6LfPIBVwEfNsYsxnoZdxQ0SJ9r7OxPhVXAEsBH1Ns+7uYxev9TZagUAeUhv1cEjy26IiIGysgPGiMeSR4uNHuSgb/u9i2Pr0cuFlETmENDV6LNd6eFRxigMX3ntcCtcaYncGfH8YKEov9vb4eOGmMaTbGDAOPYL3/i/m9DjfZ+ztn97hkCQovASuDMxQ8WImpXye4TXMuOI5+H1BljPly2EO/Bu4Ifn8H1l4Wi4Yx5p+MMSXGmHKs9/YJY8zfAk8CtwVPW1Sv2xjTAJwRkdXBQ9cBh1jk7zXWsNE2EUkL/nu3X/eifa/Hmez9/TXwjuAspG1AZ9gw04wkzeI1EbkJa9zZCdxvjPl8gps050TkCuBZYD9jY+ufxMor/AxYhlVh9k3GmPEJrEVBRK4GPmKMeZ2ILMfqOeQArwBvN8YMJrJ9c0lENmEl1j1ANXAn1ge9Rf1ei8hngDdjzbZ7BXg31vj5onqvw/ekARqx9qT5JVHe32CA/AbWUFofcKcxZtesnjdZgoJSSqnpJcvwkVJKqRhoUFBKKRWiQUEppVSIBgWllFIhGhSUUkqFaFBQ5z0RGRWRPWFfc1YETkTKw6tUzuL3N4vIfcHv14jICyIyKCIfGXde1Cq+M6n+KSIXisgDs22rUqBBQS0O/caYTWFf/5HoBoX5JPC14PdtWBU+/zP8hGmq+MZc/dMYsx8oEZFlc/oKVFLRoKAWLRE5JSJfFJH9IvKiiOx3bk0AAAKdSURBVKwIHi8XkSeCdecft2+iIlIoIr8Qkb3Br+3BSzlF5LvBGv5/EpHU4PkfEGvvin0i8pMoz+8HNhhj9gIYY5qMMS8Bw+NOjVrFd6pKryLyN2LtJ7BXRJ4Ju9ZvsFZ1KzUrGhTUYpA6bvjozWGPdRpjLsRa7fnfwWNfB35gjNkAPMjYJ/mvAU8bYzZi1RE6GDy+EvimMWY90AG8MXj8E8Dm4HXujtKurUAsQ0+TVbicqtLrp4Abgm29Oex3dwGviuE5lYpKg4JaDMYPH/007LGHwv57WfD7y4AfB7//EXBF8PtrCW5OYowZNcZ0Bo+fNMbsCX6/GygPfr8PeFBE3o5VcmG8Iqzy1vHwF+ABEXkPVukWWxNW9VClZkWDglrszCTfz0R4DZ1RrLLVYG3q802sXsVLYVU6bf1ASgzXn6zCZSuTVP80xtwN/HPw93aLSG7wnJTg8yo1KxoU1GL35rD/vhD8/nnGxt3/FquIIFjbG74XQvs9Z052URFxAKXGmCeBjwOZQPq406qAFTG0MWoV32C9/KjVP0Wk0hiz0xjzKazeiB1UVhHbkJVSUY3/ZKPU+ShVRPaE/fyoMcae1pktIvuwPu2/NXjsH7B2LPso1g31zuDxe4B7ReRdWD2C92Lt7hWNE/h/wcAhwNeC22GGGGMOi0imiPiNMd0isgRrzD8DCIjIB4F1xpguEXk/8EfGqvja+YyPAz8RkX/Dqv55X/D4l0RkZfC5Hwf2Bo9fA/xu+j+ZUtFplVS1aAU33dlqjGlJYBs+BHQbY743D8/lBZ4GrghLTis1Izp8pFR8fZvInEQ8LQM+oQFBnQvtKSillArRnoJSSqkQDQpKKaVCNCgopZQK0aCglFIqRIOCUkqpEA0KSimlQv5/9Xo3oWGtFmUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's plot the loss we got during the training procedure\n",
    "plt.figure()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel('Epochs (100s)')\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Predict\n",
    "Great! We have successfully created our two models for bob and alice in parallel using federated learning! I experimented with federated averaging, but it turned out that for a batch size of 1, as in the present case, the model was diverging. Let's try using our models for prediction now, shall we? The final reward for our effort!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_line, worker, n_predictions=3):\n",
    "    model = model.copy().get()\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        model_remote = model.send(worker)\n",
    "        line_tensor = lineToTensor(input_line)\n",
    "        line_remote = line_tensor.copy().send(worker)\n",
    "        #line_tensor = lineToTensor(input_line)\n",
    "        #output = evaluate(model, line_remote)\n",
    "        # Get top N categories\n",
    "        hidden = model_remote.initHidden()\n",
    "        hidden_remote = hidden.copy().send(worker)\n",
    "\n",
    "        for i in range(line_remote.shape[0]):\n",
    "            output, hidden_remote = model_remote(line_remote[i], hidden_remote)\n",
    "        \n",
    "        topv, topi = output.copy().get().topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Xin\n",
      "(-0.27) Chinese\n",
      "(-2.59) German\n",
      "(-3.29) Japanese\n",
      "\n",
      "> Xin\n",
      "(-0.81) Korean\n",
      "(-1.41) Vietnamese\n",
      "(-3.22) Scottish\n",
      "\n",
      "> Daniele\n",
      "(-1.35) Italian\n",
      "(-1.44) German\n",
      "(-1.60) Japanese\n",
      "\n",
      "> Daniele\n",
      "(-1.58) French\n",
      "(-2.11) Czech\n",
      "(-2.18) Portuguese\n"
     ]
    }
   ],
   "source": [
    "#Notice how the different models learned may perform different predictions, based on the data that\n",
    "#was shown to them.\n",
    "predict(model_pointers[\"alice\"], \"Qing\", alice) \n",
    "predict(model_pointers[\"bob\"], \"Qing\", bob) \n",
    "\n",
    "predict(model_pointers[\"alice\"], \"Daniele\", alice) \n",
    "predict(model_pointers[\"bob\"], \"Daniele\", bob) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
