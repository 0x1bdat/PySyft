{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load required libraries\n",
    "\n",
    "'''Torch Libraries required to train Neural Network'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "'''Library required to train neural network using federated learning. Syft overloads PyTorch'''\n",
    "\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Setting of Learning Task</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = True\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        self.save_model = False\n",
    "        \n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load and Distribute Dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Load_Data():\n",
    "    \n",
    "    '''Load CIFAR dataset from torch vision module and split to training/test set'''\n",
    "    \n",
    "\n",
    "    federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    return (federated_train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Initialize Workers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n",
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause                     unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "#Hook Torch instance with Syft Hook\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "#Initialize workers\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "\n",
    "#Define compute nodes\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "#Add worker instances\n",
    "me.add_workers([bob, alice])\n",
    "bob.add_workers([me, alice])\n",
    "alice.add_workers([me, bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Distribute Data to Workers</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Scanning and sending data to bob, alice...\n"
     ]
    }
   ],
   "source": [
    "#TODO leverage FederatedDataset and FederatedDataloader\n",
    "\n",
    "#Load required dataset\n",
    "federated_train_loader,test_loader=Load_Data()\n",
    "\n",
    "print(federated_train_loader.__len__())\n",
    "\n",
    "#Distribute dataset among workers\n",
    "#train_distributed_dataset = FederatedDataset(compute_nodes,trainloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define Neural Network Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    '''Neural Network Model'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Net,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            criterion=nn.CrossEntropyLoss()\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrishikesh/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/native.py:216: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  response = eval(cmd)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50048 (0%)]\tLoss: 2.301906\n",
      "Train Epoch: 1 [640/50048 (1%)]\tLoss: 2.305471\n",
      "Train Epoch: 1 [1280/50048 (3%)]\tLoss: 2.295641\n",
      "Train Epoch: 1 [1920/50048 (4%)]\tLoss: 2.309566\n",
      "Train Epoch: 1 [2560/50048 (5%)]\tLoss: 2.302849\n",
      "Train Epoch: 1 [3200/50048 (6%)]\tLoss: 2.316859\n",
      "Train Epoch: 1 [3840/50048 (8%)]\tLoss: 2.296539\n",
      "Train Epoch: 1 [4480/50048 (9%)]\tLoss: 2.311052\n",
      "Train Epoch: 1 [5120/50048 (10%)]\tLoss: 2.296306\n",
      "Train Epoch: 1 [5760/50048 (12%)]\tLoss: 2.310413\n",
      "Train Epoch: 1 [6400/50048 (13%)]\tLoss: 2.305417\n",
      "Train Epoch: 1 [7040/50048 (14%)]\tLoss: 2.300943\n",
      "Train Epoch: 1 [7680/50048 (15%)]\tLoss: 2.294203\n",
      "Train Epoch: 1 [8320/50048 (17%)]\tLoss: 2.292655\n",
      "Train Epoch: 1 [8960/50048 (18%)]\tLoss: 2.301416\n",
      "Train Epoch: 1 [9600/50048 (19%)]\tLoss: 2.296557\n",
      "Train Epoch: 1 [10240/50048 (20%)]\tLoss: 2.309853\n",
      "Train Epoch: 1 [10880/50048 (22%)]\tLoss: 2.309187\n",
      "Train Epoch: 1 [11520/50048 (23%)]\tLoss: 2.301913\n",
      "Train Epoch: 1 [12160/50048 (24%)]\tLoss: 2.301756\n",
      "Train Epoch: 1 [12800/50048 (26%)]\tLoss: 2.302919\n",
      "Train Epoch: 1 [13440/50048 (27%)]\tLoss: 2.293123\n",
      "Train Epoch: 1 [14080/50048 (28%)]\tLoss: 2.322248\n",
      "Train Epoch: 1 [14720/50048 (29%)]\tLoss: 2.299092\n",
      "Train Epoch: 1 [15360/50048 (31%)]\tLoss: 2.316940\n",
      "Train Epoch: 1 [16000/50048 (32%)]\tLoss: 2.303377\n",
      "Train Epoch: 1 [16640/50048 (33%)]\tLoss: 2.303687\n",
      "Train Epoch: 1 [17280/50048 (35%)]\tLoss: 2.298665\n",
      "Train Epoch: 1 [17920/50048 (36%)]\tLoss: 2.305929\n",
      "Train Epoch: 1 [18560/50048 (37%)]\tLoss: 2.308558\n",
      "Train Epoch: 1 [19200/50048 (38%)]\tLoss: 2.305110\n",
      "Train Epoch: 1 [19840/50048 (40%)]\tLoss: 2.291895\n",
      "Train Epoch: 1 [20480/50048 (41%)]\tLoss: 2.290227\n",
      "Train Epoch: 1 [21120/50048 (42%)]\tLoss: 2.302121\n",
      "Train Epoch: 1 [21760/50048 (43%)]\tLoss: 2.307569\n",
      "Train Epoch: 1 [22400/50048 (45%)]\tLoss: 2.305248\n",
      "Train Epoch: 1 [23040/50048 (46%)]\tLoss: 2.301556\n",
      "Train Epoch: 1 [23680/50048 (47%)]\tLoss: 2.307478\n",
      "Train Epoch: 1 [24320/50048 (49%)]\tLoss: 2.297625\n",
      "Train Epoch: 1 [24960/50048 (50%)]\tLoss: 2.298223\n",
      "Train Epoch: 1 [25600/50048 (51%)]\tLoss: 2.293038\n",
      "Train Epoch: 1 [26240/50048 (52%)]\tLoss: 2.326135\n",
      "Train Epoch: 1 [26880/50048 (54%)]\tLoss: 2.313550\n",
      "Train Epoch: 1 [27520/50048 (55%)]\tLoss: 2.304935\n",
      "Train Epoch: 1 [28160/50048 (56%)]\tLoss: 2.291486\n",
      "Train Epoch: 1 [28800/50048 (58%)]\tLoss: 2.306650\n",
      "Train Epoch: 1 [29440/50048 (59%)]\tLoss: 2.297400\n",
      "Train Epoch: 1 [30080/50048 (60%)]\tLoss: 2.315862\n",
      "Train Epoch: 1 [30720/50048 (61%)]\tLoss: 2.302680\n",
      "Train Epoch: 1 [31360/50048 (63%)]\tLoss: 2.296542\n",
      "Train Epoch: 1 [32000/50048 (64%)]\tLoss: 2.298044\n",
      "Train Epoch: 1 [32640/50048 (65%)]\tLoss: 2.300698\n",
      "Train Epoch: 1 [33280/50048 (66%)]\tLoss: 2.302430\n",
      "Train Epoch: 1 [33920/50048 (68%)]\tLoss: 2.299843\n",
      "Train Epoch: 1 [34560/50048 (69%)]\tLoss: 2.313731\n",
      "Train Epoch: 1 [35200/50048 (70%)]\tLoss: 2.295603\n",
      "Train Epoch: 1 [35840/50048 (72%)]\tLoss: 2.304348\n",
      "Train Epoch: 1 [36480/50048 (73%)]\tLoss: 2.304428\n",
      "Train Epoch: 1 [37120/50048 (74%)]\tLoss: 2.297380\n",
      "Train Epoch: 1 [37760/50048 (75%)]\tLoss: 2.296679\n",
      "Train Epoch: 1 [38400/50048 (77%)]\tLoss: 2.300205\n",
      "Train Epoch: 1 [39040/50048 (78%)]\tLoss: 2.302526\n",
      "Train Epoch: 1 [39680/50048 (79%)]\tLoss: 2.314425\n",
      "Train Epoch: 1 [40320/50048 (81%)]\tLoss: 2.302215\n",
      "Train Epoch: 1 [40960/50048 (82%)]\tLoss: 2.302273\n",
      "Train Epoch: 1 [41600/50048 (83%)]\tLoss: 2.318301\n",
      "Train Epoch: 1 [42240/50048 (84%)]\tLoss: 2.304747\n",
      "Train Epoch: 1 [42880/50048 (86%)]\tLoss: 2.294515\n",
      "Train Epoch: 1 [43520/50048 (87%)]\tLoss: 2.314866\n",
      "Train Epoch: 1 [44160/50048 (88%)]\tLoss: 2.297338\n",
      "Train Epoch: 1 [44800/50048 (90%)]\tLoss: 2.307599\n",
      "Train Epoch: 1 [45440/50048 (91%)]\tLoss: 2.311867\n",
      "Train Epoch: 1 [46080/50048 (92%)]\tLoss: 2.297817\n",
      "Train Epoch: 1 [46720/50048 (93%)]\tLoss: 2.306828\n",
      "Train Epoch: 1 [47360/50048 (95%)]\tLoss: 2.317946\n",
      "Train Epoch: 1 [48000/50048 (96%)]\tLoss: 2.309517\n",
      "Train Epoch: 1 [48640/50048 (97%)]\tLoss: 2.304506\n",
      "Train Epoch: 1 [49280/50048 (98%)]\tLoss: 2.300368\n",
      "Train Epoch: 1 [49920/50048 (100%)]\tLoss: 2.302522\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 1074/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/50048 (0%)]\tLoss: 2.317316\n",
      "Train Epoch: 2 [640/50048 (1%)]\tLoss: 2.296633\n",
      "Train Epoch: 2 [1280/50048 (3%)]\tLoss: 2.296724\n",
      "Train Epoch: 2 [1920/50048 (4%)]\tLoss: 2.292758\n",
      "Train Epoch: 2 [2560/50048 (5%)]\tLoss: 2.316243\n",
      "Train Epoch: 2 [3200/50048 (6%)]\tLoss: 2.309159\n",
      "Train Epoch: 2 [3840/50048 (8%)]\tLoss: 2.296275\n",
      "Train Epoch: 2 [4480/50048 (9%)]\tLoss: 2.302202\n",
      "Train Epoch: 2 [5120/50048 (10%)]\tLoss: 2.297363\n",
      "Train Epoch: 2 [5760/50048 (12%)]\tLoss: 2.300304\n",
      "Train Epoch: 2 [6400/50048 (13%)]\tLoss: 2.299806\n",
      "Train Epoch: 2 [7040/50048 (14%)]\tLoss: 2.304250\n",
      "Train Epoch: 2 [7680/50048 (15%)]\tLoss: 2.297725\n",
      "Train Epoch: 2 [8320/50048 (17%)]\tLoss: 2.303432\n",
      "Train Epoch: 2 [8960/50048 (18%)]\tLoss: 2.304918\n",
      "Train Epoch: 2 [9600/50048 (19%)]\tLoss: 2.309633\n",
      "Train Epoch: 2 [10240/50048 (20%)]\tLoss: 2.293561\n",
      "Train Epoch: 2 [10880/50048 (22%)]\tLoss: 2.306179\n",
      "Train Epoch: 2 [11520/50048 (23%)]\tLoss: 2.306967\n",
      "Train Epoch: 2 [12160/50048 (24%)]\tLoss: 2.302497\n",
      "Train Epoch: 2 [12800/50048 (26%)]\tLoss: 2.313168\n",
      "Train Epoch: 2 [13440/50048 (27%)]\tLoss: 2.305549\n",
      "Train Epoch: 2 [14080/50048 (28%)]\tLoss: 2.311478\n",
      "Train Epoch: 2 [14720/50048 (29%)]\tLoss: 2.301735\n",
      "Train Epoch: 2 [15360/50048 (31%)]\tLoss: 2.301591\n",
      "Train Epoch: 2 [16000/50048 (32%)]\tLoss: 2.290012\n",
      "Train Epoch: 2 [16640/50048 (33%)]\tLoss: 2.313203\n",
      "Train Epoch: 2 [17280/50048 (35%)]\tLoss: 2.296669\n",
      "Train Epoch: 2 [17920/50048 (36%)]\tLoss: 2.290156\n",
      "Train Epoch: 2 [18560/50048 (37%)]\tLoss: 2.296611\n",
      "Train Epoch: 2 [19200/50048 (38%)]\tLoss: 2.299465\n",
      "Train Epoch: 2 [19840/50048 (40%)]\tLoss: 2.307649\n",
      "Train Epoch: 2 [20480/50048 (41%)]\tLoss: 2.310691\n",
      "Train Epoch: 2 [21120/50048 (42%)]\tLoss: 2.293922\n",
      "Train Epoch: 2 [21760/50048 (43%)]\tLoss: 2.299693\n",
      "Train Epoch: 2 [22400/50048 (45%)]\tLoss: 2.299449\n",
      "Train Epoch: 2 [23040/50048 (46%)]\tLoss: 2.299310\n",
      "Train Epoch: 2 [23680/50048 (47%)]\tLoss: 2.305183\n",
      "Train Epoch: 2 [24320/50048 (49%)]\tLoss: 2.293789\n",
      "Train Epoch: 2 [24960/50048 (50%)]\tLoss: 2.302300\n",
      "Train Epoch: 2 [25600/50048 (51%)]\tLoss: 2.321152\n",
      "Train Epoch: 2 [26240/50048 (52%)]\tLoss: 2.313195\n",
      "Train Epoch: 2 [26880/50048 (54%)]\tLoss: 2.297019\n",
      "Train Epoch: 2 [27520/50048 (55%)]\tLoss: 2.295202\n",
      "Train Epoch: 2 [28160/50048 (56%)]\tLoss: 2.304458\n",
      "Train Epoch: 2 [28800/50048 (58%)]\tLoss: 2.304284\n",
      "Train Epoch: 2 [29440/50048 (59%)]\tLoss: 2.307869\n",
      "Train Epoch: 2 [30080/50048 (60%)]\tLoss: 2.308987\n",
      "Train Epoch: 2 [30720/50048 (61%)]\tLoss: 2.316041\n",
      "Train Epoch: 2 [31360/50048 (63%)]\tLoss: 2.299174\n",
      "Train Epoch: 2 [32000/50048 (64%)]\tLoss: 2.313178\n",
      "Train Epoch: 2 [32640/50048 (65%)]\tLoss: 2.309153\n",
      "Train Epoch: 2 [33280/50048 (66%)]\tLoss: 2.310271\n",
      "Train Epoch: 2 [33920/50048 (68%)]\tLoss: 2.306247\n",
      "Train Epoch: 2 [34560/50048 (69%)]\tLoss: 2.299121\n",
      "Train Epoch: 2 [35200/50048 (70%)]\tLoss: 2.311957\n",
      "Train Epoch: 2 [35840/50048 (72%)]\tLoss: 2.299449\n",
      "Train Epoch: 2 [36480/50048 (73%)]\tLoss: 2.310889\n",
      "Train Epoch: 2 [37120/50048 (74%)]\tLoss: 2.292708\n",
      "Train Epoch: 2 [37760/50048 (75%)]\tLoss: 2.297534\n",
      "Train Epoch: 2 [38400/50048 (77%)]\tLoss: 2.303682\n",
      "Train Epoch: 2 [39040/50048 (78%)]\tLoss: 2.303735\n",
      "Train Epoch: 2 [39680/50048 (79%)]\tLoss: 2.305977\n",
      "Train Epoch: 2 [40320/50048 (81%)]\tLoss: 2.298608\n",
      "Train Epoch: 2 [40960/50048 (82%)]\tLoss: 2.309521\n",
      "Train Epoch: 2 [41600/50048 (83%)]\tLoss: 2.325654\n",
      "Train Epoch: 2 [42240/50048 (84%)]\tLoss: 2.306872\n",
      "Train Epoch: 2 [42880/50048 (86%)]\tLoss: 2.304972\n",
      "Train Epoch: 2 [43520/50048 (87%)]\tLoss: 2.293444\n",
      "Train Epoch: 2 [44160/50048 (88%)]\tLoss: 2.304765\n",
      "Train Epoch: 2 [44800/50048 (90%)]\tLoss: 2.306074\n",
      "Train Epoch: 2 [45440/50048 (91%)]\tLoss: 2.295180\n",
      "Train Epoch: 2 [46080/50048 (92%)]\tLoss: 2.302280\n",
      "Train Epoch: 2 [46720/50048 (93%)]\tLoss: 2.310316\n",
      "Train Epoch: 2 [47360/50048 (95%)]\tLoss: 2.309874\n",
      "Train Epoch: 2 [48000/50048 (96%)]\tLoss: 2.303206\n",
      "Train Epoch: 2 [48640/50048 (97%)]\tLoss: 2.302365\n",
      "Train Epoch: 2 [49280/50048 (98%)]\tLoss: 2.308958\n",
      "Train Epoch: 2 [49920/50048 (100%)]\tLoss: 2.293766\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 1095/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/50048 (0%)]\tLoss: 2.300969\n",
      "Train Epoch: 3 [640/50048 (1%)]\tLoss: 2.298902\n",
      "Train Epoch: 3 [1280/50048 (3%)]\tLoss: 2.301850\n",
      "Train Epoch: 3 [1920/50048 (4%)]\tLoss: 2.311142\n",
      "Train Epoch: 3 [2560/50048 (5%)]\tLoss: 2.297498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [3200/50048 (6%)]\tLoss: 2.296962\n",
      "Train Epoch: 3 [3840/50048 (8%)]\tLoss: 2.306081\n",
      "Train Epoch: 3 [4480/50048 (9%)]\tLoss: 2.306893\n",
      "Train Epoch: 3 [5120/50048 (10%)]\tLoss: 2.304414\n",
      "Train Epoch: 3 [5760/50048 (12%)]\tLoss: 2.300484\n",
      "Train Epoch: 3 [6400/50048 (13%)]\tLoss: 2.303024\n",
      "Train Epoch: 3 [7040/50048 (14%)]\tLoss: 2.301856\n",
      "Train Epoch: 3 [7680/50048 (15%)]\tLoss: 2.308972\n",
      "Train Epoch: 3 [8320/50048 (17%)]\tLoss: 2.304061\n",
      "Train Epoch: 3 [8960/50048 (18%)]\tLoss: 2.301983\n",
      "Train Epoch: 3 [9600/50048 (19%)]\tLoss: 2.305756\n",
      "Train Epoch: 3 [10240/50048 (20%)]\tLoss: 2.307072\n",
      "Train Epoch: 3 [10880/50048 (22%)]\tLoss: 2.308706\n",
      "Train Epoch: 3 [11520/50048 (23%)]\tLoss: 2.292641\n",
      "Train Epoch: 3 [12160/50048 (24%)]\tLoss: 2.302902\n",
      "Train Epoch: 3 [12800/50048 (26%)]\tLoss: 2.291541\n",
      "Train Epoch: 3 [13440/50048 (27%)]\tLoss: 2.286159\n",
      "Train Epoch: 3 [14080/50048 (28%)]\tLoss: 2.302714\n",
      "Train Epoch: 3 [14720/50048 (29%)]\tLoss: 2.309542\n",
      "Train Epoch: 3 [15360/50048 (31%)]\tLoss: 2.301406\n",
      "Train Epoch: 3 [16000/50048 (32%)]\tLoss: 2.312171\n",
      "Train Epoch: 3 [16640/50048 (33%)]\tLoss: 2.310222\n",
      "Train Epoch: 3 [17280/50048 (35%)]\tLoss: 2.289137\n",
      "Train Epoch: 3 [17920/50048 (36%)]\tLoss: 2.293822\n",
      "Train Epoch: 3 [18560/50048 (37%)]\tLoss: 2.303947\n",
      "Train Epoch: 3 [19200/50048 (38%)]\tLoss: 2.305846\n",
      "Train Epoch: 3 [19840/50048 (40%)]\tLoss: 2.299147\n",
      "Train Epoch: 3 [20480/50048 (41%)]\tLoss: 2.309641\n",
      "Train Epoch: 3 [21120/50048 (42%)]\tLoss: 2.302933\n",
      "Train Epoch: 3 [21760/50048 (43%)]\tLoss: 2.311274\n",
      "Train Epoch: 3 [22400/50048 (45%)]\tLoss: 2.295445\n",
      "Train Epoch: 3 [23040/50048 (46%)]\tLoss: 2.300507\n",
      "Train Epoch: 3 [23680/50048 (47%)]\tLoss: 2.288085\n",
      "Train Epoch: 3 [24320/50048 (49%)]\tLoss: 2.308856\n",
      "Train Epoch: 3 [24960/50048 (50%)]\tLoss: 2.317375\n",
      "Train Epoch: 3 [25600/50048 (51%)]\tLoss: 2.301784\n",
      "Train Epoch: 3 [26240/50048 (52%)]\tLoss: 2.294520\n",
      "Train Epoch: 3 [26880/50048 (54%)]\tLoss: 2.299816\n",
      "Train Epoch: 3 [27520/50048 (55%)]\tLoss: 2.310594\n",
      "Train Epoch: 3 [28160/50048 (56%)]\tLoss: 2.297890\n",
      "Train Epoch: 3 [28800/50048 (58%)]\tLoss: 2.301039\n",
      "Train Epoch: 3 [29440/50048 (59%)]\tLoss: 2.304067\n",
      "Train Epoch: 3 [30080/50048 (60%)]\tLoss: 2.307398\n",
      "Train Epoch: 3 [30720/50048 (61%)]\tLoss: 2.294605\n",
      "Train Epoch: 3 [31360/50048 (63%)]\tLoss: 2.303395\n",
      "Train Epoch: 3 [32000/50048 (64%)]\tLoss: 2.311188\n",
      "Train Epoch: 3 [32640/50048 (65%)]\tLoss: 2.308505\n",
      "Train Epoch: 3 [33280/50048 (66%)]\tLoss: 2.304073\n",
      "Train Epoch: 3 [33920/50048 (68%)]\tLoss: 2.314057\n",
      "Train Epoch: 3 [34560/50048 (69%)]\tLoss: 2.296731\n",
      "Train Epoch: 3 [35200/50048 (70%)]\tLoss: 2.293136\n",
      "Train Epoch: 3 [35840/50048 (72%)]\tLoss: 2.294504\n",
      "Train Epoch: 3 [36480/50048 (73%)]\tLoss: 2.300399\n",
      "Train Epoch: 3 [37120/50048 (74%)]\tLoss: 2.314528\n",
      "Train Epoch: 3 [37760/50048 (75%)]\tLoss: 2.304609\n",
      "Train Epoch: 3 [38400/50048 (77%)]\tLoss: 2.309499\n",
      "Train Epoch: 3 [39040/50048 (78%)]\tLoss: 2.305309\n",
      "Train Epoch: 3 [39680/50048 (79%)]\tLoss: 2.297381\n",
      "Train Epoch: 3 [40320/50048 (81%)]\tLoss: 2.307928\n",
      "Train Epoch: 3 [40960/50048 (82%)]\tLoss: 2.308985\n",
      "Train Epoch: 3 [41600/50048 (83%)]\tLoss: 2.306192\n",
      "Train Epoch: 3 [42240/50048 (84%)]\tLoss: 2.304962\n",
      "Train Epoch: 3 [42880/50048 (86%)]\tLoss: 2.293572\n",
      "Train Epoch: 3 [43520/50048 (87%)]\tLoss: 2.304447\n",
      "Train Epoch: 3 [44160/50048 (88%)]\tLoss: 2.302078\n",
      "Train Epoch: 3 [44800/50048 (90%)]\tLoss: 2.301325\n",
      "Train Epoch: 3 [45440/50048 (91%)]\tLoss: 2.307192\n",
      "Train Epoch: 3 [46080/50048 (92%)]\tLoss: 2.314483\n",
      "Train Epoch: 3 [46720/50048 (93%)]\tLoss: 2.312006\n",
      "Train Epoch: 3 [47360/50048 (95%)]\tLoss: 2.307140\n",
      "Train Epoch: 3 [48000/50048 (96%)]\tLoss: 2.287905\n",
      "Train Epoch: 3 [48640/50048 (97%)]\tLoss: 2.304231\n",
      "Train Epoch: 3 [49280/50048 (98%)]\tLoss: 2.295957\n",
      "Train Epoch: 3 [49920/50048 (100%)]\tLoss: 2.298162\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 1108/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [0/50048 (0%)]\tLoss: 2.303227\n",
      "Train Epoch: 4 [640/50048 (1%)]\tLoss: 2.300807\n",
      "Train Epoch: 4 [1280/50048 (3%)]\tLoss: 2.310152\n",
      "Train Epoch: 4 [1920/50048 (4%)]\tLoss: 2.304716\n",
      "Train Epoch: 4 [2560/50048 (5%)]\tLoss: 2.290301\n",
      "Train Epoch: 4 [3200/50048 (6%)]\tLoss: 2.301809\n",
      "Train Epoch: 4 [3840/50048 (8%)]\tLoss: 2.312242\n",
      "Train Epoch: 4 [4480/50048 (9%)]\tLoss: 2.297694\n",
      "Train Epoch: 4 [5120/50048 (10%)]\tLoss: 2.292104\n",
      "Train Epoch: 4 [5760/50048 (12%)]\tLoss: 2.298719\n",
      "Train Epoch: 4 [6400/50048 (13%)]\tLoss: 2.301243\n",
      "Train Epoch: 4 [7040/50048 (14%)]\tLoss: 2.301570\n",
      "Train Epoch: 4 [7680/50048 (15%)]\tLoss: 2.312602\n",
      "Train Epoch: 4 [8320/50048 (17%)]\tLoss: 2.308678\n",
      "Train Epoch: 4 [8960/50048 (18%)]\tLoss: 2.308816\n",
      "Train Epoch: 4 [9600/50048 (19%)]\tLoss: 2.296004\n",
      "Train Epoch: 4 [10240/50048 (20%)]\tLoss: 2.303689\n",
      "Train Epoch: 4 [10880/50048 (22%)]\tLoss: 2.303249\n",
      "Train Epoch: 4 [11520/50048 (23%)]\tLoss: 2.294491\n",
      "Train Epoch: 4 [12160/50048 (24%)]\tLoss: 2.300812\n",
      "Train Epoch: 4 [12800/50048 (26%)]\tLoss: 2.300937\n",
      "Train Epoch: 4 [13440/50048 (27%)]\tLoss: 2.302474\n",
      "Train Epoch: 4 [14080/50048 (28%)]\tLoss: 2.303760\n",
      "Train Epoch: 4 [14720/50048 (29%)]\tLoss: 2.302441\n",
      "Train Epoch: 4 [15360/50048 (31%)]\tLoss: 2.307011\n",
      "Train Epoch: 4 [16000/50048 (32%)]\tLoss: 2.297541\n",
      "Train Epoch: 4 [16640/50048 (33%)]\tLoss: 2.319039\n",
      "Train Epoch: 4 [17280/50048 (35%)]\tLoss: 2.300500\n",
      "Train Epoch: 4 [17920/50048 (36%)]\tLoss: 2.303744\n",
      "Train Epoch: 4 [18560/50048 (37%)]\tLoss: 2.299559\n",
      "Train Epoch: 4 [19200/50048 (38%)]\tLoss: 2.302380\n",
      "Train Epoch: 4 [19840/50048 (40%)]\tLoss: 2.301655\n",
      "Train Epoch: 4 [20480/50048 (41%)]\tLoss: 2.299114\n",
      "Train Epoch: 4 [21120/50048 (42%)]\tLoss: 2.291993\n",
      "Train Epoch: 4 [21760/50048 (43%)]\tLoss: 2.308296\n",
      "Train Epoch: 4 [22400/50048 (45%)]\tLoss: 2.299921\n",
      "Train Epoch: 4 [23040/50048 (46%)]\tLoss: 2.294219\n",
      "Train Epoch: 4 [23680/50048 (47%)]\tLoss: 2.297627\n",
      "Train Epoch: 4 [24320/50048 (49%)]\tLoss: 2.309881\n",
      "Train Epoch: 4 [24960/50048 (50%)]\tLoss: 2.296255\n",
      "Train Epoch: 4 [25600/50048 (51%)]\tLoss: 2.305334\n",
      "Train Epoch: 4 [26240/50048 (52%)]\tLoss: 2.291676\n",
      "Train Epoch: 4 [26880/50048 (54%)]\tLoss: 2.293454\n",
      "Train Epoch: 4 [27520/50048 (55%)]\tLoss: 2.294753\n",
      "Train Epoch: 4 [28160/50048 (56%)]\tLoss: 2.310113\n",
      "Train Epoch: 4 [28800/50048 (58%)]\tLoss: 2.310080\n",
      "Train Epoch: 4 [29440/50048 (59%)]\tLoss: 2.300570\n",
      "Train Epoch: 4 [30080/50048 (60%)]\tLoss: 2.300135\n",
      "Train Epoch: 4 [30720/50048 (61%)]\tLoss: 2.303057\n",
      "Train Epoch: 4 [31360/50048 (63%)]\tLoss: 2.309777\n",
      "Train Epoch: 4 [32000/50048 (64%)]\tLoss: 2.302276\n",
      "Train Epoch: 4 [32640/50048 (65%)]\tLoss: 2.297347\n",
      "Train Epoch: 4 [33280/50048 (66%)]\tLoss: 2.313491\n",
      "Train Epoch: 4 [33920/50048 (68%)]\tLoss: 2.306667\n",
      "Train Epoch: 4 [34560/50048 (69%)]\tLoss: 2.305256\n",
      "Train Epoch: 4 [35200/50048 (70%)]\tLoss: 2.308849\n",
      "Train Epoch: 4 [35840/50048 (72%)]\tLoss: 2.302259\n",
      "Train Epoch: 4 [36480/50048 (73%)]\tLoss: 2.307699\n",
      "Train Epoch: 4 [37120/50048 (74%)]\tLoss: 2.305402\n",
      "Train Epoch: 4 [37760/50048 (75%)]\tLoss: 2.305888\n",
      "Train Epoch: 4 [38400/50048 (77%)]\tLoss: 2.303717\n",
      "Train Epoch: 4 [39040/50048 (78%)]\tLoss: 2.299617\n",
      "Train Epoch: 4 [39680/50048 (79%)]\tLoss: 2.304186\n",
      "Train Epoch: 4 [40320/50048 (81%)]\tLoss: 2.299893\n",
      "Train Epoch: 4 [40960/50048 (82%)]\tLoss: 2.303155\n",
      "Train Epoch: 4 [41600/50048 (83%)]\tLoss: 2.296914\n",
      "Train Epoch: 4 [42240/50048 (84%)]\tLoss: 2.303036\n",
      "Train Epoch: 4 [42880/50048 (86%)]\tLoss: 2.309986\n",
      "Train Epoch: 4 [43520/50048 (87%)]\tLoss: 2.296298\n",
      "Train Epoch: 4 [44160/50048 (88%)]\tLoss: 2.301119\n",
      "Train Epoch: 4 [44800/50048 (90%)]\tLoss: 2.300730\n",
      "Train Epoch: 4 [45440/50048 (91%)]\tLoss: 2.302037\n",
      "Train Epoch: 4 [46080/50048 (92%)]\tLoss: 2.294914\n",
      "Train Epoch: 4 [46720/50048 (93%)]\tLoss: 2.304936\n",
      "Train Epoch: 4 [47360/50048 (95%)]\tLoss: 2.301934\n",
      "Train Epoch: 4 [48000/50048 (96%)]\tLoss: 2.306504\n",
      "Train Epoch: 4 [48640/50048 (97%)]\tLoss: 2.300678\n",
      "Train Epoch: 4 [49280/50048 (98%)]\tLoss: 2.302412\n",
      "Train Epoch: 4 [49920/50048 (100%)]\tLoss: 2.297056\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 1148/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [0/50048 (0%)]\tLoss: 2.307684\n",
      "Train Epoch: 5 [640/50048 (1%)]\tLoss: 2.295979\n",
      "Train Epoch: 5 [1280/50048 (3%)]\tLoss: 2.296262\n",
      "Train Epoch: 5 [1920/50048 (4%)]\tLoss: 2.303340\n",
      "Train Epoch: 5 [2560/50048 (5%)]\tLoss: 2.303581\n",
      "Train Epoch: 5 [3200/50048 (6%)]\tLoss: 2.314043\n",
      "Train Epoch: 5 [3840/50048 (8%)]\tLoss: 2.309157\n",
      "Train Epoch: 5 [4480/50048 (9%)]\tLoss: 2.300177\n",
      "Train Epoch: 5 [5120/50048 (10%)]\tLoss: 2.304024\n",
      "Train Epoch: 5 [5760/50048 (12%)]\tLoss: 2.311169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6400/50048 (13%)]\tLoss: 2.309510\n",
      "Train Epoch: 5 [7040/50048 (14%)]\tLoss: 2.300089\n",
      "Train Epoch: 5 [7680/50048 (15%)]\tLoss: 2.299743\n",
      "Train Epoch: 5 [8320/50048 (17%)]\tLoss: 2.299838\n",
      "Train Epoch: 5 [8960/50048 (18%)]\tLoss: 2.305449\n",
      "Train Epoch: 5 [9600/50048 (19%)]\tLoss: 2.301354\n",
      "Train Epoch: 5 [10240/50048 (20%)]\tLoss: 2.304584\n",
      "Train Epoch: 5 [10880/50048 (22%)]\tLoss: 2.302247\n",
      "Train Epoch: 5 [11520/50048 (23%)]\tLoss: 2.298271\n",
      "Train Epoch: 5 [12160/50048 (24%)]\tLoss: 2.300226\n",
      "Train Epoch: 5 [12800/50048 (26%)]\tLoss: 2.296985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dfdee50344a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1366efdeccac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- NEW: get the model back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/hook.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0mto_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPointerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mto_return\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m                         \u001b[0mto_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/pointer.py\u001b[0m in \u001b[0;36mis_none\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_is_remote_tensor_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36mrequest_is_remote_tensor_none\u001b[0;34m(self, pointer)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequest_is_remote_tensor_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMSGTYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_NONE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpointer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, msg_type, message, location)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# Step 1: serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mbin_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/serde.py\u001b[0m in \u001b[0;36mserialize\u001b[0;34m(obj, compress, compress_scheme)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# for details on how this works. The general purpose is to handle types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# which the fast serializer cannot handle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0msimple_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# 2) Serialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/serde.py\u001b[0m in \u001b[0;36m_simplify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mcurrent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msimplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/serde.py\u001b[0m in \u001b[0;36m_simplify_collection\u001b[0;34m(my_collection)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;31m# Step 1: serialize each part of the collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_collection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_simplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;31m# Step 2: convert back to original type and return serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/serde.py\u001b[0m in \u001b[0;36m_simplify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mcurrent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msimplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/serde.py\u001b[0m in \u001b[0;36m_simplify_pointer_tensor\u001b[0;34m(ptr)\u001b[0m\n\u001b[1;32m    727\u001b[0m     \"\"\"\n\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoint_to_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;31m# a more general but slower/more verbose option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/pointer.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/pointer.py\u001b[0m in \u001b[0;36mget_shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/workers/base.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *query)\u001b[0m\n\u001b[1;32m    670\u001b[0m                         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mquery_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m                         \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mdescription\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/syft_flstm/lib/python3.6/site-packages/syft-0.1.3a1-py3.6.egg/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhas_child\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhas_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"child\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"Federated_CIFAR10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
