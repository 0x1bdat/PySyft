{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALSfsSFjeV1o"
   },
   "source": [
    "# Federated Learning with Secure Aggregation and Diff Privacy using PySyft\n",
    "\n",
    "This is an example of using our new Secure Multi-Party Computation tensor (SPDZTensor) to perform an encrypted average of gradients across multiple data owners.\n",
    "\n",
    "Before starting with this notebook, we recommend looking at `Boston_Housing_Federated_Training.ipynb` which is located in the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFvJUEHOfA-r"
   },
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! URL=\"https://github.com/LaRiffle/differential-privacy.git\" && FOLDER=\"differential_privacy\" && if [ ! -d $FOLDER ]; then git clone $URL $FOLDER; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
    "! pip install --upgrade --force-reinstall websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "caoZ_dM0wzdf",
    "outputId": "1a138f2e-f184-4b94-a93b-486617bde21a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('../other/data/boston_housing.pickle','rb')\n",
    "((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az9PH1BrfK46"
   },
   "source": [
    "#  Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5TyXfcOXp1w"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        print(\"WEIGHT LOCATION:\" + str(self.fc1.weight.location))\n",
    "        print(\"BIAS LOCATION:\" + str(self.fc1.bias.location))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def divide_clip_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            param.grad /= n_batch\n",
    "            gradient_clip(param)\n",
    "            \n",
    "    def add_noise_to_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            noise = 1/LOT_SIZE * gaussian_noise(param.grad)\n",
    "            param.grad += noise\n",
    "model = Net()\n",
    "model_params = list(model.parameters())\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r78FvSffTp0"
   },
   "source": [
    "# Hooking into Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai5bqlEWSFgf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "import syft\n",
    "import syft as sy\n",
    "from syft.core import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from syft.core.frameworks.torch import utils as torch_utils\n",
    "from torch.autograd import Variable\n",
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "me.is_client_worker = False\n",
    "\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "me.add_workers([bob, alice])\n",
    "bob.add_workers([me, alice])\n",
    "alice.add_workers([me, bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apSGFY_fv_7z"
   },
   "source": [
    "**Send data to the worker** <br>\n",
    "Usually they would already have it, this is just for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UByFBzz2wARz"
   },
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 9.689610525210778\n",
      "The mechanism is (O(0.297030), 0.000010)-differentially private\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Inspired from Abadi et al., Deep Learning with Differential Privacy, \n",
    "    Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications\n",
    "    Security, 2016\n",
    "\"\"\"\n",
    "from differential_privacy.privacy_accountant.pytorch import accountant\n",
    "import numpy as np\n",
    "\n",
    "n_batch = 3\n",
    "NUM_TRAINING_IMAGES = X.size()[0]\n",
    "LOT_SIZE = n_batch * args.batch_size\n",
    "N_LOTS = 100\n",
    "T = N_LOTS # number of samplings\n",
    "\n",
    "bound = 10\n",
    "epsilon = 0.5\n",
    "delta = 10**(-5)\n",
    "sigma = np.sqrt(2 * np.log(1.25/delta))/epsilon \n",
    "\n",
    "def sum_batch(grads):\n",
    "    n_items = len(grads)\n",
    "    return grads.view(n_items, -1).sum(dim=1)\n",
    "\n",
    "def gradient_clip(param):\n",
    "    \"\"\"Clip gradient to ensure ||param.grad||2 < bound\"\"\"\n",
    "    nn.utils.clip_grad_norm([param], bound)\n",
    "\n",
    "def gaussian_noise(grads):\n",
    "    \"\"\"Add gaussian noise to gradients\"\"\"\n",
    "    shape = grads.shape\n",
    "    noise = Variable(torch.zeros(shape))\n",
    "    noise.data.normal_(0.0, std=bound*sigma)\n",
    "    return noise\n",
    "\n",
    "q = LOT_SIZE / NUM_TRAINING_IMAGES\n",
    "spent_epsilon = q * epsilon * np.sqrt(T)\n",
    "spent_delta = delta\n",
    "print('sigma =', sigma)\n",
    "print('The mechanism is (O(%f), %f)-differentially private' % (spent_epsilon, spent_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_accountant = accountant.GaussianMomentsAccountant(NUM_TRAINING_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def select_lot(worker_dataset):\n",
    "    \"\"\"\n",
    "    Build the lot by sampling over the dataset\n",
    "    \"\"\"\n",
    "    #- select indices in worker_dataset of tensors \n",
    "    valid_ids = np.arange(len(worker_dataset)-1) \n",
    "    #- Select indices and reshape into batches\n",
    "    batches_ids = np.random.choice(valid_ids,size=LOT_SIZE, replace=False).reshape(-1, args.batch_size)\n",
    "    #- Build lot\n",
    "    lot = []\n",
    "    for batch_ids in batches_ids:\n",
    "        batch_data = []\n",
    "        batch_target = []\n",
    "        for batch_id in batch_ids:\n",
    "            data, target = worker_dataset[batch_id]\n",
    "            batch_data.append(data)\n",
    "            batch_target.append(target)\n",
    "        \n",
    "        lot.append((torch.stack(batch_data), torch.stack(batch_target)))\n",
    "    return lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(worker_idx, model, optimizer, lot_idx):\n",
    "    # Build the lot by sampling over the dataset\n",
    "    worker_dataset = remote_dataset[worker_idx]\n",
    "    lot = select_lot(worker_dataset)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Iterate on the lot batch per batch\n",
    "    for batch_idx, (data,target) in enumerate(lot):\n",
    "        # update the model\n",
    "        \n",
    "        #send the model to the worker\n",
    "        worker = data.location\n",
    "        print(\"DATA LOCATION: \" + str(data.location))\n",
    "        model.send(worker)\n",
    "        #debug\n",
    "        for param in model.parameters():\n",
    "            if not (param.id_at_location in worker._objects.keys()):\n",
    "                print(param)\n",
    "                print(\"Param Id: \" + str(param.id_at_location))\n",
    "                print(worker._objects.keys())\n",
    "            assert(param.id_at_location in worker._objects.keys())\n",
    "        pred = model(data)\n",
    "\n",
    "        loss = F.mse_loss(pred, target.float())\n",
    "        # Note that because we apply backward() several times without resetting \n",
    "        # the grads (optimizer.zero_grad()), we sum the gradients \n",
    "        loss.backward() \n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            loss.get()\n",
    "            print('Train Lot: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                lot_idx, batch_idx * args.batch_size, LOT_SIZE,\n",
    "                100. * batch_idx * args.batch_size / LOT_SIZE, loss.data[0]))\n",
    "            print(priv_accountant.get_privacy_spent(target_deltas=[spent_delta]))\n",
    "       \n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    priv_accountant.accumulate_privacy_spending(bound * sigma, LOT_SIZE)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Federated Learning training\n",
    "def train(lot_idx):        \n",
    "    # update remote models\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        models[remote_index].train()\n",
    "        models[remote_index].send(compute_nodes[remote_index])\n",
    "        print(\"LOT number: \" + str (lot_idx))\n",
    "        print(\"Remote_index: \"+ str(remote_index))\n",
    "        models[remote_index] = update(remote_index, models[remote_index], optimizers[remote_index], lot_idx)\n",
    "\n",
    "    new_params = list()\n",
    "\n",
    "    for param_i in range(len(params[0])):\n",
    "\n",
    "        spdz_params = list()\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            spdz_params.append((params[remote_index][param_i].data+0).fix_precision().share(bob, alice).get())\n",
    "\n",
    "       # new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "        new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "        new_params.append(new_param)\n",
    "\n",
    "    for model in params:\n",
    "        for param in model:\n",
    "            param.data *= 0\n",
    "\n",
    "    for remote_index, model in enumerate(models):\n",
    "        model.get()\n",
    "        model.divide_clip_grads()\n",
    "        model.add_noise_to_grads()\n",
    "        model.send(compute_nodes[remote_index])\n",
    "\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        for param_index in range(len(params[remote_index])):\n",
    "            params[remote_index][param_index].data.set_(new_params[param_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_gUp-uFfwGL"
   },
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNED3GD6Y3Va"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmvTEpIbfzoC"
   },
   "source": [
    "# Training The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1094
    },
    "colab_type": "code",
    "id": "9JWkpGtoY48Y",
    "outputId": "4c6fbc4d-29f4-42bb-cd43-e7a76d8ea04e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "LOT number: 1\n",
      "Remote_index: 0\n",
      "DATA LOCATION: <syft.core.workers.virtual.VirtualWorker id:bob>\n",
      "Parameter containing:FloatTensor[_PointerTensor - id:9514100049 owner:0 loc:bob id@loc:99345430526]\n",
      "Param Id: 4215937605\n",
      "dict_keys([38055227604, 2502488806, 31299956160, 95867909540, 70132257572, 43151358781, 22970661714, 4600909910, 8977475118, 14558501983, 98953060477, 96585027247, 7803534624, 53531350996, 94744651793, 50743002523, 19140874110, 99213740109, 77187647414, 57940585027, 46460525656, 69983098331, 41162072773, 53064050138, 42531165268, 89932534062, 10597746684, 52352559364, 88947841680, 60854081447, 15311174941, 93442574190, 24525684463, 4063087290, 56561787164, 33778733302, 25515241214, 85320580068, 94133156376, 25312430229, 88642945681, 89832632899, 53735569920, 78690606291, 92728683149, 1262397789, 62468171334, 43375830962, 72560534195, 21904711029, 72241638295, 43972176126, 71461438361, 20769329852, 78498432502, 28800700527, 89234844981, 30856094039, 42211554756, 67378393685, 46107608444, 13595610331, 67034105334, 46470038165, 69855233993, 76527172699, 32287659659, 66841318864, 94749727008, 2472103221, 46871252572, 66154986510, 61727565747, 3054915989, 22947167618, 47054489216, 18871595318, 41970881004, 1200625029, 42482035631, 73879277094, 45685735505, 27504171155, 45296228261, 55898552343, 70648909850, 19316550784, 73637185467, 86648433118, 46392860290, 63546261632, 63184055674, 76137550265, 29671790212, 16943375171, 45624590381, 23805420958, 80220064136, 3419803723, 93705340838, 41571433584, 87661519393, 38286820304, 8839531617, 8630031728, 10793178985, 4126569576, 72320264441, 97007584915, 89861164730, 69581209963, 39730985405, 29517560634, 64119194976, 35523636191, 4867711516, 9186221936, 50661248942, 36110977368, 51167397459, 3774665178, 48413633838, 12879932617, 33022159999, 74926315111, 40755181535, 52802401835, 57906341823, 14663129181, 84626023025, 30002762495, 23987654204, 36470852139, 80355086129, 57630602641, 36253444362, 2923465845, 33817151215, 64985359314, 19944759789, 51783579360, 1687933603, 45928361181, 46367244842, 46964511080, 15631281972, 88900312088, 28727542202, 43592914355, 10412485542, 20654311907, 30741833397, 81289853058, 27275196702, 97175383192, 4397693439, 94625556113, 58421302228, 74851433163, 32022102801, 10774033843, 59666920368, 90288383052, 1968627247, 46619730307, 13396832657, 44849780662, 62314223872, 69467543252, 97979641085, 17041222350, 12281716019, 7449798987, 94361493586, 47421333686, 67357495908, 78929843285, 84392841180, 34855200335, 49055526888, 47101953604, 46860340551, 36722116157, 30168811990, 33001370311, 65999750864, 43807532574, 94925630735, 2857551859, 22916515737, 81999170568, 1609622244, 56916258017, 37404712670, 97974825563, 99484455067, 35143505250, 48154352820, 4070877520, 13408501624, 23848973712, 89952589336, 97183396891, 86483570456, 36779335289, 17562424410, 54006371705, 1901507922, 43832486315, 3684048150, 97636090259, 68695856838, 91977321485, 92851346564, 30235884902, 55653868560, 18224020215, 65249246846, 29009998164, 87912327728, 5736893700, 17457377467, 56627114681, 86479159002, 41572314673, 55031583672, 36886270850, 20363781949, 46814725960, 74050066929, 75406271790, 59621982263, 2625685992, 8653686063, 6017335796, 5144495327, 4649817227, 5540724230, 8285288719, 1306119480, 9671183525, 6578804582, 1436714379, 1752372555, 2668352636, 8697146870, 4016339406, 7531884388, 5704901604, 4139351661, 1342889623, 6420893896, 5662218565, 2265042912, 7534897839, 6531888126])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9833a601e930>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lot_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOT number: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlot_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Remote_index: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlot_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2c831563a711>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(worker_idx, model, optimizer, lot_idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Param Id: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(epoch)\n",
    "    for lot_idx in range(1, N_LOTS):\n",
    "        train(lot_idx)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Syft) Denver_Boston_Federated_Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
