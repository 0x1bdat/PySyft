{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Owner's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note:**\n",
    "\n",
    "Much of the code used here is either copied or adapted from the `Word-level language modeling` PyTorch example:\n",
    "\n",
    "https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "The goal being to demonstrate how the original example could be adapted to a context where the dataset is private to the data owner as it is the case in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 0: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Launch Duet Server and Connect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by launching the duet server. This server is launched by the data owner. The data scientist will connect to it afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duet = sy.launch_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class to hold the vocab of the dataset, with some utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    \"\"\"This class holds the vocabulary along\n",
    "    with some utility functions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Adds a word to the vocab.\n",
    "        \"\"\"\n",
    "        \n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            \n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the used vocab\n",
    "        \"\"\"\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a class that preprocesses the dataset, and prepares it for both training and testing. In this particular use case, preprocessing includes tokenization and transforming words into integer IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self._tokenize(os.path.join(path, \"train.txt\"))\n",
    "        self.valid = self._tokenize(os.path.join(path, \"valid.txt\"))\n",
    "        self.test = self._tokenize(os.path.join(path, \"test.txt\"))\n",
    "\n",
    "    def _tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        \n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        # Add words to the dictionary\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            \n",
    "            idss = []\n",
    "            \n",
    "            for line in f:\n",
    "                \n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                \n",
    "                ids = []\n",
    "                \n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "                    \n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "                \n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset instance for each of training, validation and testing, batchify, and share them on Grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "corpus = Corpus(path = 'data/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag and describe the datasets before sharing on Duet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "corpus.train.tag(\"wikitext2_dataset\", 'train_data')\n",
    "corpus.train.describe(f\"Wikitext2 training set. shape: ({corpus.train.shape[0]},)\")\n",
    "\n",
    "# Validation set\n",
    "corpus.valid.tag(\"wikitext2_dataset\", 'valid_data')\n",
    "corpus.valid.describe(f\"Wikitext2 validation set. shape: ({corpus.valid.shape[0]},)\")\n",
    "\n",
    "# Test set\n",
    "corpus.test.tag(\"wikitext2_dataset\", 'test_data')\n",
    "corpus.test.describe(f\"Wikitext2 test set. shape: ({corpus.test.shape[0]},)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vocabulary size to share it on Duet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = sy.lib.python.Int(len(corpus.dictionary))\n",
    "vocab_size.tag('wikitext2_dataset', 'vocab_size')\n",
    "vocab_size.describe('Vocabulary size of Wikitext2 dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Share Dataset on Duet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share the datasets on Grid\n",
    "corpus.train.send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.valid.send(duet, searchable=True)\n",
    "#corpus.test.send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share the vocab size\n",
    "vocab_size.send(duet, searchable = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the shared objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically approve all requests for the sake of this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_handler = {\n",
    "#     \"request_name\": \"age_data\",\n",
    "    \"timeout_secs\": -1,\n",
    "    \"action\": \"accept\",\n",
    "    \"print_local\": True,\n",
    "    \"log_local\": True\n",
    "}\n",
    "duet.requests.add_handler(accept_handler)\n",
    "\n",
    "# while True:\n",
    "#     if duet.requests:\n",
    "#         for request in duet.requests:\n",
    "#             request.approve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.requests.handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
