{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiText - Syft Duet - Data Owner ðŸŽ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used here is has been adapted directly from the `Word-level language modeling RNN\n",
    "` PyTorch example:\n",
    "https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "The goal is to demonstrate how the original example could be adapted to a context where you as a Data Owner can load and share your private data securely, to allow the Data Scientist to train on it over a Duet session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Launch a Duet Server and Connect\n",
    "\n",
    "As a Data Owner, you want to allow someone else to perform data science on data that you own and likely want to protect.\n",
    "\n",
    "In order to do this, we must load our data into a locally running server within this notebook. We call this server a \"Duet\".\n",
    "\n",
    "To begin, you must launch Duet and help your Duet \"partner\" (a Data Scientist) connect to this server.\n",
    "\n",
    "You do this by running the code below and sending the code snippit containing your unqiue Server ID to your partner and following the instructions it gives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "duet = sy.launch_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_do.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class to hold the vocab of the dataset, with some utility methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    \"\"\"This class holds the vocabulary along with some utility functions.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"Adds a word to the vocab.\"\"\"\n",
    "        \n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            \n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the size of the used vocab\"\"\"\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a class that preprocesses the dataset, and prepares it for both training and testing. In this particular use case, preprocessing includes tokenization and transforming words into integer IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self._tokenize(os.path.join(path, \"train.txt\"))\n",
    "        self.valid = self._tokenize(os.path.join(path, \"valid.txt\"))\n",
    "        self.test = self._tokenize(os.path.join(path, \"test.txt\"))\n",
    "\n",
    "    def _tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        \n",
    "        assert os.path.exists(path)\n",
    "        \n",
    "        # Add words to the dictionary\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            \n",
    "            for line in f:\n",
    "                words = line.split() + [\"<eos>\"]\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    ids.append(self.dictionary.word2idx[word])\n",
    "\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset instance for each of training, validation and testing, batchify, and share them with Duet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "corpus = Corpus(path = \"./original/data/wikitext-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to Tag and Describe the datasets before sharing on Duet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "corpus.train.tag(\"wikitext2_dataset\", \"train_data\")\n",
    "corpus.train.describe(f\"Wikitext2 training set. shape: ({corpus.train.shape[0]},)\")\n",
    "\n",
    "# Validation set\n",
    "corpus.valid.tag(\"wikitext2_dataset\", \"valid_data\")\n",
    "corpus.valid.describe(f\"Wikitext2 validation set. shape: ({corpus.valid.shape[0]},)\")\n",
    "\n",
    "# Test set\n",
    "corpus.test.tag(\"wikitext2_dataset\", \"test_data\")\n",
    "corpus.test.describe(f\"Wikitext2 test set. shape: ({corpus.test.shape[0]},)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vocabulary size to share it on Duet. If we use a syft primitive we can tag and send this directly to Duet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = sy.lib.python.Int(len(corpus.dictionary))\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size.tag(\"wikitext2_dataset\", \"vocab_size\")\n",
    "vocab_size.describe(\"Vocabulary size of Wikitext2 dataset\")\n",
    "vocab_size.tags, vocab_size.description\n",
    "\n",
    "# Don't forget to share the vocab size\n",
    "vocab_size.send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Share Dataset on Duet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share the datasets on Duet and make them visible to the Data Scientist with searchable=True\n",
    "corpus.train.send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.valid.send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see all the data we just created in the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Set Accept Handlers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatically approve all requests for the sake of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.requests.add_handler(\n",
    "    action=\"accept\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
