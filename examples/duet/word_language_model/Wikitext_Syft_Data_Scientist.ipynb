{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiText - Syft Duet - Data Scientist ü•Å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code used here is has been adapted directly from the `Word-level language modeling RNN\n",
    "` PyTorch example:\n",
    "https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "The goal is to demonstrate how the original example could be adapted to a context where you as a Data Scientist can access the remote private data of a Data Owner securely, and train a model over a Duet session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "# duet = sy.join_duet(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "# sy.logging(disable=True, file_path=\"wiki_ds.log\")\n",
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Get Pointers to Shared Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is to get our Data Owner to run their notebook so that the data is loaded into Duet. Once that is done we can check the store to see what pointers we can get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we should do is get a copy of the vocab_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_ptr = duet.store[\"vocab_size\"]\n",
    "vocab_size_sy = vocab_size_ptr.get(\n",
    "    request_block = True,\n",
    "    name=\"vocab_size\",\n",
    "    reason=\"I need it to define my model.\",\n",
    "    timeout_secs=30,\n",
    "    delete_obj=False,\n",
    "    verbose=True\n",
    ")\n",
    "vocab_size_sy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we get back a Syft Int. These primitive types are almost identical however in some\n",
    "cases you will need to convert them for use in other code. You could cast `int(vocab_size)` or you can use the method `upcast()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vocab_size_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = vocab_size_sy.upcast()\n",
    "type(vocab_size), vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should get some pointers to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = duet.store[\"train_data\"]\n",
    "valid_set = duet.store[\"valid_data\"]\n",
    "train_data, valid_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation sets, as shared by the data owners, are flat tensors of the form:\n",
    "\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 .....]\n",
    "```\n",
    "\n",
    "where the integers represent words.\n",
    "\n",
    "The DS (Data Scientist) here has the responsiblity of batchifying this dataset to serve training. I avoid giving the DO (Data Owner) this responsiblity as I suppose that it is up to the DS to decide how data is batchified.\n",
    "\n",
    "I suggest we reshape it in the following way:\n",
    "\n",
    "1. Reshape into a list of input/target samples:\n",
    "\n",
    "```\n",
    "[ \n",
    "  [ [1 , 2 , 3 , 4 ],\n",
    "    [5 , 6 , 7 , 8 ]  ],\n",
    "    \n",
    "  [ [9 , 10, 11, 12], \n",
    "    [13, 14, 15, 16]  ],\n",
    "  :\n",
    "  :\n",
    "  :\n",
    "]\n",
    "```\n",
    "\n",
    "This should use the `view()` method in `torch`.\n",
    "\n",
    "2. Create a `Dataloader` object on the DS side that batchified this training set. For example, for a batch size of 2, the data loader should return:\n",
    "\n",
    "```\n",
    "Input batch:\n",
    "     [ [1 , 2 , 3 , 4 ],\n",
    "       [9 , 10, 11, 12]  ]\n",
    "       \n",
    "Target batch:\n",
    "     [ [5 , 6 , 7 , 8 ],\n",
    "       [13, 14, 15, 16]  ]\n",
    "```\n",
    "\n",
    "All of these operations are carried out on the `TensorPointer` because the dataset does not leave the DO's machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need a few predetermined hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens_train = 2088628\n",
    "\n",
    "bsz = 4\n",
    "bptt = 2\n",
    "#dropout = 0.5\n",
    "\n",
    "#\n",
    "ninp = 100\n",
    "\n",
    "# Size of hidden layer\n",
    "nhid = 200\n",
    "\n",
    "# Number of RNN layer\n",
    "nlayers = 2\n",
    "\n",
    "# Initial learning rate\n",
    "lr = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to mix a local `torch` Dataset class but feed in remote `TensorPointer` objects to batchify. Take care to remember not to confuse `Tensor` and `TensorPointer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikitext2(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, ntokens, bsz, bptt):\n",
    "        # A pointer to the tensor that contains the list of \n",
    "        # all token IDs in the dataset\n",
    "        self.tokens = tokens\n",
    "\n",
    "        # The sequence length\n",
    "        self.bptt = bptt\n",
    "\n",
    "        # The batch size\n",
    "        self.bsz = bsz\n",
    "\n",
    "        # Number of tokens in the dataset\n",
    "        self.ntokens = ntokens\n",
    "        \n",
    "        # Batchify the dataset\n",
    "        self._batchify()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input, target = self._get_batch(index)\n",
    "        return input, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.ntokens // self.bsz) - (self.bptt + 1)\n",
    "    \n",
    "    def _batchify(self):  \n",
    "        # Since we are going to reshape the self.tokens of 1D tensor\n",
    "        # into a 2D tensor with a number of rows equal to the\n",
    "        # batch size, we should compute the number of columns\n",
    "        # of that reshaped tensor\n",
    "        width = self.ntokens // self.bsz\n",
    "\n",
    "        # remove surplus tokens\n",
    "        self.tokens_2d = self.tokens.narrow(0, 0, self.bsz * width)\n",
    "\n",
    "        # Reshape\n",
    "        self.tokens_2d = self.tokens_2d.view(-1, self.bsz)\n",
    "        \n",
    "    def _get_batch(self, index):\n",
    "        input = self.tokens_2d.narrow(dim = 0, start = index, length = self.bptt)\n",
    "        target = self.tokens_2d.narrow(dim = 0, start = index + 1, length = self.bptt)\n",
    "\n",
    "        return input, target.view(-1)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        return batch[0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a torch `Dataset` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Wikitext2(\n",
    "    tokens = train_data, \n",
    "    ntokens = ntokens_train, \n",
    "    bsz = bsz,\n",
    "    bptt = bptt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `DataLoader` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=1, # Should be always set to 1\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_set.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader is ready to use. Now, let's build our RNN model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Built an RNN-based Remote Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model. Take note that we are subclassing `sy.Module` not `nn.Module` and we are passing in `torch_ref`. Inside the model definition you need to use this self.torch_ref when ever referencing anything from torch. Internally this gets swapped between the real `torch` and the `remote_torch` on `duet.torch` so that the model and its definition can work in both environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(sy.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        torch_ref, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False\n",
    "    ):\n",
    "        super(RNNModel, self).__init__(torch_ref=torch_ref)\n",
    "        print(\n",
    "            \"Creating RNNModel with hyperparams: \"\n",
    "            + f\"{rnn_type} {ntoken} {ninp} {nhid} {nlayers} {dropout} {tie_weights}\"\n",
    "        )\n",
    "        self.ntoken = ntoken\n",
    "        #self.drop = self.torch_ref.nn.Dropout(dropout)\n",
    "        self.encoder = self.torch_ref.nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in [\"LSTM\", \"GRU\"]:\n",
    "            self.rnn = getattr(self.torch_ref.nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {\"RNN_TANH\": \"tanh\", \"RNN_RELU\": \"relu\"}[rnn_type]\n",
    "                \n",
    "            except KeyError:\n",
    "                raise ValueError(\"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = self.torch_ref.nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = self.torch_ref.nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError(\"When using the tied flag, nhid must be equal to emsize\")\n",
    "            # I dont think we can just assign these pointers right now\n",
    "            # self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        #self.init_weights()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.torch_ref.nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "#         self.torch_ref.nn.init.zeros_(self.decoder.weight)\n",
    "#         self.torch_ref.nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input, hidden = x\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        emb = self.encoder(input)\n",
    "\n",
    "        result = self.rnn(emb)\n",
    "        output, hidden = result[0], result[1]\n",
    "        #output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        output = self.torch_ref.nn.functional.log_softmax(decoded, dim=1) #, hidden\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model = RNNModel(\n",
    "    torch_ref=torch,\n",
    "    rnn_type = \"LSTM\", \n",
    "    ninp = ninp, \n",
    "    ntoken = vocab_size, \n",
    "    nhid = nhid, \n",
    "    nlayers = nlayers\n",
    ")\n",
    "\n",
    "print(f\"local_model is Local: {local_model.is_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try sending our model over to duet. Then we can check its `.is_local` property to see where it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_model = local_model.send(duet)\n",
    "print(f\"remote_model is Remote: {not remote_model.is_local}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is on the DO's machine we can get the remote parameters which are the ones we will want to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parameters as a pointer\n",
    "parameters = remote_model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to do a few things with the `remote_torch` so lets grab an alias to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_torch = duet.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optim = remote_torch.optim.Adadelta(parameters, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Start Remote Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=1, # Should be always set to 1\n",
    "    num_workers=0,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_set.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn on training mode\n",
    "dry_run = True\n",
    "\n",
    "remote_model.train()\n",
    "# train_loss = duet.python.Float(0)  # create a remote Float we can use for summation\n",
    "epochs = 10\n",
    "log_interval = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = remote_model((input, None))\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = remote_torch.nn.functional.nll_loss(input=output, target=target)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "\n",
    "        loss_item = loss.item()\n",
    "        #train_loss += loss_item # its still a pointer at this stage\n",
    "\n",
    "        # Update waits \n",
    "        optim.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            local_loss = None\n",
    "            local_loss = loss_item.get(\n",
    "                name=\"loss\",\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=5,\n",
    "                verbose=True\n",
    "            )\n",
    "            if local_loss is not None:\n",
    "                print(\"Train Epoch: {} {} {:.4}\".format(epoch, batch_idx, local_loss))\n",
    "            else:\n",
    "                print(\"Train Epoch: {} {} ?\".format(epoch, batch_idx))\n",
    "\n",
    "            if dry_run:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____ Local model test ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        #self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input, hidden = x\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        emb = self.encoder(input)\n",
    "\n",
    "        result = self.rnn(emb)\n",
    "        output, hidden = result[0], result[1]\n",
    "        \n",
    "        #output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        \n",
    "        return nn.functional.log_softmax(decoded, dim=1)#, hidden\n",
    "        #return result\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type = 'LSTM', ninp = 100, ntoken = 100, nhid = 100, nlayers =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(20,2,dtype = torch.long) # bsz * bptt\n",
    "hidden = torch.zeros(2,20, 100) # nb_layers * bptt * nhid\n",
    "c = torch.zeros(2, 20, 100) # nb_layers * bptt * nhid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model((input, hidden))\n",
    "#output, hidden = model((input, [hidden, c]))\n",
    "output= model((input, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.nll_loss(output, input.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining issues\n",
    "\n",
    "1. passing tuple of (hidden, c) to self.rnn gives exception in MergeFrom() call\n",
    "2. Dropout layer does not seems to be working\n",
    "3. when calling `input, hidden = self.rnn(input, None)` in LSTM case, we get two many values to unpack. This works in the local torch case. To solve the problem, I called it as `input = self.rnn(input, None)`\n",
    "4. We cannot index a list pointer `l = syft.lib.python.list.List([1,2]).send(duet); a = l[0]`\n",
    "5. We cannot unpack a list pointer `a,b = l`\n",
    "6. Couldn't implement gradient clipping since the function `torch.nn.utils.clip_grad_norm_()` is not yet implemented, and since we cannot iterate in `model.parameters()` when it is a `ListPointer`. Actually `model.parameters()` seems to be nonfunctional, it should return an iterator not a list in the allowlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.tensor([2,3,5])\n",
    "t2 = torch.tensor([4,1,6])\n",
    "\n",
    "list_ptr = sy.lib.python.list.List([t1,t2]).tag('#list').send(duet)\n",
    "elem_ptr = list_ptr[0] #Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_ptr, list_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = elem_ptr.get(request_block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = duet.torch.Tensor(elem_ptr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.get(request_block = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_ptr, t2_ptr = list_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
