{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data Scientist's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only data privacy is protected in this use case. The model created by the data scientist is not kept private since it is sent to the data owner's machine for training without being encrypted.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "Much of the code used here is either copied or adapted from the `Word-level language modeling` PyTorch example:\n",
    "\n",
    "https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "\n",
    "The goal being to demonstrate how the original example could be adapted to a context where the dataset is private to the data owner as it is the case in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 0: Connect to a Remote Duet Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before connecting to the remote duet server, the data owner should first launch a duet server. After launch, the data scientist can connect to the duet server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Get Pointers to Shared Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of the shared objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the size of the dataset's vocabulary. \n",
    "\n",
    "**Please choose the correct index from the above Pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = duet.store[1]\n",
    "vocab_size = vocab_size.get_copy(request_block = True)\n",
    "vocab_size = int(vocab_size)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get references to the datasets\n",
    "\n",
    "**Please choose the correct index from the above Pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = duet.store[0]\n",
    "#valid_set = duet.store[2]\n",
    "#train_set = duet.store['22d54a82-b7e7-40da-adfb-06a588121ba1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Prepare Datasets for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation sets, as shared by the data owners, are flat tensors of the form:\n",
    "\n",
    "```\n",
    "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 .....]\n",
    "```\n",
    "\n",
    "where the integers represent words.\n",
    "\n",
    "The data scientist here has the responsiblity of batchifying this dataset to serve training. I avoid giving the DO this responsiblity as I suppose that it is up to the DS to decide how data is batchified.\n",
    "\n",
    "I suggest to reshape in the following way:\n",
    "\n",
    "1. Reshape into a list of input/target samples:\n",
    "\n",
    "```\n",
    "[ \n",
    "  [ [1 , 2 , 3 , 4 ],\n",
    "    [5 , 6 , 7 , 8 ]  ],\n",
    "    \n",
    "  [ [9 , 10, 11, 12], \n",
    "    [13, 14, 15, 16]  ],\n",
    "  :\n",
    "  :\n",
    "  :\n",
    "]\n",
    "```\n",
    "\n",
    "This should use the `view()` method in `torch`.\n",
    "\n",
    "2. Create a `Dataloader` object in the DS side that batchified this training set. For example, for a batch size of 2, the data loader should return:\n",
    "\n",
    "```\n",
    "Input batch:\n",
    "     [ [1 , 2 , 3 , 4 ],\n",
    "       [9 , 10, 11, 12]  ]\n",
    "       \n",
    "Target batch:\n",
    "     [ [5 , 6 , 7 , 8 ],\n",
    "       [13, 14, 15, 16]  ]\n",
    "```\n",
    "\n",
    "Of course all operations are carried out on tensor pointers because the dataset does not quit the DO's node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Dataset class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix some hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens_train = 2088628\n",
    "\n",
    "bsz = 4\n",
    "bptt = 2\n",
    "#dropout = 0.5\n",
    "\n",
    "#\n",
    "ninp = 100\n",
    "\n",
    "# Size of hidden layer\n",
    "nhid = 200\n",
    "\n",
    "# Number of RNN layer\n",
    "nlayers =2\n",
    "\n",
    "# Initial learning rate\n",
    "lr = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikitext2(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, tokens, ntokens, bsz, bptt):\n",
    "        \n",
    "        # A pointer to the tensor that contains the list of \n",
    "        # all token IDs in the dataset\n",
    "        self.tokens = tokens\n",
    "        \n",
    "        # The sequence length\n",
    "        self.bptt = bptt\n",
    "        \n",
    "        # The batch size\n",
    "        self.bsz = bsz\n",
    "        \n",
    "        # Number of tokens in the dataset\n",
    "        self.ntokens = ntokens\n",
    "        \n",
    "        \n",
    "        # Batchify the dataset\n",
    "        self._batchify()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        input, target = self._get_batch(index)        \n",
    "        \n",
    "        return input, target\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "                \n",
    "        return (self.ntokens // self.bsz) - (self.bptt + 1)\n",
    "    \n",
    "    \n",
    "    def _batchify(self):  \n",
    "        \n",
    "        # Since we are going to reshape the self.tokens 1D tensor \n",
    "        # into a 2D tensor with a number of rows equal to the\n",
    "        # batch size, we should compute the number of columns\n",
    "        # of that reshaped tensor\n",
    "        width = self.ntokens // self.bsz\n",
    "\n",
    "        # remove surplus tokens\n",
    "        self.tokens_2d = self.tokens.narrow(0, 0, self.bsz * width)\n",
    "\n",
    "        # Reshape\n",
    "        self.tokens_2d = self.tokens_2d.view(-1, self.bsz)\n",
    "        \n",
    "        \n",
    "    def _get_batch(self, index):\n",
    "        \n",
    "        input = self.tokens_2d.narrow(dim = 0, start = index, length = self.bptt)\n",
    "        target = self.tokens_2d.narrow(dim = 0, start = index + 1, length = self.bptt)\n",
    "\n",
    "        return input, target.view(-1)\n",
    "    \n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        return batch[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a torch `Dataset` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Wikitext2(tokens = train_data, \n",
    "                      ntokens = ntokens_train, \n",
    "                      bsz = bsz,\n",
    "                      bptt = bptt,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `DataLoader` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, \n",
    "                                           batch_size=1, # Should be always set to 1\n",
    "                                           num_workers=0, \n",
    "                                           drop_last=True,\n",
    "                                           shuffle=True,\n",
    "                                           collate_fn = train_set.collate_fn\n",
    "    \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataloader is ready for use. Now, let's build the RNN model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: Built an RNN-based Remote Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a pointer to the remote torch and its modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_do = duet.torch\n",
    "nn = torch_do.nn\n",
    "F = torch_do.nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(sy.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        #self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input, hidden = x\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        emb = self.encoder(input)\n",
    "\n",
    "        result = self.rnn(emb)\n",
    "        output, hidden = result[0], result[1]\n",
    "        \n",
    "        #output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        \n",
    "        return F.log_softmax(decoded, dim=1)#, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type = 'LSTM', \n",
    "                 ninp = ninp, \n",
    "                 ntoken = vocab_size, \n",
    "                 nhid = nhid, \n",
    "                 nlayers = nlayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:11:39.070 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:11:44.072 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:11:49.073 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:11:54.075 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:11:59.076 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:04.079 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:09.082 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:14.086 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:19.089 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:24.091 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:29.094 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:34.096 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:39.098 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:44.102 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:49.104 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7269 / 7269 - CQ: 0 / 0 - AT: 4\n"
     ]
    }
   ],
   "source": [
    "# Get the parameters as a pointer to list\n",
    "parameters = model.parameters(params_list=duet.syft.lib.python.List())\n",
    "\n",
    "# Creates the optimizer\n",
    "optim = torch_do.optim.Adadelta(parameters, lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: Start Remote Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:54.107 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:12:59.110 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:13:04.112 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-11 19:13:09.114 | CRITICAL | syft.grid.connections.webrtc:heartbeat:563 - Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joiner PQ: 7329 / 7329 - CQ: 0 / 0 - AT: 4\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for iter, (input, target) in enumerate(train_loader):\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model((input, None))\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = F.nll_loss(input = output, target = target)\n",
    "    \n",
    "    # Backprop\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update waits \n",
    "    optim.step()\n",
    "\n",
    "    print(iter)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____ Local model test ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        #self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        #self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input, hidden = x\n",
    "        #emb = self.drop(self.encoder(input))\n",
    "        emb = self.encoder(input)\n",
    "\n",
    "        result = self.rnn(emb)\n",
    "        output, hidden = result[0], result[1]\n",
    "        \n",
    "        #output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        \n",
    "        return nn.functional.log_softmax(decoded, dim=1)#, hidden\n",
    "        #return result\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(rnn_type = 'LSTM', ninp = 100, ntoken = 100, nhid = 100, nlayers =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.ones(20,2,dtype = torch.long) # bsz * bptt\n",
    "hidden = torch.zeros(2,20, 100) # nb_layers * bptt * nhid\n",
    "c = torch.zeros(2, 20, 100) # nb_layers * bptt * nhid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = model((input, hidden))\n",
    "#output, hidden = model((input, [hidden, c]))\n",
    "output= model((input, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.functional.nll_loss(output, input.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining issues\n",
    "\n",
    "1. passing tuple of (hidden, c) to self.rnn gives exception in MergeFrom() call\n",
    "2. Dropout layer does not seems to be working\n",
    "3. when calling `input, hidden = self.rnn(input, None)` in LSTM case, we get two many values to unpack. This works in the local torch case. To solve the problem, I called it as `input = self.rnn(input, None)`\n",
    "4. We cannot index a list pointer `l = syft.lib.python.list.List([1,2]).send(duet); a = l[0]`\n",
    "5. We cannot unpack a list pointer `a,b = l`\n",
    "6. Couldn't implement gradient clipping since the function `torch.nn.utils.clip_grad_norm_()` is not yet implemented, and since we cannot iterate in `model.parameters()` when it is a `ListPointer`. Actually `model.parameters()` seems to be nonfunctional, it should return an iterator not a list in the allowlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "t1 = torch.tensor([2,3,5])\n",
    "t2 = torch.tensor([4,1,6])\n",
    "\n",
    "list_ptr = sy.lib.python.list.List([t1,t2]).tag('#list').send(duet)\n",
    "elem_ptr = list_ptr[0] #Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elem_ptr, list_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = elem_ptr.get(request_block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = duet.torch.Tensor(elem_ptr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.get(request_block = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_ptr, t2_ptr = list_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
