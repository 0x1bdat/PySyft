{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strange-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 32\n",
    "n_test_items = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modular-butter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compound-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 4\n",
    "        self.test_batch_size = 4\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polar-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "electoral-fabric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:93620973000 -> worker1:82655078790]\n",
      "\t-> [PointerTensor | me:69472800350 -> worker2:68559163561]\n",
      "\t*crypto provider: crypto_provider*\n",
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:71093437371 -> worker1:44834573280]\n",
      "\t-> [PointerTensor | me:45743744617 -> worker2:40031834581]\n",
      "\t*crypto provider: crypto_provider*\n",
      "Backward MAXPOOL2D\n"
     ]
    }
   ],
   "source": [
    "t = th.randint(1024, (4, 1, 8, 8))\n",
    "x = t.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "print(x)\n",
    "y = F.max_pool2d(x, 2)\n",
    "z = y.sum()\n",
    "print(z)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "floating-omega",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "          [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 1., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 1.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.float()\n",
    "t.requires_grad = True\n",
    "F.max_pool2d(t, 2).sum().backward()\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adequate-income",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "          [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 1., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 1.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.decrypt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "robust-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incomplete-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(x)  ## inverted!\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(x)  ## inverted!\n",
    "        x = x.reshape(-1, 256) # for some weird reason .view doesn't for after the 1st batch\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coupled-vienna",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "respiratory-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [0/32 (0%)]\tLoss: 0.983000\tTime: 42.449s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [4/32 (12%)]\tLoss: 0.895000\tTime: 37.550s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [8/32 (25%)]\tLoss: 0.930000\tTime: 39.006s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [12/32 (38%)]\tLoss: 0.978000\tTime: 41.594s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [16/32 (50%)]\tLoss: 0.996000\tTime: 38.482s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [20/32 (62%)]\tLoss: 0.938000\tTime: 38.578s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [24/32 (75%)]\tLoss: 0.898000\tTime: 52.449s\n",
      "SHAPE torch.Size([4, 256])\n",
      "FC1 weight torch.Size([100, 256])\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = torch.ones(4, 4, 4, 4)\n",
    "x = t.fix_precision().share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "F.max_pool2d(x, 2, return_indices=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-berkeley",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft_0.2.x",
   "language": "python",
   "name": "syft_0.2.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
