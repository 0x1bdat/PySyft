{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspended-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 64\n",
    "n_test_items = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chemical-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "suited-question",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 16\n",
    "        self.test_batch_size = 16\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 4\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "trying-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "upper-performance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:85658272934 -> worker1:19456649238]\n",
      "\t-> [PointerTensor | me:68430559625 -> worker2:61686137092]\n",
      "\t*crypto provider: crypto_provider*\n",
      "(Wrapper)>AutogradTensor>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
      "\t-> [PointerTensor | me:67035910710 -> worker1:93236807433]\n",
      "\t-> [PointerTensor | me:92520734188 -> worker2:35722020429]\n",
      "\t*crypto provider: crypto_provider*\n",
      "Backward MAXPOOL2D\n"
     ]
    }
   ],
   "source": [
    "t = th.randint(1024, (4, 1, 8, 8))\n",
    "x = t.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "print(x)\n",
    "y = F.max_pool2d(x, 2)\n",
    "z = y.sum()\n",
    "print(z)\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "joined-classroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "          [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 1., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 1.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = t.float()\n",
    "t.requires_grad = True\n",
    "F.max_pool2d(t, 2).sum().backward()\n",
    "t.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bizarre-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 1., 0.],\n",
       "          [0., 1., 0., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 1., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 1., 0., 1.],\n",
       "          [1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "          [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 1., 1., 0., 0., 0., 0., 1.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "          [0., 0., 1., 0., 0., 1., 1., 0.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 0.],\n",
       "          [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "          [1., 0., 0., 1., 0., 0., 0., 1.],\n",
       "          [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0., 0., 1., 0., 1.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.decrypt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "later-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flush-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(256, 100)\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(x)  ## inverted!\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(x)  ## inverted!\n",
    "        x = x.reshape(-1, 256) # for some weird reason .view doesn't for after the 1st batch\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "systematic-transformation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0860, -0.0510,  0.0000,  0.0000,  0.0000, -0.3610, -0.0210,  0.0000,\n",
      "        -0.2650, -0.0960,  0.0000,  0.0000,  0.0000, -0.4180, -0.1110,  0.0000])\n",
      "tensor([0.0290, 0.0400, 0.0160, 0.0100, 0.0670, 0.0180, 0.0250, 0.0460, 0.0440,\n",
      "        0.0560, 0.0400, 0.0480, 0.0510, 0.0430, 0.0520, 0.0640, 0.0740, 0.0720])\n",
      "tensor([[ 0.0590,  0.0720, -0.0060,  0.1200,  0.0140],\n",
      "        [ 0.0820,  0.0990, -0.0240, -0.0070,  0.0590],\n",
      "        [-0.0120,  0.0500,  0.0680,  0.0620,  0.0360],\n",
      "        [ 0.0260, -0.0030,  0.1120,  0.0550,  0.0090],\n",
      "        [ 0.0500,  0.0470, -0.0360,  0.0510,  0.0030]])\n",
      "tensor([[[-0.0610, -0.0430,  0.1020, -0.0320, -0.0380],\n",
      "         [ 0.0900,  0.0540,  0.0120,  0.0560,  0.0360],\n",
      "         [ 0.0220, -0.0530, -0.0560,  0.0020, -0.0070],\n",
      "         [-0.0530, -0.0590, -0.0120,  0.0150, -0.0630],\n",
      "         [-0.0410,  0.0810,  0.0260,  0.0170, -0.0110]]])\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "tensor([-0.0860, -0.0510,  0.0000,  0.0000,  0.0000, -0.3600, -0.0210,  0.0000,\n",
      "        -0.2640, -0.0960,  0.0000,  0.0000,  0.0000, -0.4170, -0.1110,  0.0000])\n",
      "tensor([0.0290, 0.0400, 0.0160, 0.0100, 0.0670, 0.0180, 0.0250, 0.0460, 0.0440,\n",
      "        0.0560, 0.0400, 0.0480, 0.0510, 0.0430, 0.0520, 0.0640, 0.0740, 0.0720])\n",
      "tensor([[ 0.0580,  0.0710, -0.0060,  0.1200,  0.0140],\n",
      "        [ 0.0820,  0.0980, -0.0240, -0.0070,  0.0580],\n",
      "        [-0.0120,  0.0500,  0.0680,  0.0620,  0.0360],\n",
      "        [ 0.0250, -0.0030,  0.1120,  0.0550,  0.0090],\n",
      "        [ 0.0500,  0.0470, -0.0370,  0.0510,  0.0030]])\n",
      "tensor([[[-0.0620, -0.0440,  0.0960, -0.0310, -0.0440],\n",
      "         [ 0.1050,  0.0580,  0.0100,  0.0480,  0.0300],\n",
      "         [ 0.0250, -0.0490, -0.0450,  0.0030, -0.0150],\n",
      "         [-0.0550, -0.0580, -0.0300,  0.0210, -0.0620],\n",
      "         [-0.0470,  0.0780,  0.0310,  0.0280, -0.0200]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## TEST Backprop\n",
    "batch_size = 2\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = Net()\n",
    "model.train()\n",
    "data = ((th.randint(1024, (batch_size, 1, 28, 28)).float() - 512) / 512)\n",
    "target = th.randint(2, (batch_size, 10)).float()\n",
    "\n",
    "output = model(data.float())\n",
    "loss = ((output - target)**2).sum()/batch_size\n",
    "loss.backward()\n",
    "\n",
    "print(np.round(model.fc2.weight.grad[5][10: 26], 3))\n",
    "print(np.round(model.fc1.weight.grad[5][8: 26], 3))\n",
    "print(np.round(model.conv2.weight.grad[0][3], 3))\n",
    "print(np.round(model.conv1.weight.grad[0], 3))\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = Net()\n",
    "model = model.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "data = data.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "target = target.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "output = model(data)\n",
    "loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "loss.backward()\n",
    "print(np.round(model.fc2.weight.grad.get().float_prec()[5][10: 26], 3))\n",
    "print(np.round(model.fc1.weight.grad.get().float_prec()[5][8: 26], 3))\n",
    "print(np.round(model.conv2.weight.grad.get().float_prec()[0][3], 3))\n",
    "print(np.round(model.conv1.weight.grad.get().float_prec()[0], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interim-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "editorial-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nervous-chess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [0/640 (0%)]\tLoss: 0.988000\tTime: 43.597s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [16/640 (2%)]\tLoss: 0.930000\tTime: 47.622s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [32/640 (5%)]\tLoss: 0.945000\tTime: 42.074s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [48/640 (8%)]\tLoss: 0.928000\tTime: 41.003s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [64/640 (10%)]\tLoss: 0.909000\tTime: 42.062s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [80/640 (12%)]\tLoss: 0.843000\tTime: 41.636s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [96/640 (15%)]\tLoss: 0.892000\tTime: 43.023s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [112/640 (18%)]\tLoss: 0.830000\tTime: 43.727s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [128/640 (20%)]\tLoss: 0.884000\tTime: 49.863s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [144/640 (22%)]\tLoss: 0.890000\tTime: 42.171s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [160/640 (25%)]\tLoss: 0.916000\tTime: 44.108s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [176/640 (28%)]\tLoss: 0.903000\tTime: 39.208s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.823000\tTime: 39.647s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [208/640 (32%)]\tLoss: 0.823000\tTime: 41.671s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [224/640 (35%)]\tLoss: 0.847000\tTime: 41.363s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [240/640 (38%)]\tLoss: 0.833000\tTime: 43.464s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.894000\tTime: 42.359s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [272/640 (42%)]\tLoss: 0.898000\tTime: 40.558s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [288/640 (45%)]\tLoss: 0.685000\tTime: 40.980s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [304/640 (48%)]\tLoss: 0.879000\tTime: 40.334s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.848000\tTime: 41.158s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [336/640 (52%)]\tLoss: 0.808000\tTime: 40.980s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [352/640 (55%)]\tLoss: 0.780000\tTime: 42.096s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [368/640 (58%)]\tLoss: 0.788000\tTime: 44.683s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.799000\tTime: 39.320s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [400/640 (62%)]\tLoss: 0.781000\tTime: 50.430s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [416/640 (65%)]\tLoss: 0.859000\tTime: 41.961s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [432/640 (68%)]\tLoss: 0.789000\tTime: 41.290s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.725000\tTime: 40.454s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [464/640 (72%)]\tLoss: 0.752000\tTime: 45.829s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [480/640 (75%)]\tLoss: 0.827000\tTime: 39.634s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [496/640 (78%)]\tLoss: 0.794000\tTime: 43.425s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.847000\tTime: 41.187s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [528/640 (82%)]\tLoss: 0.772000\tTime: 40.981s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [544/640 (85%)]\tLoss: 0.768000\tTime: 41.883s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [560/640 (88%)]\tLoss: 0.694000\tTime: 41.289s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.792000\tTime: 43.137s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [592/640 (92%)]\tLoss: 0.764000\tTime: 41.102s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [608/640 (95%)]\tLoss: 0.828000\tTime: 40.195s\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Backward RELU\n",
      "Backward MAXPOOL2D\n",
      "Backward CONV2D\n",
      "Train Epoch: 1 [624/640 (98%)]\tLoss: 0.730000\tTime: 40.745s\n",
      "\n",
      "Test set: Accuracy: 323.0/640 (50%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision(precision_fractional=args.precision_fractional).share(*workers, crypto_provider=crypto_provider, protocol=\"fss\", requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision(precision_fractional=args.precision_fractional) \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-condition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft_0.2.x",
   "language": "python",
   "name": "syft_0.2.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
