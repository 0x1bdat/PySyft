{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Support fetch plan + AST tensor\n",
    "import torch as th\n",
    "import syft as sy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "plan_func = True\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "me = hook.local_worker\n",
    "me.is_client_worker = False\n",
    "\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "charlie = sy.VirtualWorker(hook, id=\"charlie\")\n",
    "dan = sy.VirtualWorker(hook, id=\"dan\")\n",
    "\n",
    "if plan_func:\n",
    "    @sy.func2plan(args_shape=[(1,)], state={\"bias\": th.tensor([3.0])})\n",
    "    def plan_mult_3(x, state):\n",
    "        bias = state.read(\"bias\")\n",
    "        x = x + bias\n",
    "        return x\n",
    "else:\n",
    "    class Net(sy.Plan):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__(id=\"net\")\n",
    "            self.fc1 = nn.Linear(1, 1)\n",
    "            self.add_to_state([\"fc1\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc1(x)\n",
    "    \n",
    "    plan_mult_3 = Net()\n",
    "    plan_mult_3.build(th.tensor(1))\n",
    "    \n",
    "print([p for p in plan_mult_3.parameters()])\n",
    "sent_plan = plan_mult_3.send(alice).fix_prec().share(bob, charlie, crypto_provider=dan)\n",
    "\n",
    "# Fetch plan\n",
    "fetched_plan = alice.fetch_plan(sent_plan.id)\n",
    "x = th.tensor([1.])\n",
    "x_ptr = x.fix_prec().share(bob, charlie, crypto_provider=dan)\n",
    "\n",
    "# TODO: this should be stored automatically\n",
    "me._objects[x_ptr.id] = x_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this should be done internally\n",
    "new_state_ids = []\n",
    "for state_id in fetched_plan.state_ids:\n",
    "    # TODO: we should not have direct access to the weights\n",
    "    a_sh = me._objects[state_id].fix_prec().share(bob, charlie, crypto_provider=dan).get()\n",
    "    # TODO: this should be stored automatically\n",
    "    me._objects[a_sh.id] = a_sh\n",
    "    new_state_ids.append(a_sh.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_plan.replace_ids(fetched_plan.state_ids, new_state_ids)\n",
    "fetched_plan.state_ids = new_state_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_plan.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "print(fetched_plan(x_ptr).get().float_prec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "# Support fetching a plan\n",
    "\n",
    "import torch as th\n",
    "import syft as sy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "plan_func = True\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "me = hook.local_worker\n",
    "me.is_client_worker = False\n",
    "\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "\n",
    "if plan_func:\n",
    "    @sy.func2plan(args_shape=[(1,)], state={\"bias\": th.tensor([3.0])})\n",
    "    def plan_mult_3(x, state):\n",
    "        bias = state.read(\"bias\")\n",
    "        x = x * bias\n",
    "        return x\n",
    "else:\n",
    "    class Net(sy.Plan):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(1, 1)\n",
    "            self.add_to_state([\"fc1\"])\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc1(x)\n",
    "    \n",
    "    plan_mult_3 = Net()\n",
    "    plan_mult_3.build(th.tensor(1))\n",
    "\n",
    "sent_plan = plan_mult_3.send(alice)\n",
    "\n",
    "# Fetch plan\n",
    "fetched_plan = alice.fetch_plan(sent_plan.id)\n",
    "\n",
    "x = th.tensor([1.])\n",
    "print(fetched_plan(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
