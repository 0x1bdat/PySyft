{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping torch.torch.Tensor.__div__ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.__floordiv__ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.__rfloordiv__ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.bitwise_and not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.bitwise_and_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.bitwise_or not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.bitwise_or_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.div not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.div_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.floor_divide not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.floor_divide_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.is_meta not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.logical_and not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.logical_and_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.logical_or not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.logical_or_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.remainder not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.remainder_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.square not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.square_ not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.true_divide not supported in 1.4.0\n",
      "Skipping torch.torch.Tensor.true_divide_ not supported in 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import copy\n",
    "from sympy import symbols\n",
    "import random\n",
    "sy.VERBOSE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duet = sy.launch_duet()\n",
    "# duet = sy.launch_duet(budget_database='~./duet/privacy_budgets.db')\n",
    "# duet.store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def gen_privscal(value, entity, range_min=0., range_max=10.):\n",
    "    # generate private scalar\n",
    "    p_id = str(entity) + str(PrivateScalar.create_id())\n",
    "    x = {entity:{p_id:value}}\n",
    "    g = symbols(p_id)\n",
    "    f = {entity:{p_id:range_min}} \n",
    "    c = {entity:{p_id:range_max}}\n",
    "    return PrivateScalar(x, g, f, c, p_id)\n",
    "\n",
    "def concat_nestdicts(a, b):\n",
    "    # concatenate 2 nested dictionaries\n",
    "    x = {}\n",
    "    for ent, val_dict in a.items():\n",
    "        x[ent] = copy.deepcopy(val_dict)\n",
    "    for ent, val_dict  in b.items():\n",
    "        if ent not in x:\n",
    "            x[ent] = copy.deepcopy(val_dict)\n",
    "        else:\n",
    "            for val_id, val in val_dict.items():\n",
    "                x[ent][val_id] = val\n",
    "    return x\n",
    "\n",
    "def add_gaussian_noise(data, eps, l2_sens, delta):\n",
    "    mean = 0.0\n",
    "    variance = (2 * math.log(1.25 / delta) * (l2_sens ** 2)) / (epsilon ** 2)\n",
    "    st_dev = math.sqrt(variance)\n",
    "    noise = np.random.normal(loc=mean, scale=variance, size = len(data))\n",
    "    res = data + noise\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrivateScalar:\n",
    "    \n",
    "    def __init__(self, x, g, f, c, p_id):\n",
    "        \n",
    "        # We assume that each private scalar value corresponds to the contribution of a single entity to data\n",
    "        self.p_id = p_id\n",
    "        self.x = x # intermediate data\n",
    "        self.g = g # polynomial\n",
    "        self.f = f # lower bound val\n",
    "        self.c = c # upper bound class\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(\"<p_id: \" + self.p_id \n",
    "                   + \" x: \" + str(self.x)\n",
    "                   + \" g: \" + str(self.g)\n",
    "                   + \" f: \" + str(self.f)\n",
    "                   + \" c: \" + str(self.c)\n",
    "                   + \">\")\n",
    "    @staticmethod\n",
    "    def create_id():\n",
    "        return str(random.randint(0, 1e10))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        x = concat_nestdicts(self.x, other.x)\n",
    "        f = concat_nestdicts(self.f, other.f)\n",
    "        c = concat_nestdicts(self.c, other.c)\n",
    "        g = self.g + other.g\n",
    "        p_id = self.create_id()\n",
    "        return PrivateScalar(x, g, f, c, p_id)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        x = concat_nestdicts(self.x, other.x)\n",
    "        f = concat_nestdicts(self.f, other.f)\n",
    "        c = concat_nestdicts(self.c, other.c)\n",
    "        g = self.g * other.g\n",
    "        p_id = self.create_id()\n",
    "        return PrivateScalar(x, g, f, c, p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p_id: 9708218172 x: {'bob': {'bob5935859961': 1.5}, 'andrew': {'andrew3598768805': 3}} g: andrew3598768805*bob5935859961 f: {'bob': {'bob5935859961': 0.0}, 'andrew': {'andrew3598768805': 0.0}} c: {'bob': {'bob5935859961': 10.0}, 'andrew': {'andrew3598768805': 10.0}}>\n",
      "<p_id: 7888410950 x: {'bob': {'bob5935859961': 1.5}, 'andrew': {'andrew3598768805': 3}} g: andrew3598768805 + bob5935859961 f: {'bob': {'bob5935859961': 0.0}, 'andrew': {'andrew3598768805': 0.0}} c: {'bob': {'bob5935859961': 10.0}, 'andrew': {'andrew3598768805': 10.0}}>\n",
      "<p_id: bob5935859961 x: {'bob': {'bob5935859961': 1.5}} g: bob5935859961 f: {'bob': {'bob5935859961': 0.0}} c: {'bob': {'bob5935859961': 10.0}}>\n",
      "<p_id: andrew3598768805 x: {'andrew': {'andrew3598768805': 3}} g: andrew3598768805 f: {'andrew': {'andrew3598768805': 0.0}} c: {'andrew': {'andrew3598768805': 10.0}}>\n"
     ]
    }
   ],
   "source": [
    "# Example testing\n",
    "\n",
    "x = [1.5, 2, 3, 4, 5]\n",
    "entities = ['bob', 'kriti', 'andrew', 'bob', 'amber'] # every value corresponds to an entity\n",
    "num_entities = len(entities) # ??? number of unique entities instead\n",
    "priv_a = gen_privscal(x[0], entities[0])\n",
    "priv_b = gen_privscal(x[2], entities[2])\n",
    "print(priv_a * priv_b)\n",
    "print(priv_a + priv_b)\n",
    "print(priv_a)\n",
    "print(priv_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return (preds == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RDP_Accountant:\n",
    "    \n",
    "    # create an accountant instance per dataset / Tensor\n",
    "    def __init__(self, entities, num_unique_entities, ac_id, l2_norm, alpha, lipschitz, sigma, data_profile):\n",
    "        self.act_id = ac_id\n",
    "        self.map_type = data_profile # every row, column or value corresponds to an entity\n",
    "        self.entities = entities # dict, where every entity instance is recorded\n",
    "        self.num_unique_entities = num_unique_entities # count of the number of unique entities\n",
    "        self.num_entities = len(entities)\n",
    "        # num_entities >= num_unique_entities\n",
    "        \n",
    "        # below budget values are upper bounds (remaining at the start)\n",
    "        self.data_budget = th.tensor([1.] * num_entities)\n",
    "        self.global_data_budget = 10.0\n",
    "        self.scientist_budget = th.tensor([0.1] * num_entities)\n",
    "        self.global_scientist_budget = 2.0\n",
    "        \n",
    "        self.rdp_guarantee = th.tensor([0.] * num_entities)\n",
    "        self.global_rdp_guarantee = 0.\n",
    "        self.l2_norm = l2_norm # array of values per entity\n",
    "        self.alpha = alpha # alpha > 1.0 \n",
    "        self.lipschitz = lipschitz # array of values per entity\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # current value of budget spent per entity \n",
    "        self.eps = (alpha * (l2_norm ** 2) * (lipschitz ** 2)) / (2 * (sigma ** 2)) \n",
    "        # array of values per entity\n",
    "        \n",
    "    def update_budgets(self):\n",
    "        self.data_budget -= self.eps\n",
    "        self.rdp_guarantee = self.eps\n",
    "    \n",
    "    def private_Renyi_filter(self, max_rounds, total_budget):\n",
    "        # input: public approx list of chosen epsilons and deltas (per round), \n",
    "        #        public global epsilon & delta, round of computation t / T individual filtering \n",
    "        # output: Continue / Halt execution\n",
    "        return True\n",
    "    \n",
    "    def private_individual_Renyi_filter(self, max_rounds, ind_budget_bounds):\n",
    "        # input: public approx list of chosen epsilons and deltas (per round), \n",
    "        #        public global epsilon & delta, round of computation t / T individual filtering \n",
    "        # output: adaptively drop entities from the data\n",
    "        return True\n",
    "    \n",
    "    def public_approx_Renyi_odometer(self, max_rounds):\n",
    "        # gives a valid upper bound on the privacy loss incurred thus far\n",
    "        # input: round of computation t / T\n",
    "        \n",
    "        # add upper and lower bounds on the below values of budget remaining\n",
    "#         data_budget = th.tensor([1.] * num_entities)\n",
    "#         global_data_budget = 10.0\n",
    "#         scientist_budget = th.tensor([0.1] * num_entities)\n",
    "#         global_scientist_budget = 2.0\n",
    "        rdp_guarantee = tuple(alpha, eps)\n",
    "        return rdp_guarantee\n",
    "    \n",
    "    def within_budget():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figuring stuff out\n",
    "x = [1, 2, 3, 4]\n",
    "y = [2, 4, 6, 7]\n",
    "ent_x = [bob, andrew, amber, bob]\n",
    "y = sum(x) + x[0]\n",
    "y = 2 * bob[0] + andrew[0] + amber[0] # this is wrong\n",
    "\n",
    "# from sympy import symbols\n",
    "x, y = symbols('x y')\n",
    "print(x)\n",
    "print(y)\n",
    "print(y * (x * 2))\n",
    "\n",
    "expr = x + y\n",
    "# expr = expr.subs(x, 0)\n",
    "# expr = expr.subs(y, 1)\n",
    "expr.evalf(subs={x: 2.4, y:0})\n",
    "# expr\n",
    "# print(expr)\n",
    "\n",
    "# ages = th.tensor([23,52,31,16]).tag(\"ages\").private(every_value_unique_entity=True, # every_row_unique_entity=False,\n",
    "#                                             # every_column_unique_entity=False,\n",
    "#                                             max_value=125, min_value=0).send(duet, searchable=True)\n",
    "# # ages.entities by default equals = [acc.n_entities+0,acc.n_entities+1,acc.n_entities+2,acc.n_entities+3]\n",
    "# heights = th.tensor([150,160,140,100]).tag(\"heights\").private(every_value_unique_entity=True, max_value=250, \n",
    "#                                             min_value=0, entities=ages.entities).send(duet, searchable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # duet.budget\n",
    "\n",
    "# print(\"OVERALL - Per Scientist Entity Budget:\" + str(3))\n",
    "# print(\"OVERALL - Universal Entity Budget:\" + str(5))\n",
    "# print()\n",
    "# print(\"EST. REMAINING - MIN Universal Entity Budget:\" + str(4.2))\n",
    "# print(\"EST. REMAINING - MAX Universal Entity Budget:\" + str(4.9))\n",
    "# print(\"EST. REMAINING - MEAN Universal Entity Budget:\" + str(4.21))\n",
    "# print()\n",
    "# print(\"EST. REMAINING - MIN Per Scientist Entity Budget:\" + str(2.91))\n",
    "# print(\"EST. REMAINING - MAX Per Scientist Entity Budget:\" + str(3.0))\n",
    "# print(\"EST. REMAINING - MEAN Per Scientist Entity Budget:\" + str(2.92))\n",
    "\n",
    "# # estimated budget spend for this tensor\n",
    "# print(\"EST SPEND - MIN Per Scientist Entity Budget:\" + str(0))\n",
    "# print(\"EST SPEND - MAX Per Scientist Entity Budget:\" + str(0))\n",
    "# print(\"EST SPEND - MEAN Per Scientist Entity Budget:\" + str(0.12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle x + 1$"
      ],
      "text/plain": [
       "x + 1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class PrivateTensor:\n",
    "    \n",
    "#     # each row is a data point, number of data points = no. of columns. 2-D tensor\n",
    "#     def __init__(self, tensor, min_tensor, max_tensor, entity_cont, num_entities):\n",
    "#         self.data = tensor\n",
    "#         self.entity_cont = entity_cont # entity contribution to each data point \n",
    "#         # self.data[i] = sum over all j entity_cont[i][j]\n",
    "#         self.data_min = min_tensor\n",
    "#         self.data_max = max_tensor\n",
    "#         self.lipschitz = []\n",
    "#         self.l2_norm = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
