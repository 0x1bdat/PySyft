{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded constant2epsilon cache of size: (300000,)\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from syft.core.adp.entity_list import EntityList\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read file with 1000.0 million rows : 93.2172794342041 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "df = pd.read_parquet(\"/home/ruchi/1B_rows_dataset_sample.parquet\")\n",
    "# df = pd.read_parquet(\"../Ishan/1M_rows_dataset_sample.parquet\")\n",
    "tf = time() - t0\n",
    "\n",
    "print(f\"Time taken to read file with {df.shape[0]/1e6} million rows : {tf} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create inputs for Syft Tensor: 49.56277680397034 seconds\n"
     ]
    }
   ],
   "source": [
    "scale = 1000  # This is put here to reduce the size of the cache :)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "impressions = df['impressions'].to_numpy(np.int64) // scale\n",
    "data_subjects = EntityList.from_series(df['user_id'])\n",
    "tf = time() - t0\n",
    "\n",
    "print(f\"Time taken to create inputs for Syft Tensor: {tf} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time make Private Syft Tensor: 2.2971034049987793 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "tweets_data = sy.Tensor(impressions).private(min_val=700_000/scale, max_val=20e6/scale, entities = data_subjects,ndept=True)  # RUn this for 1 billion rows\n",
    "# tweets_data = sy.Tensor(impressions).private(min_val=500/scale, max_val=25000/scale, entities = data_subjects,ndept=True)  # Run this for 1 million rows\n",
    "tf = time() - t0\n",
    "\n",
    "print(f\"Time make Private Syft Tensor: {tf} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: CHANGE YOUR USERNAME AND PASSWORD!!! \n",
      "\n",
      "Anyone can login as an admin to your node right now because your password is still the default PySyft username and password!!!\n",
      "\n",
      "Connecting to None... done! \t Logging into sad_howard... done!\n"
     ]
    }
   ],
   "source": [
    "domain_node = sy.login(email=\"info@openmined.org\",password=\"changethis\",port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999999.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from syft.core.node.common.node_service.user_manager.user_messages import (\n",
    "    UpdateUserMessage,\n",
    ")\n",
    "\n",
    "# Upgrade admins budget\n",
    "content = {\"user_id\": 1, \"budget\": 9_999_999}\n",
    "domain_node._perform_grid_request(grid_msg=UpdateUserMessage, content=content)\n",
    "\n",
    "domain_node.privacy_budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... checking asset types...                                                                                                                                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Tweets data- 1648095079.271325: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:08<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset... uploading... SUCCESS!                        \n",
      "\n",
      "Run <your client variable>.datasets to see your new dataset loaded into your machine!\n",
      "CPU times: user 39.8 s, sys: 10.2 s, total: 50 s\n",
      "Wall time: 56.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "name = f\"Tweets data- {time()}\"\n",
    "\n",
    "domain_node.load_dataset(\n",
    "    assets={name: tweets_data},\n",
    "    name=name,\n",
    "    description=\" Tweets- 100M rows\",\n",
    "    use_blob_storage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                #myInput {\n",
       "                  background-position: 10px 12px; /* Position the search icon */\n",
       "                  background-repeat: no-repeat; /* Do not repeat the icon image */\n",
       "                  background-color: #bbb;\n",
       "                  width: 98%; /* Full-width */\n",
       "                  font-size: 14px; /* Increase font-size */\n",
       "                  padding: 12px 20px 12px 40px; /* Add some padding */\n",
       "                  border: 1px solid #ddd; /* Add a grey border */\n",
       "                  margin-bottom: 12px; /* Add some space below the input */\n",
       "                }\n",
       "\n",
       "                #myTable {\n",
       "                  border-collapse: collapse; /* Collapse borders */\n",
       "                  width: 100%; /* Full-width */\n",
       "                  border: 1px solid #ddd; /* Add a grey border */\n",
       "                  font-size: 14px; /* Increase font-size */\n",
       "                }\n",
       "\n",
       "                #myTable th, #myTable td {\n",
       "                  text-align: left; /* Left-align text */\n",
       "                  padding: 10px; /* Add padding */\n",
       "                }\n",
       "\n",
       "                #myTable tr {\n",
       "                  /* Add a bottom border to all table rows */\n",
       "                  border-bottom: 1px solid #ddd;\n",
       "                }\n",
       "\n",
       "                #myTable tr.header, #myTable tr:hover {\n",
       "                  /* Add a grey background color to the table header and on hover */\n",
       "                  background-color: #777;\n",
       "                }\n",
       "                </style>\n",
       "\n",
       "                <table id=\"myTable\" style=\"width:1000px\">\n",
       "                  <tr class=\"header\">\n",
       "                    <th style=\"width:30px\">Idx</th>\n",
       "                    <th style=\"width:20%;\">Name</th>\n",
       "                    <th style=\"width:35%;\">Description</th>\n",
       "                    <th style=\"width:20%;\">Assets</th>\n",
       "                    <th style=\"width:300px;\">Id</th>\n",
       "                  </tr>\n",
       "                \n",
       "\n",
       "          <tr>\n",
       "            <td>[0]</td>\n",
       "            <td>Tweets data- 1648091454.57071</td>\n",
       "            <td> Tweets- 1B rows</td>\n",
       "            <td>[\"Tweets data- 1648091454.57071\"] -> <class 'syft.core.tensor.tensor.Tensor'><br /><br /></td>\n",
       "            <td>7a0fd5ba-7dd5-4be7-82e0-2f0dcb029465</td>\n",
       "          </tr>\n",
       "\n",
       "          <tr>\n",
       "            <td>[1]</td>\n",
       "            <td>Tweets data- 1648094463.3827522</td>\n",
       "            <td> Tweets- 100M rows</td>\n",
       "            <td>[\"Tweets data- 1648094463.3827522\"] -> <class 'syft.core.tensor.tensor.Tensor'><br /><br /></td>\n",
       "            <td>4de29a72-a23a-474f-aaa8-c6ae20d483f4</td>\n",
       "          </tr>\n",
       "\n",
       "          <tr>\n",
       "            <td>[2]</td>\n",
       "            <td>Tweets data- 1648095079.271325</td>\n",
       "            <td> Tweets- 100M rows</td>\n",
       "            <td>[\"Tweets data- 1648095079.271325\"] -> <class 'syft.core.tensor.tensor.Tensor'><br /><br /></td>\n",
       "            <td>e7f5d697-87f5-433d-89a5-15dd0b8f2d0f</td>\n",
       "          </tr>\n",
       "        </table>\n",
       "\n",
       "        <script>\n",
       "        function myFunction() {\n",
       "          // Declare variables\n",
       "          var input, filter, table, tr, td, i, txtValue;\n",
       "          input = document.getElementById(\"myInput\");\n",
       "          filter = input.value.toUpperCase();\n",
       "          table = document.getElementById(\"myTable\");\n",
       "          tr = table.getElementsByTagName(\"tr\");\n",
       "\n",
       "          // Loop through all table rows, and hide those who don't match the search query\n",
       "          for (i = 0; i < tr.length; i++) {\n",
       "            name_td = tr[i].getElementsByTagName(\"td\")[1];\n",
       "            desc_td = tr[i].getElementsByTagName(\"td\")[2];\n",
       "            asset_td = tr[i].getElementsByTagName(\"td\")[3];\n",
       "            id_td = tr[i].getElementsByTagName(\"td\")[4];\n",
       "            if (name_td || desc_td || asset_td || id_td) {\n",
       "              name_txtValue = name_td.textContent || name_td.innerText;\n",
       "              desc_txtValue = desc_td.textContent || name_td.innerText;\n",
       "              asset_txtValue = asset_td.textContent || name_td.innerText;\n",
       "              id_txtValue = id_td.textContent || name_td.innerText;\n",
       "              name_bool = name_txtValue.toUpperCase().indexOf(filter) > -1;\n",
       "              desc_bool = desc_txtValue.toUpperCase().indexOf(filter) > -1;\n",
       "              asset_bool = asset_txtValue.toUpperCase().indexOf(filter) > -1;\n",
       "              id_bool = id_txtValue.toUpperCase().indexOf(filter) > -1;\n",
       "              if (name_bool || desc_bool || asset_bool || id_bool) {\n",
       "                tr[i].style.display = \"\";\n",
       "              } else {\n",
       "                tr[i].style.display = \"none\";\n",
       "              }\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "        </script>"
      ],
      "text/plain": [
       "<syft.core.node.common.client_manager.dataset_api.DatasetRequestAPI at 0x7f78e5cb8070>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_node.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = domain_node.datasets[-1][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 170 ms, total: 2.83 s\n",
      "Wall time: 1min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TensorPointer -> sad_howard:93b6f8d6f23249c5821b5f87a1804c66>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sum_result = data.sum()\n",
    "sum_result.block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_result.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_result = sum_result.publish(sigma=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_result.exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# published_result.request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_node.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_node.requests[-1].accept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# domain_node.requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_of_our_hard_labour = published_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WE GOT AN OVERFLOW ERROR\n",
    "- doesn't occur with 1M rows\n",
    "- publish had completed, tho it took 602 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_of_our_hard_labour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impressions.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_error(true_value, noisy_value):\n",
    "    return (true_value - noisy_value)/true_value * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage_error(true_value=impressions.sum(), noisy_value=result_of_our_hard_labour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1644527104 - 11941995258)/11941995258 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11941995258 /1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds_for_mechanism(\n",
    "     min_val_array, max_val_array\n",
    "):\n",
    "    \"\"\"Calculates the squared L2 norm values needed to create a Mechanism, and calculate\n",
    "    privacy budget + spend. If you calculate the privacy budget spend with the worst\n",
    "    case bound, you can show this number to the DS. If you calculate it with the\n",
    "    regular value (the value computed below when public_only = False, you cannot show\n",
    "    the privacy budget to the DS because this violates privacy.\"\"\"\n",
    "    l2_norm_min = np.sqrt(np.sum(np.square(min_val_array)))\n",
    "    l2_norm_max = np.sqrt(np.sum(np.square(max_val_array)))\n",
    "    return l2_norm_min, l2_norm_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_rdp_constants(\n",
    "    sigma, scale, min_val, max_val,  L=1\n",
    ") -> np.ndarray:\n",
    "    min_val = min_val/scale\n",
    "    max_val = max_val/scale\n",
    "    # print(min_val, max_val)\n",
    "    l2_norm_min, l2_norm_max = calculate_bounds_for_mechanism(min_val, max_val)\n",
    "    \n",
    "    # use the indices to get a \"batch\" of the full ledger. this is the only part\n",
    "    # of the ledger we care about (the entries corresponding to specific entities)\n",
    "    squared_Ls = L**2\n",
    "    squared_sigma = sigma**2\n",
    "    squared_L2_norms_min = l2_norm_min**2\n",
    "    squared_L2_norms_max = l2_norm_max**2\n",
    "    \n",
    "    constant_min = (\n",
    "        squared_Ls * squared_L2_norms_min / (2 * squared_sigma)\n",
    "    )\n",
    "    constant_max = (\n",
    "        squared_Ls * squared_L2_norms_max / (2 * squared_sigma)\n",
    "    )\n",
    "    \n",
    "#     constant = np.bincount(batch_entity_ids, weights=constant).take(\n",
    "#         entity_ids_query\n",
    "#     )\n",
    "    # # update our serialized format with the calculated constants\n",
    "    # self._rdp_constants = np.concatenate([self._rdp_constants, constant])\n",
    "    # self._entity_ids_query = np.concatenate(\n",
    "    #     [self._entity_ids_query, entity_ids_query]\n",
    "    # )\n",
    "    return constant_min, constant_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#_get_batch_rdp_constants(sigma=100, scale=1000, min_val=700_000, max_val=20e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "630449982b6186a6531308cd76ed4d510e9db65154e43844c2906c6a20ad2a6d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
