{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Rewrite overbudgeted entities code to use vectorized code (replace rows with 0s)\n",
    "- Figure out how to read/write to databases effectively?\n",
    "    - Make it so the ledger remembers what it got from the DB, and what it got locally (make a diff object, and make it so that it can read that effectively)\n",
    "\n",
    "Later:\n",
    "- Give each DataSubject a unique integer ID\n",
    "- Custom deltas? (please no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1647811044.2405317"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LedgerUpdate:\n",
    "    def __init__(self, sigmas, l2_norms, l2_norm_bounds, Ls, coeffs, entity_ids, update_number, timestamp):\n",
    "        self.sigmas = sigmas\n",
    "        self.l2_norms = l2_norms\n",
    "        self.l2_norm_bounds = l2_norm_bounds \n",
    "        self.Ls = Ls\n",
    "        self.coeffs = coeffs \n",
    "        self.entity_ids = entity_ids\n",
    "        self.update_number = update_number\n",
    "        self.timestamp = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "class DataSubjectLedger:\n",
    "    \"\"\"for a particular data subject, this is the list\n",
    "    of all mechanisms releasing informationo about this\n",
    "    particular subject, stored in a vectorized form\"\"\"\n",
    "    \n",
    "    def __init__(self, default_cache_size=1e3):\n",
    "        \n",
    "        self.delta = 1e-6  # WARNING: CHANGING DELTA INVALIDATES THE CACHE\n",
    "        self.reset()\n",
    "        self.cache_constant2epsilon = list()\n",
    "        self.increase_max_cache(int(default_cache_size))\n",
    "        \n",
    "        # save initial size (number of rows from DB) when deserialized\n",
    "        self.known_db_size = 0\n",
    "        self.update_number = 0\n",
    "        self.timestamp_of_last_update = None\n",
    "    \n",
    "    def write_to_db(self):\n",
    "        self.update_number += 1\n",
    "    \n",
    "        result = LedgerUpdate(sigmas=self.sigmas[self.known_db_size:],\n",
    "                            l2_norms=self.l2_norms[self.known_db_size:],\n",
    "                            l2_norm_bounds=self.l2_norms[self.known_db_size:],\n",
    "                            Ls=self.Ls[self.known_db_size:],\n",
    "                            coeffs=self.coeffs[self.known_db_size:],\n",
    "                            entity_ids=self.entity_ids[self.known_db_size:],\n",
    "                            update_number=self.update_number,\n",
    "                            timestamp=time.time()\n",
    "                           )\n",
    "        self.known_db_size += len(self.sigmas)\n",
    "        return result\n",
    "\n",
    "    def read_from_db(self, update: LedgerUpdate):\n",
    "        if update.update_number == self.update_number + 1:\n",
    "            if self.timestamp_of_last_update is not None and update.timestamp < self.timestamp:\n",
    "                raise Exception(\"It appears that updates were created out of order.\" +  \n",
    "                \"This is probably due to multiple python threads creating updates- which should NOT happen.\" + \n",
    "                \"This is a very serious error- please contact OpenMined immediately.\" + \"Thank you!\")\n",
    "            self.sigmas = np.concatenate([self.sigmas, update.sigmas])\n",
    "            self.l2_norms = np.concatenate([self.l2_norms, update.l2_norms])\n",
    "            self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, update.l2_norm_bounds]) \n",
    "            self.Ls = np.concatenate([self.Ls, update.Ls])\n",
    "            self.coeffs = np.concatenate([self.coeffs, update.coeffs]) \n",
    "            self.entity_ids = np.concatenate([self.entity_ids, update.entity_ids])\n",
    "            self.update_number = update.update_number\n",
    "            self.timestamp = update.timestamp\n",
    "        else:\n",
    "            raise Exception(\"Cannot add update to Ledger\")\n",
    "                                                   \n",
    "                                                   \n",
    "    def reset(self):\n",
    "        self.sigmas = np.array([])\n",
    "        self.l2_norms = np.array([])\n",
    "        self.l2_norm_bounds = np.array([])\n",
    "        self.Ls = np.array([])\n",
    "        self.coeffs = np.array([])\n",
    "        self.entity_ids = np.array([])\n",
    "        self.entity2budget = np.array([])\n",
    "        \n",
    "    def batch_append(self, \n",
    "                     sigmas: np.ndarray, \n",
    "                     l2_norms: np.ndarray, \n",
    "                     l2_norm_bounds: np.ndarray, \n",
    "                     Ls: np.ndarray, \n",
    "                     coeffs: np.ndarray, \n",
    "                     entity_ids: np.ndarray):\n",
    "        \n",
    "        self.sigmas = np.concatenate([self.sigmas, sigmas])\n",
    "        self.l2_norms = np.concatenate([self.l2_norms, l2_norms])        \n",
    "        self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, l2_norm_bounds])        \n",
    "        self.Ls = np.concatenate([self.Ls, Ls])        \n",
    "        self.coeffs = np.concatenate([self.coeffs, coeffs])               \n",
    "        self.entity_ids = np.concatenate([self.entity_ids, entity_ids])\n",
    "        \n",
    "    def increase_max_cache(self, new_size):\n",
    "        new_entries = []\n",
    "        current_size = len(self.cache_constant2epsilon)\n",
    "        for i in range(new_size - current_size):\n",
    "            alpha, eps = self.get_optimal_alpha_for_constant(i+1 + current_size)\n",
    "            new_entries.append(eps)\n",
    "        self.cache_constant2epsilon = np.concatenate([self.cache_constant2epsilon, np.array(new_entries)])\n",
    "        # print(self.cache_constant2epsilon)\n",
    "        \n",
    "    def get_fake_rdp_func(self, constant):\n",
    "        \n",
    "        def func(alpha):\n",
    "            return alpha * constant\n",
    "        \n",
    "        return func\n",
    "\n",
    "    def get_alpha_search_function(self, rdp_compose_func):\n",
    "            \n",
    "        # if len(self.deltas) > 0:\n",
    "            # delta = np.max(self.deltas)\n",
    "        # else:\n",
    "        log_delta = np.log(self.delta)\n",
    "        \n",
    "        def fun(alpha):  # the input is the RDP's \\alpha\n",
    "            \n",
    "            if alpha <= 1:\n",
    "                return np.inf\n",
    "            else:\n",
    "                alpha_minus_1 = alpha-1\n",
    "                return np.maximum(rdp_compose_func(alpha) + np.log(alpha_minus_1/alpha)\n",
    "                                  - (log_delta + np.log(alpha))/alpha_minus_1, 0)\n",
    "        return fun    \n",
    "    \n",
    "    def get_optimal_alpha_for_constant(self, constant=3):\n",
    "        \n",
    "        f = self.get_fake_rdp_func(constant)\n",
    "        f2 = self.get_alpha_search_function(rdp_compose_func=f)\n",
    "        results = minimize_scalar(f2, method='Brent', bracket=(1,2), bounds=[1, np.inf])\n",
    "        \n",
    "        return results.x, results.fun\n",
    "\n",
    "        \n",
    "    def get_batch_rdp_constants(self, entity_ids_query, private=True):\n",
    "        \n",
    "        # get indices for all ledger rows corresponding to any of the entities in entity_ids_query\n",
    "        indices_batch = np.where(np.in1d(self.entity_ids, entity_ids_query))[0]\n",
    "        \n",
    "        # use the indices to get a \"batch\" of the full ledger. this is the only part\n",
    "        # of the ledger we care about (the entries corresponding to specific entities)\n",
    "        batch_sigmas = self.sigmas.take(indices_batch)\n",
    "        batch_Ls = self.Ls.take(indices_batch)\n",
    "        batch_l2_norms = self.l2_norms.take(indices_batch)\n",
    "        batch_l2_norm_bounds = self.l2_norm_bounds.take(indices_batch)\n",
    "        batch_coeffs = self.coeffs.take(indices_batch)\n",
    "        batch_entity_ids = self.entity_ids.take(indices_batch).astype(np.int64)\n",
    "        \n",
    "        squared_Ls = batch_Ls**2\n",
    "        squared_sigma = batch_sigmas**2\n",
    "        \n",
    "        if private:\n",
    "            squared_L2_norms = batch_l2_norms**2\n",
    "            constant = (squared_Ls * squared_L2_norms / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        else:\n",
    "            squared_L2_norm_bounds = batch_l2_norm_bounds**2\n",
    "            constant = (squared_Ls * squared_L2_norm_bounds / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        \n",
    "    def get_epsilon_spend(self, entity_ids_query):\n",
    "        rdp_constants = self.get_batch_rdp_constants(entity_ids_query=entity_ids_query).astype(np.int64)\n",
    "        rdp_constants_lookup = rdp_constants - 1\n",
    "        try:\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        except IndexError:\n",
    "            self.increase_max_cache(int(max(rdp_constants_lookup) * 1.1))\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        return eps_spend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/PycharmProjects/PySyft/.tox/syft.jupyter/lib/python3.8/site-packages/scipy/optimize/_optimize.py:2782: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom\n"
     ]
    }
   ],
   "source": [
    "ledger = DataSubjectLedger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger.reset()\n",
    "n = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ledger.batch_append(sigmas=np.ones(n),\n",
    "                    l2_norms=np.ones(n)*10,\n",
    "                    l2_norm_bounds=np.ones(n)*40,\n",
    "                    Ls=np.ones(n)*5,\n",
    "                    coeffs=np.ones(n),\n",
    "                    entity_ids=np.arange(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = np.arange(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 ms, sys: 0 ns, total: 158 ms\n",
      "Wall time: 161 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eps = ledger.get_epsilon_spend(entity_ids_query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps = ledger.get_epsilon_spend(entity_ids_query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1509.52100047, 1509.52100047, 1509.52100047, ..., 1509.52100047,\n",
       "       1509.52100047, 1509.52100047])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "update1 = ledger.write_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ledger.sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update1.sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Cannot add update to Ledger",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [223]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mledger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [212]\u001b[0m, in \u001b[0;36mDataSubjectLedger.read_from_db\u001b[0;34m(self, update)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamp \u001b[38;5;241m=\u001b[39m update\u001b[38;5;241m.\u001b[39mtimestamp\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot add update to Ledger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mException\u001b[0m: Cannot add update to Ledger"
     ]
    }
   ],
   "source": [
    "ledger.read_from_db(update1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1373"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ledger.cache_constant2epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ledger.sigmas)/1e8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eps = ledger.get_epsilon_spend(entity_ids_query=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
