{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b0505e-7294-4880-98a1-18084fd47a9c",
   "metadata": {},
   "source": [
    "### data_subject_ledger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf403c42-ebb4-449b-b23e-2e4b84614103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class LedgerUpdate:\n",
    "    def __init__(self, sigmas, l2_norms, l2_norm_bounds, Ls, coeffs, entity_ids, update_number, timestamp):\n",
    "        self.sigmas = sigmas\n",
    "        self.l2_norms = l2_norms\n",
    "        self.l2_norm_bounds = l2_norm_bounds \n",
    "        self.Ls = Ls\n",
    "        self.coeffs = coeffs \n",
    "        self.entity_ids = entity_ids\n",
    "        self.update_number = update_number\n",
    "        self.timestamp = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6e571d6-df49-4fcd-9304-86077b28910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "import time\n",
    "\n",
    "class DataSubjectLedger:\n",
    "    \"\"\"for a particular data subject, this is the list\n",
    "    of all mechanisms releasing informationo about this\n",
    "    particular subject, stored in a vectorized form\"\"\"\n",
    "    \n",
    "    def __init__(self, default_cache_size=1e3):\n",
    "        \n",
    "        self.delta = 1e-6  # WARNING: CHANGING DELTA INVALIDATES THE CACHE\n",
    "        self.reset()\n",
    "        self.cache_constant2epsilon = list()\n",
    "        self.increase_max_cache(int(default_cache_size))\n",
    "        \n",
    "        # save initial size (number of rows from DB) when deserialized\n",
    "        self.known_db_size = 0\n",
    "        self.update_number = 0\n",
    "        self.timestamp_of_last_update = None\n",
    "    \n",
    "    def write_to_db(self):\n",
    "        self.update_number += 1\n",
    "    \n",
    "        result = LedgerUpdate(sigmas=self.sigmas[self.known_db_size:],\n",
    "                            l2_norms=self.l2_norms[self.known_db_size:],\n",
    "                            l2_norm_bounds=self.l2_norms[self.known_db_size:],\n",
    "                            Ls=self.Ls[self.known_db_size:],\n",
    "                            coeffs=self.coeffs[self.known_db_size:],\n",
    "                            entity_ids=self.entity_ids[self.known_db_size:],\n",
    "                            update_number=self.update_number,\n",
    "                            timestamp=time.time()\n",
    "                           )\n",
    "        self.known_db_size += len(self.sigmas)\n",
    "        return result\n",
    "\n",
    "    def read_from_db(self, update: LedgerUpdate):\n",
    "        if update.update_number == self.update_number + 1:\n",
    "            if self.timestamp_of_last_update is not None and update.timestamp < self.timestamp:\n",
    "                raise Exception(\"It appears that updates were created out of order.\" +  \n",
    "                \"This is probably due to multiple python threads creating updates- which should NOT happen.\" + \n",
    "                \"This is a very serious error- please contact OpenMined immediately.\" + \"Thank you!\")\n",
    "            self.sigmas = np.concatenate([self.sigmas, update.sigmas])\n",
    "            self.l2_norms = np.concatenate([self.l2_norms, update.l2_norms])\n",
    "            self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, update.l2_norm_bounds]) \n",
    "            self.Ls = np.concatenate([self.Ls, update.Ls])\n",
    "            self.coeffs = np.concatenate([self.coeffs, update.coeffs]) \n",
    "            self.entity_ids = np.concatenate([self.entity_ids, update.entity_ids])\n",
    "            self.update_number = update.update_number\n",
    "            self.timestamp = update.timestamp\n",
    "        else:\n",
    "            raise Exception(\"Cannot add update to Ledger\")\n",
    "                                                   \n",
    "                                                   \n",
    "    def reset(self):\n",
    "        self.sigmas = np.array([])\n",
    "        self.l2_norms = np.array([])\n",
    "        self.l2_norm_bounds = np.array([])\n",
    "        self.Ls = np.array([])\n",
    "        self.coeffs = np.array([])\n",
    "        self.entity_ids = np.array([])\n",
    "        self.entity2budget = np.array([])\n",
    "        \n",
    "    def batch_append(self, \n",
    "                     sigmas: np.ndarray, \n",
    "                     l2_norms: np.ndarray, \n",
    "                     l2_norm_bounds: np.ndarray, \n",
    "                     Ls: np.ndarray, \n",
    "                     coeffs: np.ndarray, \n",
    "                     entity_ids: np.ndarray):\n",
    "        \n",
    "        self.sigmas = np.concatenate([self.sigmas, sigmas])\n",
    "        self.l2_norms = np.concatenate([self.l2_norms, l2_norms])        \n",
    "        self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, l2_norm_bounds])        \n",
    "        self.Ls = np.concatenate([self.Ls, Ls])        \n",
    "        self.coeffs = np.concatenate([self.coeffs, coeffs])               \n",
    "        self.entity_ids = np.concatenate([self.entity_ids, entity_ids])\n",
    "        \n",
    "    def increase_max_cache(self, new_size):\n",
    "        new_entries = []\n",
    "        current_size = len(self.cache_constant2epsilon)\n",
    "        for i in range(new_size - current_size):\n",
    "            alpha, eps = self.get_optimal_alpha_for_constant(i+1 + current_size)\n",
    "            new_entries.append(eps)\n",
    "        self.cache_constant2epsilon = np.concatenate([self.cache_constant2epsilon, np.array(new_entries)])\n",
    "        # print(self.cache_constant2epsilon)\n",
    "        \n",
    "    def get_fake_rdp_func(self, constant):\n",
    "        \n",
    "        def func(alpha):\n",
    "            return alpha * constant\n",
    "        \n",
    "        return func\n",
    "\n",
    "    def get_alpha_search_function(self, rdp_compose_func):\n",
    "            \n",
    "        # if len(self.deltas) > 0:\n",
    "            # delta = np.max(self.deltas)\n",
    "        # else:\n",
    "        log_delta = np.log(self.delta)\n",
    "        \n",
    "        def fun(alpha):  # the input is the RDP's \\alpha\n",
    "            \n",
    "            if alpha <= 1:\n",
    "                return np.inf\n",
    "            else:\n",
    "                alpha_minus_1 = alpha-1\n",
    "                return np.maximum(rdp_compose_func(alpha) + np.log(alpha_minus_1/alpha)\n",
    "                                  - (log_delta + np.log(alpha))/alpha_minus_1, 0)\n",
    "        return fun    \n",
    "    \n",
    "    def get_optimal_alpha_for_constant(self, constant=3):\n",
    "        \n",
    "        f = self.get_fake_rdp_func(constant)\n",
    "        f2 = self.get_alpha_search_function(rdp_compose_func=f)\n",
    "        results = minimize_scalar(f2, method='Brent', bracket=(1,2), bounds=[1, np.inf])\n",
    "        \n",
    "        return results.x, results.fun\n",
    "\n",
    "        \n",
    "    def get_batch_rdp_constants(self, entity_ids_query, private=True):\n",
    "        \n",
    "        # get indices for all ledger rows corresponding to any of the entities in entity_ids_query\n",
    "        indices_batch = np.where(np.in1d(self.entity_ids, entity_ids_query))[0]\n",
    "        \n",
    "        # use the indices to get a \"batch\" of the full ledger. this is the only part\n",
    "        # of the ledger we care about (the entries corresponding to specific entities)\n",
    "        batch_sigmas = self.sigmas.take(indices_batch)\n",
    "        batch_Ls = self.Ls.take(indices_batch)\n",
    "        batch_l2_norms = self.l2_norms.take(indices_batch)\n",
    "        batch_l2_norm_bounds = self.l2_norm_bounds.take(indices_batch)\n",
    "        batch_coeffs = self.coeffs.take(indices_batch)\n",
    "        batch_entity_ids = self.entity_ids.take(indices_batch).astype(np.int64)\n",
    "        \n",
    "        squared_Ls = batch_Ls**2\n",
    "        squared_sigma = batch_sigmas**2\n",
    "        \n",
    "        if private:\n",
    "            squared_L2_norms = batch_l2_norms**2\n",
    "            constant = (squared_Ls * squared_L2_norms / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        else:\n",
    "            squared_L2_norm_bounds = batch_l2_norm_bounds**2\n",
    "            constant = (squared_Ls * squared_L2_norm_bounds / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        \n",
    "    def get_epsilon_spend(self, entity_ids_query):\n",
    "        rdp_constants = self.get_batch_rdp_constants(entity_ids_query=entity_ids_query).astype(np.int64)\n",
    "        rdp_constants_lookup = rdp_constants - 1\n",
    "        try:\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        except IndexError:\n",
    "            self.increase_max_cache(int(max(rdp_constants_lookup) * 1.1))\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        return eps_spend\n",
    "    \n",
    "    def get_overbudgeted_entities(self, user_budget: float, unique_entity_ids_query): \n",
    "        \"\"\" TODO: \n",
    "        In our current implementation, user_budget is obtained by querying the Adversarial Accountant's entity2ledger with the Data Scientist's User Key.\n",
    "        When we replace the entity2ledger with something else, we could perhaps directly add it into this method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the privacy budget spent by all the entities\n",
    "        epsilon_spent = self.get_epsilon_spend(unique_entity_ids_query.astype(np.int64))\n",
    "        \n",
    "        # Create a mask\n",
    "        is_overbudget = np.ones_like(epsilon_spent) * user_budget < epsilon_spent\n",
    "        return is_overbudget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86548a-1653-47f4-95af-67f11d904b3d",
   "metadata": {},
   "source": [
    "### gamma_tensor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161e732-56a8-47bc-9372-46cf3aaef375",
   "metadata": {},
   "source": [
    "### vectorized_publish.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f68566f-618d-4551-a026-674806d8abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from syft.core.adp.entity_list import EntityList\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e1924df-3547-4bf5-9c99-e0eb9ef4f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds_for_mechanism(value_array, min_val_array, max_val_array):\n",
    "    \"\"\"Calculates the squared L2 norm values needed to create a Mechanism, and calculate privacy budget + spend\"\"\"\n",
    "    \"\"\" If you calculate the privacy budget spend with the worst case bound, you can show this number to the D.S.\n",
    "    If you calculate it with the regular value (the value computed below when public_only = False, you cannot show the \n",
    "    privacy budget to the DS because this violates privacy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Double check whether the iDPGaussianMechanism class squares its squared_l2_norm values!!\n",
    "    worst_case_l2_norm = np.sqrt(np.sum(np.square(max_val_array - min_val_array))) * np.ones_like(value_array)\n",
    "    l2_norm = np.sqrt(np.sum(np.square(value_array)))\n",
    "    return l2_norm, worst_case_l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e880ffaa-6a75-40e2-a42e-3f2577b6a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_publish(\n",
    "    min_vals: np.ndarray, \n",
    "    max_vals: np.ndarray, \n",
    "    values: np.ndarray, \n",
    "    data_subjects: EntityList, \n",
    "    is_linear: bool = True, \n",
    "    ledger: Optional[DataSubjectLedger] = None, \n",
    "    data_scientist_budget: float = 1000, \n",
    "    sigma: float = 1.5, \n",
    "    # private: bool = False\n",
    "):\n",
    "    print(\"Starting vectorized publish\")\n",
    "    # Get all unique entities\n",
    "    unique_data_subjects = data_subjects.one_hot_lookup\n",
    "    unique_data_subject_indices = np.arange(len(unique_data_subjects)) # because unique_data_subjects returns an array, but we need indices\n",
    "    \n",
    "    print(\"Obtained data subject indices\")\n",
    "    \n",
    "    # Calculate everything needed for RDP\n",
    "    sigmas = np.ones_like(values) * sigma\n",
    "    l2_norms, l2_norm_bounds = calculate_bounds_for_mechanism(value_array=values, min_val_array=min_vals, max_val_array=max_vals)\n",
    "    coeffs = np.ones_like(values)\n",
    "    \n",
    "    if is_linear:\n",
    "        lipschitz_bounds = np.ones_like(values)\n",
    "    else:\n",
    "        raise Exception(\"gamma_tensor.lipschitz_bound property would be used here\")\n",
    "    \n",
    "    print(\"Obtained all parameters for RDP\")\n",
    "    \n",
    "    if ledger is None:\n",
    "        ledger = DataSubjectLedger()\n",
    "    print(\"Initialized ledger!\")\n",
    "    \n",
    "    ledger.reset()\n",
    "    # Get the Ledger started\n",
    "    ledger.batch_append(\n",
    "        sigmas=sigmas,\n",
    "        l2_norms=l2_norms,\n",
    "        l2_norm_bounds=l2_norm_bounds,\n",
    "        Ls=lipschitz_bounds,\n",
    "        coeffs=coeffs,\n",
    "        entity_ids=data_subjects.entities_indexed\n",
    "    )\n",
    "    \n",
    "    print(\"Concluded batch append\")\n",
    "    \n",
    "    # Query budget spend of all unique entities\n",
    "    mask = ledger.get_overbudgeted_entities(user_budget=data_scientist_budget, unique_entity_ids_query=unique_data_subject_indices)\n",
    "    \n",
    "    print(\"Obtained overbudgeted entity mask\")\n",
    "    \n",
    "    # TODO: Send this LedgerUpdate to the actual database\n",
    "    update = ledger.write_to_db()\n",
    "    \n",
    "    print(\"Written to DB!\")\n",
    "    \n",
    "    # Get result & add Gaussian noise\n",
    "    output = values * (mask^1) + gauss(sigma)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dc1fd-f7fc-4a57-8ccf-a76d2fa98e9e",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb1cc29a-c074-4f03-a16b-73b949f64795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.core.adp.entity import Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16717113-0bac-437d-ab2e-39ad4cd0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10**3\n",
    "unique_data_subject_count = int(size/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed248cd8-a907-4169-91e2-ecf2b66bb5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 6 µs, total: 1.36 ms\n",
      "Wall time: 958 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_subjects = np.random.choice([Entity(str(i)) for i in range(unique_data_subject_count)], size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abc21818-3c08-41b3-95de-ac09967cc844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_subjects)/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c49be1fa-ae85-4077-92bc-85e5cb7e0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sy.Tensor(np.ones(size, dtype=np.int32)).private(min_val=0, max_val=2, entities=data_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c12ae347-eb70-4244-a940-582cb3f0f8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.core.tensor.autodp.ndim_entity_phi.NDimEntityPhiTensor"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ced18dd-a63c-4be9-a0cd-2148df9a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_publish = data.child.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b653806-5a50-4082-beb3-055f72b737bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.core.tensor.autodp.gamma_tensor.GammaTensor"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_to_publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3bb70242-64c2-457b-89dc-a4116c152e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vectorized publish\n",
      "Obtained data subject indices\n",
      "Obtained all parameters for RDP\n",
      "Initialized ledger!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/PycharmProjects/PySyft/.tox/syft.jupyter/lib/python3.8/site-packages/scipy/optimize/_optimize.py:2782: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 0 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectorized_publish\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_vals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_to_publish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_vals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_to_publish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_to_publish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_subjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_to_publish\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_subjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36mvectorized_publish\u001b[0;34m(min_vals, max_vals, values, data_subjects, is_linear, ledger, data_scientist_budget, sigma)\u001b[0m\n\u001b[1;32m     35\u001b[0m ledger\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Get the Ledger started\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[43mledger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_append\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigmas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_norms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_norms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_norm_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_norm_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlipschitz_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoeffs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoeffs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_subjects\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentities_indexed\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConcluded batch append\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Query budget spend of all unique entities\u001b[39;00m\n",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36mDataSubjectLedger.batch_append\u001b[0;34m(self, sigmas, l2_norms, l2_norm_bounds, Ls, coeffs, entity_ids)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_append\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m     65\u001b[0m                  sigmas: np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m     66\u001b[0m                  l2_norms: np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m                  coeffs: np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m     70\u001b[0m                  entity_ids: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmas \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_norms \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_norms, l2_norms])        \n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_norm_bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_norm_bounds, l2_norm_bounds])        \n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 0 dimension(s)"
     ]
    }
   ],
   "source": [
    "vectorized_publish(\n",
    "    min_vals=result_to_publish.min_val,\n",
    "    max_vals=result_to_publish.max_val,\n",
    "    values=result_to_publish.value,\n",
    "    data_subjects=result_to_publish.data_subjects,\n",
    "    sigma=size/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fab3329f-c1ed-4bdf-8e95-145f4817d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = np.ones_like(result_to_publish.value) * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b23699bd-4d9e-41d4-8cef-5cab2a1d2464",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [62]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msigma\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sigma' is not defined"
     ]
    }
   ],
   "source": [
    "sigma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8bcb80-e9e9-4fa8-ab06-5e27aa34c089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
