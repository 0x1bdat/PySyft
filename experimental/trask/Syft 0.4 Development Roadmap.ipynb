{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General PySyft Principles\n",
    "- Everything is a Tensor\n",
    "- All execution happens on a Worker via a Client\n",
    "- Security is at the Tensor Level\n",
    "- Everything can be packaged as a Plan or Protocol\n",
    "- All objects are serializable via Protobuf to Polyglot Runtimes\n",
    "\n",
    "\n",
    "# New Features We Want\n",
    "- PyTorch has new subclassing capabilities\n",
    "- Serialization should all use Protobuf\n",
    "- Multi-framework support. \n",
    "- Code generate framework specific tensors based on a SyftTensor abstraction\n",
    "- Automatic testing for all permuations of tensor abstractions, across all methods, and all frameworks\n",
    "- All tensor abstractions should have a type inhereitance which limits their use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps Per Framework:\n",
    "\n",
    "- 1. Setup Global Flags\n",
    "    - syft.typecheck (bool): specifies whether to actively typecheck or not\n",
    "\n",
    "- 2. Custom Constructor method\n",
    "    - All Frameworks / All Tensor Types\n",
    "        - pre_init(*args, **kwargs)\n",
    "        - init()\n",
    "        - post_init(*args, **kwargs)\n",
    "    \n",
    "- 3. Reliably generate unique tensor ID (tensor.id)\n",
    "    - All Tensorflow Tensor Types:  \n",
    "        - use internal experimental id attribute\n",
    "        - during post_init, if ID is non-unique, call .regeenerate_id() which \n",
    "            re-initialize the tensor again (with the same children) until you get a unique one\n",
    "    - All Torch/Numpy Tensor Types:\n",
    "        - use random number, store on tensor (only attribute to be stored this way)\n",
    "        - during post_init, if ID is non-unique, call .regenerate_id() which will recreate a new one\n",
    "\n",
    "- BaseWorker\n",
    "    - Object Store\n",
    "    - set_obj()\n",
    "    - get_obj()\n",
    "\n",
    "- Reliably Register and Delete Tensors\n",
    "    - All Frameworks / All Tensor Types:\n",
    "        - post_init() should register the tensor with the correct BaseWorker object. \n",
    "        - del my_tensor should de-register the tensor\n",
    "\n",
    "- Custom attributes via attribute store (id -> attributes [key -> value])\n",
    "    - All Frameworks / All Tensor Types\n",
    "        - __setattr__(attribute=value) \n",
    "        - __getattr__(\"attribute\") and \n",
    "            - overload __hasattr__ too\n",
    "    \n",
    "- Custom Tensor Type\n",
    "    - All Frameworks / All Tensor Types\n",
    "    - Should be able to subclass and extend custom tensor types\n",
    "    - Initial tensor types should include:\n",
    "        - RestrictedTensor: all methods but no functionality\n",
    "        - AbstractTensor: all methods with default torch functionality (i.e., just pass through to .child)\n",
    "        - \"PlusIsMinusTensor\" which executes subtraction when addition is called\n",
    "\n",
    "- Syft Tensor Type -> CodeGen Custom Tensor Types\n",
    "    - Use AST to code generate all custom tensors from a base SyftTensor type\n",
    "\n",
    "- Automatic Testing Suite\n",
    "    - Automatically test a custom tensor with all tensor methods in all frameworks. Every operation should either produce the same results (within an error bound) as the native version of a PyTorch tensor or it should raise a NotImplemented exception (explicitly) with an error message saying that this operation is not supporteed by a specific custom tensor type.\n",
    "    - Note:\n",
    "        - Ideally we would just keep one master list of methods for each framework\n",
    "        - We would add a tensor type to a list of tensor types to be tested, possibly with a method for how to instantiate the tensor and how to convert its values to something comparable with each respective native tensor\n",
    "\n",
    "- Custom Tensor Function\n",
    "    - \n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "    \n",
    "    - Automatically test all custom tensor types with all tensor functions in all frameworks. Every operation should either produce the same results (within an error bound) as the native version of a PyTorch tensor or it should raise a NotImplemented exception (explicitly) with an error message saying that this operation is not supporteed by a specific custom tensor type.\n",
    "\n",
    "- Tensor Chaining\n",
    "    - Include type checking for tensor types\n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - We should be able to automatically permute over every plausible chain of tensor abstractions and test them for all methods and functions\n",
    "\n",
    "- Native Tracing (JIT)\n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - For all methods and functions on all tensor chains in all frameworks we should be able to test that JIT works correctly, building plans and executing them.\n",
    "    \n",
    "- AutogradTensor\n",
    "    - We should be able to specify an arbitrary point in the chain to run autograd by inserting an AutogradTensor at that point.\n",
    "    \n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - For all methods and functions on all tensor chains in all frameworks both JIT traced in plans and executed eagerly, we should be able to test that backpropagation works correctly at any (continuous/floating point) level in the tensor chain hierarchy.\n",
    "    \n",
    "- Karl's Worker, Operation, and Command Abstractions\n",
    "    \n",
    "- PointerTensor\n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - For all methods and functions on all tensor chains in all frameworks both JIT traced in plans and executed eagerly, we should be able to test that backpropagation works correctly at any (continuous/floating point) level in the tensor chain hierarchy. This includes PointerTensors\n",
    "\n",
    "- PromiseTensor\n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - For all methods and functions on all tensor chains in all frameworks both JIT traced in plans and executed eagerly, we should be able to test that backpropagation works correctly at any (continuous/floating point) level in the tensor chain hierarchy. This includes PromiseTensors\n",
    "\n",
    "- Protocol\n",
    "\n",
    "- Automatic Testing Suite Improvements\n",
    "\n",
    "    - For all methods and functions on all tensor chains in all frameworks both JIT traced in plans and executed eagerly, we should be able to test that backpropagation works correctly at any (continuous/floating point) level in the tensor chain hierarchy. This includes Protocols\n",
    "    \n",
    "- ThreadWorker\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
