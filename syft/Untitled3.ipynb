{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as _tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.tensor import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Eager Wishlist:\n",
    "\n",
    "- build something which feels like you're subclassing tensorflow even if you're not\n",
    "- GPU support\n",
    "- autograd support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractTensor():\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self._id = random.randint(0, 10e30)\n",
    "        self.register()\n",
    "        self.data = data\n",
    "\n",
    "class PlusIsMinusTensor(AbstractTensor):\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if(hasattr(self, 'child') and self.child is not None):\n",
    "            return self.child - other.child\n",
    "        else:\n",
    "            return self.data - other.data\n",
    "    \n",
    "class MinusIsMultiplyTensor(AbstractTensor):\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(hasattr(self, 'child') and self.child is not None):\n",
    "            return self.child * other.child\n",
    "        else:\n",
    "            return self.data * other.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_store = {}\n",
    "\n",
    "@property\n",
    "def id(self):\n",
    "    if(hasattr(self, \"_id\")):\n",
    "        return self._id\n",
    "    return hash(self.experimental_ref())\n",
    "\n",
    "@property\n",
    "def child(self):\n",
    "    return self.attr(\"child\")\n",
    "\n",
    "def attr(self, attr_name):\n",
    "    try:\n",
    "        return object_store[self.id][attr_name]\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def set_attr(self, attr_name, value):\n",
    "    object_store[self.id][attr_name] = value\n",
    "\n",
    "def register(self):\n",
    "    attrs = {}\n",
    "    object_store[self.id] = attrs\n",
    "    return self\n",
    "\n",
    "methods = list()\n",
    "methods.append((\"id\", id))\n",
    "methods.append((\"attr\", attr))\n",
    "methods.append((\"register\", register))\n",
    "methods.append((\"set_attr\", set_attr))\n",
    "methods.append((\"child\", child))\n",
    "    \n",
    "objects = list()\n",
    "objects.append(tf.Tensor)\n",
    "objects.append(tf.Variable)\n",
    "objects.append(tf.ResourceVariable)\n",
    "objects.append(PlusIsMinusTensor)\n",
    "objects.append(MinusIsMultiplyTensor)\n",
    "\n",
    "for obj in objects:\n",
    "    for method_name, method in methods:\n",
    "        setattr(obj, method_name, method)\n",
    "\n",
    "        \n",
    "@_tf.custom_gradient    \n",
    "def _var_add(self, other):\n",
    "    print(\"calling _var_add\")\n",
    "    if(hasattr(self, 'child') and self.child is not None):\n",
    "        result = self.child + other.child\n",
    "    else:\n",
    "        result = tf.add(self, other)\n",
    "    \n",
    "    def grad(dy):\n",
    "        return dy, dy\n",
    "    \n",
    "    return result, grad\n",
    "    \n",
    "def var_add(self, other):\n",
    "    return _var_add(self, other)\n",
    "    \n",
    "_tf.Variable.__add__ = var_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling _var_add\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable([[100.0]])\n",
    "\n",
    "child = PlusIsMinusTensor(w)\n",
    "grandchild = MinusIsMultiplyTensor(w)\n",
    "\n",
    "w.set_attr(\"child\", child)\n",
    "child.set_attr(\"child\", grandchild)\n",
    "\n",
    "# w = ResourceVariable([[100.0]])\n",
    "with _tf.GradientTape() as tape:\n",
    "    loss = w + w\n",
    "\n",
    "grad = tape.gradient(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[10000.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tf.custom_gradient\n",
    "def _add(self, other):\n",
    "\n",
    "    def grad(dy):\n",
    "        return dy, dy\n",
    "    \n",
    "    result = tf.subtract(self, other)\n",
    "    \n",
    "    return result, grad\n",
    "\n",
    "def add(self, other):\n",
    "    return _add(self, other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w = tf.Variable([[100.0]])\n",
    "# w = ResourceVariable([[100.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = add(w,w)\n",
    "\n",
    "grad = tape.gradient(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu',\n",
    "                         input_shape=(None, None, 1)),\n",
    "  tf.keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:  [[-0.04930329  0.00467802 -0.00234521 -0.03856605  0.006839   -0.03424904\n",
      "   0.05314591 -0.01709108  0.00721264 -0.01741508]]\n"
     ]
    }
   ],
   "source": [
    "for images,labels in dataset.take(1):\n",
    "  print(\"Logits: \", mnist_model(images[0:1]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "    \n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "    \n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      train_step(images, labels)\n",
    "    print ('Epoch {} finished'.format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n"
     ]
    }
   ],
   "source": [
    "train(epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp2(self):\n",
    "\n",
    "    result = simplelog2pexp(self)\n",
    "\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + tf.exp(self)))\n",
    "\n",
    "    return result, grad\n",
    "\n",
    "def outer_log1pexp(self):\n",
    "    return log1pexp2(self)\n",
    "\n",
    "def simplelog2pexp(self):\n",
    "    e = tf.exp(self)\n",
    "    result = tf.math.log(1 + e)\n",
    "    return result\n",
    "    \n",
    "\n",
    "# def log1pexp(x):\n",
    "#   return tf.math.log(1 + tf.exp(x))\n",
    "\n",
    "tf.Tensor.log1pexp2 = simplelog2pexp\n",
    "tf.Variable.log1pexp2 = outer_log1pexp\n",
    "\n",
    "w = tf.Variable([[100.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    ResourceVariable =type(w)\n",
    "    \n",
    "ResourceVariable.log1pexp2 = outer_log1pexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1, 2],\n",
    "                 [3, 4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.3132616, 2.126928 ],\n",
       "       [3.0485873, 4.01815  ]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.log1pexp2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([[100.0]])\n",
    "# w = ResourceVariable([[100.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w.log1pexp2()\n",
    "\n",
    "grad = tape.gradient(loss, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
